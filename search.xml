<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[ES中的分析器和IK分词器插件]]></title>
      <url>%2Fpost%2Felasticsearch-analyzer%2F</url>
      <content type="text"><![CDATA[一些概念Token（词元）全文搜索引擎会用某种算法对要建索引的文档进行分析， 从文档中提取出若干Tokenizer(分词器) Tokenizer(分词器)这些算法叫做Tokenizer(分词器) Token Filter(词元处理器)这些Token会被进一步处理， 比如转成小写等， 这些处理算法被称为TokenFilter(词元处理器) Term(词)被处理后的结果被称为Term(词) Character Filter(字符过滤器)文本被Tokenizer处理前可能要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为Character Filter(字符过滤器) Analyzer(分析器)这整个的分析算法被称为Analyzer(分析器) Analyzer（分析器）由Tokenizer（分词器）和Filter（过滤器）组成 图片 ES中的分词器ES内置分析器 standard analyzer simple analyzer stop analyzer keyword analyzer pattern analyzer language analyzers snowball analyzer custom analyzer ES内置分析器 standard tokenizer edge ngram tokenizer keyword tokenizer letter analyzer lowercase analyzer ngram analyzers whitespace analyzer pattern analyzer uax email url analyzer path hierarchy analyzer ES内置过滤器 standard filter ascii folding filter length filter lowercase filter ngram filter edge ngram filter porter stem filter shingle filter stop filter stop word delimiter filter stemmer token filter stemmer override filter keyword marker filter keyword repeat filter kstem filter snowball filter phonetic filter synonym filter compound word filter reverse filter elision filter truncate filter unique filter pattern capture filter pattern replace filter trim filter limit token count filter hunspell filter common grams filter normalization filter ES内置的character filter mapping char filter 根据配置的映射关系替换字符 html strip char filter 去掉HTML元素 pattern replace char filter 用正则表达式处理字符串 自定义分析器ES允许用户通过配置文件elasticsearch.yml自定义分析器Analyzer 123456index: analysis: analyzer: myAnalyzer: tokenizer: standard filter: [standard, lowercase, stop] 也可以使用第三方分析器，比如IKAnalyzer IKAnalyzerIK简介IK Analyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。从2006年12月推出1.0版开始， IKAnalyzer已经推出了4个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。从3.0版本开 始，IK发展为面向Java的公用分词组件，独立于Lucene项目，同时提供了对Lucene的默认优化实现。在2012版本中，IK实现了简单的分词 歧义排除算法，标志着IK分词器从单纯的词典分词向模拟语义分词衍化。 IK Analyzer 2012特性1.采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和智能分词两种切分模式； 2.在系统环境：Core2 i7 3.4G双核，4G内存，window 7 64位， Sun JDK 1.6_29 64位 普通pc环境测试，IK2012具有160万字/秒（3000KB/S）的高速处理能力。 3.2012版本的智能分词模式支持简单的分词排歧义处理和数量词合并输出。 4.采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符 5.优化的词典存储，更小的内存占用。支持用户词典扩展定义。特别的，在2012版本，词典支持中文，英文，数字混合词语。 安装 通过git clone https://github.com/medcl/elasticsearch-analysis-ik，下载分词器源码 执行命令：mvn clean package，打包生成elasticsearch-analysis-ik-1.2.5.jar 将这个jar拷贝到ES_HOME/plugins/analysis-ik目录下面，如果没有该目录，则先创建该目录 ES_HOME/config/elasticsearch.yml文件在文件最后加入如下内容：12345678910111213index: analysis: analyzer: ik: alias: [ik_analyzer] type: org.elasticsearch.index.analysis.IkAnalyzerProvider ik_max_word: type: ik use_smart: false ik_smart: type: ik use_smart: trueindex.analysis.analyzer.default.type: ik 测试 创建一个索引，名为index。 1curl -XPUT http://localhost:9200/index 为索引index创建mapping 1234567891011121314151617curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'&#123; "fulltext": &#123; "_all": &#123; "analyzer": "ik" &#125;, "properties": &#123; "content": &#123; "type" : "string", "boost" : 8.0, "term_vector" : "with_positions_offsets", "analyzer" : "ik", "include_in_all" : true &#125; &#125; &#125;&#125;' 测试 1234curl 'http://localhost:9200/index/_analyze?analyzer=ik&amp;pretty=true' -d '&#123;"text":"世界如此之大"&#125;' 4.显示结果123456789101112131415161718192021222324252627&#123; "tokens" : [ &#123; "token" : "text", "start_offset" : 4, "end_offset" : 8, "type" : "ENGLISH", "position" : 1 &#125;, &#123; "token" : "世界", "start_offset" : 11, "end_offset" : 13, "type" : "CN_WORD", "position" : 2 &#125;, &#123; "token" : "如此", "start_offset" : 13, "end_offset" : 15, "type" : "CN_WORD", "position" : 3 &#125;, &#123; "token" : "之大", "start_offset" : 15, "end_offset" : 17, "type" : "CN_WORD", "position" : 4 &#125; ]&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ES5.x部署遇到的问题汇总]]></title>
      <url>%2Fpost%2Fes5.x-deploy-problem%2F</url>
      <content type="text"><![CDATA[问题一 can not run elasticsearch as root 不能以root用户启动ES服务器 非要以root用户运行,对于5.X，在config/jvm.options配置文件中，添加 -Des.insecure.allow.root=true 问题二 max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 原因： 最大虚拟内存太小 解决： 切换到root用户 vi /etc/sysctl.conf 添加内容：vm.max_map_count=655360 执行命令：sysctl -p 问题三 max number of threads [1024] for user [xxx] likely too low, increase to at least [2048] 原因： 无法创建本地线程问题,用户最大可创建线程数太小 解决： 切换到root用户 vi /etc/security/limits.d/90-nproc.conf 找到如下内容：1* soft nproc 1024 修改为1* soft nproc 2048 保存、退出、重新登录才可生效 问题四 max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 原因： 无法创建本地文件问题,用户最大可创建文件数太小 解决方案： 切换到root用户 vi /etc/security/limits.conf 添加如下内容:1234567* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 *表示所有用户 保存、退出、重新登录才可生效 问题五 system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk 这是在因为操作系统不支持SecComp，而ES5.2.2默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。 在elasticsearch.yml中配置bootstrap.system_call_filter为false，注意要在Memory下面:12bootstrap.memory_lock: falsebootstrap.system_call_filter: false 问题六12345[2016-11-06T16:27:21,712][WARN ][o.e.b.JNANatives ] unable to install syscall filter:Java.lang.UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMPandCONFIG_SECCOMP_FILTERcompiledinatorg.elasticsearch.bootstrap.Seccomp.linuxImpl(Seccomp.java:349) ~[elasticsearch-5.0.0.jar:5.0.0]at org.elasticsearch.bootstrap.Seccomp.init(Seccomp.java:630) ~[elasticsearch-5.0.0.jar:5.0.0] 报了一大串错误，大家不必惊慌，其实只是一个警告，主要是因为你Linux版本过低造成的。 1、重新安装新版本的Linux系统2、警告不影响使用，可以忽略 问题七 org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream 原因: ElasticSearch节点之间的jdk版本不一致 解决方案： ElasticSearch集群统一jdk环境 问题八 Unsupported major.minor version 52.0 原因: jdk版本问题太低 解决方案： 更换jdk版本，ElasticSearch5.0.0支持jdk1.8.0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[微服务架构解析(附思维导图)]]></title>
      <url>%2Fpost%2Farchitecture-microservice%2F</url>
      <content type="text"><![CDATA[思维导图 介绍微服务架构（Microservice Architecture）是一种架构概念 旨在通过将功能分解到各个离散的服务中以实现对解决方案的解耦 将功能分解到离散的各个服务当中，从而降低系统的耦合性，并提供更加灵活的服务支持。 传统开发模式和微服务的区别优点 开发简单，集中式管理 基本不会重复开发 功能都在本地，没有分布式的管理和调用消耗 缺点 效率低：开发都在同一个项目改代码，相互等待，冲突不断 维护难：代码功功能耦合在一起，新人不知道何从下手 不灵活：构建时间长，任何小修改都要重构整个项目，耗时 稳定性差：一个微小的问题，都可能导致整个应用挂掉 扩展性不够：无法满足高并发下的业务需求 微服务架构特征官方的定义： 一些列的独立的服务共同组成系统 单独部署，跑在自己的进程中 每个服务为独立的业务开发 分布式管理 非常强调隔离性 大概的标准 分布式服务组成的系统 按照业务，而不是技术来划分组织 做有生命的产品而不是项目 强服务个体和弱通信（ Smart endpoints and dumb pipes ） 自动化运维（ DevOps ） 高度容错性 快速演化和迭代 SOA和微服务的区别 SOA喜欢重用，微服务喜欢重写 SOA喜欢水平服务，微服务喜欢垂直服务 SOA喜欢自上而下，微服务喜欢自下而上 实践微服务客户端如何访问这些服务一般在后台N个服务和UI之间一般会一个代理或者叫API Gateway 作用： 提供统一服务入口，让微服务对前台透明 聚合后台的服务，节省流量，提升性能 提供安全，过滤，流控等API管理功能 每个服务之间如何通信 REST（JAX-RS，Spring Boot） RPC（Thrift, Dubbo） 服务发现服务注册 zookeeper dubbo 服务挂了，如何解决 重试机制 限流 熔断机制 负载均衡 降级（本地缓存） 参考：Netflix的Hystrix 优缺点 优点复杂度可控，独立按需扩展，技术选型灵活，容错，可用性高 缺点多服务运维难度，系统部署依赖，服务间通信成本，数据一致性，系统集成测试，重复工作，性能监控等 思考微服务对我们的思考，更多的是思维上的转变。对于微服务架构：技术上不是问题，意识比工具重要。 关于微服务的几点设计出发点： 应用程序的核心是业务逻辑，按照业务或客户需求组织资源（这是最难的） 做有生命的产品，而不是项目 头狼战队，全栈化 后台服务贯彻Single Responsibility Principle（单一职责原则） VM-&gt;Docker （to PE） DevOps (to PE) 同时，对于开发同学，有这么多的中间件和强大的PE支持固然是好事，我们也需要深入去了解这些中间件背后的原理，知其然知其所以然，在有限的技术资源如何通过开源技术实施微服务？ 最后，一般提到微服务都离不开DevOps和Docker，理解微服务架构是核心，devops和docker是工具，是手段。 参考http://www.cnblogs.com/imyalost/p/6792724.html?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io http://www.jianshu.com/p/77ce2dbd1d6e http://kb.cnblogs.com/page/520922/ http://www.infoq.com/cn/articles/seven-uservices-antipatterns http://www.csdn.net/article/2015-08-07/2825412 http://blog.csdn.net/mindfloating/article/details/45740573 http://blog.csdn.net/sunhuiliang85/article/details/52976210 http://www.oschina.net/news/70121/microservice]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM编译优化]]></title>
      <url>%2Fpost%2Fjava-compile-optimization%2F</url>
      <content type="text"><![CDATA[在部分的商用虚拟机中，Java 程序最初是通过解释器（Interpreter ）进行解释执行的，当虚拟机发现某个方法或代码块的运行特别频繁的时候，就会把这些代码认定为“热点代码”。为了提高热点代码的执行效率，在运行时，即时编译器（Just In Time Compiler ）会把这些代码编译成与本地平台相关的机器码，并进行各种层次的优化。 HotSpot 内的即时编译器解释器和编译器各有各的优点 解释器优点：当程序需要迅速启动的时候，解释器可以首先发挥作用，省去了编译的时间，立即执行。解释执行占用更小的内存空间。同时，当编译器进行的激进优化失败的时候，还可以进行逆优化来恢复到解释执行的状态。 编译器优点：在程序运行时，随着时间的推移，编译器逐渐发挥作用根据热点探测功能，，将有价值的字节码编译为本地机器指令，以换取更高的程序执行效率。因此，整个虚拟机执行架构中，解释器与编译器经常配合工作。 HotSpot中内置了两个即时编译器，分别称为 Client Compiler和 Server Compiler ，或者简称为 C1编译器和 C2编译器。 目前的 HotSpot编译器默认的是解释器和其中一个即时编译器配合的方式工作，具体是哪一个编译器，取决于虚拟机运行的模式，HotSpot虚拟机会根据自身版本与计算机的硬件性能自动选择运行模式 用户也可以使用 -client和 -server参数强制指定虚拟机运行在 Client模式或者 Server模式。这种配合使用的方式称为“混合模式”（Mixed Mode） 用户可以使用参数 -Xint 强制虚拟机运行于 “解释模式”（Interpreted Mode），这时候编译器完全不介入工作。 另外，使用 -Xcomp 强制虚拟机运行于 “编译模式”（Compiled Mode），这时候将优先采用编译方式执行，但是解释器仍然要在编译无法进行的情况下接入执行过程。通过虚拟机 -version 命令可以查看当前默认的运行模式。 被编译对象和触发条件在运行过程中会被即时编译的“热点代码”有两类 被多次调用的方法 被多次执行的循环体 对于第一种，编译器会将整个方法作为编译对象，这也是标准的JIT 编译方式。 对于第二种是由循环体出发的，但是编译器依然会以整个方法作为编译对象，因为发生在方法执行过程中，称为栈上替换。 判断一段代码是否是热点代码，是不是需要出发即时编译，这样的行为称为热点探测（Hot Spot Detection），探测算法有两种，分别为。 基于采样的热点探测（Sample Based Hot Spot Detection）：虚拟机会周期的对各个线程栈顶进行检查，如果某些方法经常出现在栈顶，这个方法就是“热点方法”。好处是实现简单、高效，很容易获取方法调用关系。缺点是很难确认方法的reduce，容易受到线程阻塞或其他外因扰乱。 基于计数器的热点探测（Counter Based Hot Spot Detection）：为每个方法（甚至是代码块）建立计数器，执行次数超过阈值就认为是“热点方法”。优点是统计结果精确严谨。缺点是实现麻烦，不能直接获取方法的调用关系。HotSpot 使用的是第二种-基于技术其的热点探测，并且有两类计数器：方法调用计数器（Invocation Counter）和回边计数器（Back Edge Counter）。这两个计数器都有一个确定的阈值，超过后便会触发 JIT 编译。 编译过程编译期优化（早期优化）为了保证JRuby，Groovy等语言编译的字节码也能得到性能优化，JVM将性能优化放在了后期的运行时优化，即JIT运行时编译优化中。具体优化: 编译期优化主要为语法糖，用来实现Java的各种新的语法特性，比如泛型，变长参数，自动装箱/拆箱。 Java语法糖：与字节码无关，编译后会去掉它们。作用仅仅为方便码农写代码，以及将运行时异常在编译期及早发现（如泛型的使用）。 泛型与类型擦除Java泛型只在编译期存在，编译完成后的字节码中会替换为原生类型。故称Java泛型为伪泛型。C#的泛型在运行期仍然存在。 条件编译if语句中使用常量。比如if(false) {},这个语句块不会被编译到字节码中.这个过程在编译时的控制流分析中完成。 运行时优化（晚期优化）不同JVM的运行时优化策略Hotspot采用解释器与编译器并存的构架。第0层，解释执行，不开启性能监控器，可触发第一层编译第1层，将字节码编译为机器码，进行简单可靠的优化，可以开启性能监控第2层，将字节码编译为机器码，会开启一些编译耗时的优化和一些不可靠的激进 具体优化 公共字表达式消除如果一个表达式E已经被计算过了，并且从先前的计算到现在E中所有变量的值都没有发生变化，那么E的这次出现就称为了公共子表达式。对于这种表达式，没有必要花时间再对它进行计算，只需要直接用前面计算过的表达式结果代替E就可以了。 数组边界检查消除数组边界检查消除（Array Bounds Checking Elimination）是即时编译器中的一项语言相关的经典优化技术。Java访问数组的时候系统将会自动进行上下界的范围检查，但对于虚拟机的执行子 系统来说，每次数组元素的读写都带有一次隐含的条件判定操作，对于拥有大量数组访问的程序代码，这无疑也是一种性能负担。数组边界检查时必须做的，但数组边界检查在某些情况下可以简化。例如数组下标示一个常量，如foo3，只要在编译器根据数据流分析来确定foo.length的值，并判断下标“3”没有越界，执行的时候就无须判断了。再例如数组访问发生在循环之中，并且使用循环变量来进行数组访问，如果编译器只要通过数据流分析就可以判定循环变量的取值范围永远在区间[0, foo.length)之内，那在整个循环中就可以把数组的上下界检查消除掉，这可以节省很多次的条件判断操作。与语言相关的其他消除操作还有自动装箱消除（Autobox Elimination）、安全点消除（Safepoint Elimination）、消除反射（Dereflection）等。 方法内联方法内联是编译器最重要的优化手段之一，除了消除方法调用的成本之外，更重要的是可以为其他优化手段建立良好的基础。逃逸分析逃逸分析（Escape Analysis）并不是直接优化代码的手段，而是为其他优化手段提供依据的分析技术。 参考： http://blog.csdn.net/chdjj/article/details/24010581 http://blog.csdn.net/qq_16681169/article/details/72945113 http://yueyemaitian.iteye.com/blog/1185297 http://ifeve.com/jvm-compiler/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM垃圾回收器详解]]></title>
      <url>%2Fpost%2Fjava-gc-collector%2F</url>
      <content type="text"><![CDATA[说明针对jdk7 关于垃圾回收可以查看我之前的博文：JVM内存管理-垃圾回收与内存分配 垃圾收集器分类 按线程数分 可以分为串行垃圾回收器和并行垃圾回收器。串行垃圾回收器一次只使用一个线程进行垃圾回收；并行垃圾回收器一次将开启多个线程同时进行垃圾回收。在并行能力较强的 CPU 上，使用并行垃圾回收器可以缩短 GC 的停顿时间。 按照工作模式分 可以分为并发式垃圾回收器和独占式垃圾回收器。并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间；独占式垃圾回收器 (Stop the world) 一旦运行，就停止应用程序中的其他所有线程，直到垃圾回收过程完全结束 按碎片处理方式 可分为压缩式垃圾回收器和非压缩式垃圾回收器。压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片；非压缩式的垃圾回收器不进行这步操作。 按工作的内存区间 可分为新生代垃圾回收器和老年代垃圾回收器 垃圾收集器的指标 吞吐量 指在应用程序的生命周期内，应用程序所花费的时间和系统总运行时间的比值。系统总运行时间=应用程序耗时+GC 耗时。如果系统运行了 100min，GC 耗时 1min，那么系统的吞吐量就是 (100-1)/100=99%。垃圾回收器负载：和吞吐量相反，垃圾回收器负载指来记回收器耗时与系统运行总时间的比值。 停顿时间 指垃圾回收器正在运行时，应用程序的暂停时间。对于独占回收器而言，停顿时间可能会比较长。使用并发的回收器时，由于垃圾回收器和应用程序交替运行，程序的停顿时间会变短，但是，由于其效率很可能不如独占垃圾回收器，故系统的吞吐量可能会较低。 垃圾回收频率 指垃圾回收器多长时间会运行一次。一般来说，对于固定的应用而言，垃圾回收器的频率应该是越低越好。通常增大堆空间可以有效降低垃圾回收发生的频率，但是可能会增加回收产生的停顿时间。 反应时间 指当一个对象被称为垃圾后多长时间内，它所占据的内存空间会被释放。 堆分配 不同的垃圾回收器对堆内存的分配方式可能是不同的。一个良好的垃圾收集器应该有一个合理的堆内存区间划分。 垃圾收集器介绍 新生代串行收集器串行收集器主要有两个特点：第一，它仅仅使用单线程进行垃圾回收；第二，它独占式的垃圾回收。 在串行收集器进行垃圾回收时，Java 应用程序中的线程都需要暂停，等待垃圾回收的完成，这样给用户体验造成较差效果。虽然如此，串行收集器却是一个成熟、经过长时间生产环境考验的极为高效的收集器。新生代串行处理器使用复制算法，实现相对简单，逻辑处理特别高效，且没有线程切换的开销。在诸如单 CPU 处理器或者较小的应用内存等硬件平台不是特别优越的场合，它的性能表现可以超过并行回收器和并发回收器。在 HotSpot 虚拟机中，使用-XX：+UseSerialGC 参数可以指定使用新生代串行收集器和老年代串行收集器。当 JVM 在 Client 模式下运行时，它是默认的垃圾收集器。 老年代串行收集器老年代串行收集器使用的是标记-压缩算法。和新生代串行收集器一样，它也是一个串行的、独占式的垃圾回收器。由于老年代垃圾回收通常会使用比新生代垃圾回收更长的时间，因此，在堆空间较大的应用程序中，一旦老年代串行收集器启动，应用程序很可能会因此停顿几秒甚至更长时间。虽然如此，老年代串行回收器可以和多种新生代回收器配合使用，同时它也可以作为 CMS 回收器的备用回收器。若要启用老年代串行回收器，可以尝试使用以下参数：-XX:+UseSerialGC: 新生代、老年代都使用串行回收器。 如果使用-XX:+UseParNewGC 参数设置，表示新生代使用并行收集器，老年代使用串行收集器 如果使用-XX:+UseParallelGC 参数设置，表示新生代和老年代均使用并行回收收集器 并行收集器并行收集器是工作在新生代的垃圾收集器，它只简单地将串行回收器多线程化。它的回收策略、算法以及参数和串行回收器一样。 并行回收器也是独占式的回收器，在收集过程中，应用程序会全部暂停。但由于并行回收器使用多线程进行垃圾回收，因此，在并发能力比较强的 CPU 上，它产生的停顿时间要短于串行回收器，而在单 CPU 或者并发能力较弱的系统中，并行回收器的效果不会比串行回收器好，由于多线程的压力，它的实际表现很可能比串行回收器差。 开启并行回收器可以使用参数-XX:+UseParNewGC，该参数设置新生代使用并行收集器，老年代使用串行收集器。 设置参数-XX:+UseConcMarkSweepGC 可以要求新生代使用并行收集器，老年代使用 CMS。 并行收集器工作时的线程数量可以使用-XX:ParallelGCThreads 参数指定。一般，最好与 CPU 数量相当，避免过多的线程数影响垃圾收集性能 新生代并行回收 (Parallel Scavenge) 收集器新生代并行回收收集器也是使用复制算法的收集器。从表面上看，它和并行收集器一样都是多线程、独占式的收集器。但是，并行回收收集器有一个重要的特点：它非常关注系统的吞吐量。 新生代并行回收收集器可以使用以下参数启用： -XX:+UseParallelGC:新生代使用并行回收收集器，老年代使用串行收集器。 -XX:+UseParallelOldGC:新生代和老年代都是用并行回收收集器。 新生代并行回收收集器可以使用以下参数启用： -XX:+MaxGCPauseMills:设置最大垃圾收集停顿时间，它的值是一个大于 0 的整数。收集器在工作时会调整 Java 堆大小或者其他一些参数，尽可能地把停顿时间控制在 MaxGCPauseMills 以内。如果希望减少停顿时间，而把这个值设置得很小，为了达到预期的停顿时间，JVM 可能会使用一个较小的堆 (一个小堆比一个大堆回收快)，而这将导致垃圾回收变得很频繁，从而增加了垃圾回收总时间，降低了吞吐量。 -XX:+GCTimeRatio：设置吞吐量大小，它的值是一个 0-100 之间的整数。假设 GCTimeRatio 的值为 n，那么系统将花费不超过 1/(1+n) 的时间用于垃圾收集。比如 GCTimeRatio 等于 19，则系统用于垃圾收集的时间不超过 1/(1+19)=5%。默认情况下，它的取值是 99，即不超过 1%的时间用于垃圾收集。 除此之外，并行回收收集器与并行收集器另一个不同之处在于，它支持一种自适应的 GC 调节策略，使用-XX:+UseAdaptiveSizePolicy 可以打开自适应 GC 策略。在这种模式下，新生代的大小、eden 和 survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。在手工调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量 (GCTimeRatio) 和停顿时间 (MaxGCPauseMills)，让虚拟机自己完成调优工作 老年代并行回收收集器老年代的并行回收收集器也是一种多线程并发的收集器。和新生代并行回收收集器一样，它也是一种关注吞吐量的收集器。老年代并行回收收集器使用标记-压缩算法，JDK1.6 之后开始启用。 使用-XX:+UseParallelOldGC 可以在新生代和老生代都使用并行回收收集器，这是一对非常关注吞吐量的垃圾收集器组合，在对吞吐量敏感的系统中，可以考虑使用。参数-XX:ParallelGCThreads 也可以用于设置垃圾回收时的线程数量。 CMS 收集器与并行回收收集器不同，CMS 收集器主要关注于系统停顿时间。CMS 是 Concurrent Mark Sweep 的缩写，意为并发标记清除，从名称上可以得知，它使用的是标记-清除算法，同时它又是一个使用多线程并发回收的垃圾收集器。 CMS 工作时，主要步骤有：初始标记、并发标记、重新标记、并发清除和并发重置。其中初始标记和重新标记是独占系统资源的，而并发标记、并发清除和并发重置是可以和用户线程一起执行的。因此，从整体上来说，CMS 收集不是独占式的，它可以在应用程序运行过程中进行垃圾回收。 根据标记-清除算法，初始标记、并发标记和重新标记都是为了标记出需要回收的对象。并发清理则是在标记完成后，正式回收垃圾对象；并发重置是指在垃圾回收完成后，重新初始化 CMS 数据结构和数据，为下一次垃圾回收做好准备。并发标记、并发清理和并发重置都是可以和应用程序线程一起执行的。 CMS 收集器在其主要的工作阶段虽然没有暴力地彻底暂停应用程序线程，但是由于它和应用程序线程并发执行，相互抢占 CPU，所以在 CMS 执行期内对应用程序吞吐量造成一定影响。CMS 默认启动的线程数是 (ParallelGCThreads+3)/4),ParallelGCThreads 是新生代并行收集器的线程数，也可以通过-XX:ParallelCMSThreads 参数手工设定 CMS 的线程数量。当 CPU 资源比较紧张时，受到 CMS 收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 由于 CMS 收集器不是独占式的回收器，在 CMS 回收过程中，应用程序仍然在不停地工作。在应用程序工作过程中，又会不断地产生垃圾。这些新生成的垃圾在当前 CMS 回收过程中是无法清除的。同时，因为应用程序没有中断，所以在 CMS 回收过程中，还应该确保应用程序有足够的内存可用。因此，CMS 收集器不会等待堆内存饱和时才进行垃圾回收，而是当前堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在 CMS 工作过程中依然有足够的空间支持应用程序运行。 这个回收阈值可以使用-XX:CMSInitiatingOccupancyFraction 来指定，默认是 68。即当老年代的空间使用率达到 68%时，会执行一次 CMS 回收。如果应用程序的内存使用率增长很快，在 CMS 的执行过程中，已经出现了内存不足的情况，此时，CMS 回收将会失败，JVM 将启动老年代串行收集器进行垃圾回收。如果这样，应用程序将完全中断，直到垃圾收集完成，这时，应用程序的停顿时间可能很长。因此，根据应用程序的特点，可以对-XX:CMSInitiatingOccupancyFraction 进行调优。如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低 CMS 的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。 标记-清除算法将会造成大量内存碎片，离散的可用空间无法分配较大的对象。在这种情况下，即使堆内存仍然有较大的剩余空间，也可能会被迫进行一次垃圾回收，以换取一块可用的连续内存，这种现象对系统性能是相当不利的，为了解决这个问题，CMS 收集器还提供了几个用于内存压缩整理的算法。 -XX:+UseCMSCompactAtFullCollection 参数可以使 CMS 在垃圾收集完成后，进行一次内存碎片整理。内存碎片的整理并不是并发进行的。-XX:CMSFullGCsBeforeCompaction 参数可以用于设定进行多少次 CMS 回收后，进行一次内存压缩。 -XX:CMSInitiatingOccupancyFraction 设置为 100，同时设置-XX:+UseCMSCompactAtFullCollection 和-XX:CMSFullGCsBeforeCompaction，日志输出如下： 关于CMS收集器，深入了解阅读下面的博文link G1 收集器 (Garbage First)G1 收集器的目标是作为一款服务器的垃圾收集器，因此，它在吞吐量和停顿控制上，预期要优于 CMS 收集器。 与 CMS 收集器相比，G1 收集器是基于标记-压缩算法的。因此，它不会产生空间碎片，也没有必要在收集完成后，进行一次独占式的碎片整理工作。G1 收集器还可以进行非常精确的停顿控制。它可以让开发人员指定当停顿时长为 M 时，垃圾回收时间不超过 N。使用参数-XX:+UnlockExperimentalVMOptions –XX:+UseG1GC 来启用 G1 回收器，设置 G1 回收器的目标停顿时间：-XX:MaxGCPauseMills=20,-XX:GCPauseIntervalMills=200。 关于G1收集器，深入了解阅读下面的博文linklink GC 相关参数总结 与串行回收器相关的参数-XX:+UseSerialGC:在新生代和老年代使用串行回收器。-XX:+SurvivorRatio:设置 eden 区大小和 survivor 区大小的比例。-XX:+PretenureSizeThreshold:设置大对象直接进入老年代的阈值。当对象的大小超过这个值时，将直接在老年代分配。-XX:MaxTenuringThreshold:设置对象进入老年代的年龄的最大值。每一次 Minor GC 后，对象年龄就加 1。任何大于这个年龄的对象，一定会进入老年代。 与并行 GC 相关的参数-XX:+UseParNewGC: 在新生代使用并行收集器。-XX:+UseParallelOldGC: 老年代使用并行回收收集器。-XX:ParallelGCThreads：设置用于垃圾回收的线程数。通常情况下可以和 CPU 数量相等。但在 CPU 数量比较多的情况下，设置相对较小的数值也是合理的。-XX:MaxGCPauseMills：设置最大垃圾收集停顿时间。它的值是一个大于 0 的整数。收集器在工作时，会调整 Java 堆大小或者其他一些参数，尽可能地把停顿时间控制在 MaxGCPauseMills 以内。-XX:GCTimeRatio:设置吞吐量大小，它的值是一个 0-100 之间的整数。假设 GCTimeRatio 的值为 n，那么系统将花费不超过 1/(1+n) 的时间用于垃圾收集。-XX:+UseAdaptiveSizePolicy:打开自适应 GC 策略。在这种模式下，新生代的大小，eden 和 survivor 的比例、晋升老年代的对象年龄等参数会被自动调整，以达到在堆大小、吞吐量和停顿时间之间的平衡点。 与 CMS 回收器相关的参数-XX:+UseConcMarkSweepGC: 新生代使用并行收集器，老年代使用 CMS+串行收集器。-XX:+ParallelCMSThreads: 设定 CMS 的线程数量。-XX:+CMSInitiatingOccupancyFraction:设置 CMS 收集器在老年代空间被使用多少后触发，默认为 68%。-XX:+UseFullGCsBeforeCompaction:设定进行多少次 CMS 垃圾回收后，进行一次内存压缩。-XX:+CMSClassUnloadingEnabled:允许对类元数据进行回收。-XX:+CMSParallelRemarkEndable:启用并行重标记。-XX:CMSInitatingPermOccupancyFraction:当永久区占用率达到这一百分比后，启动 CMS 回收 (前提是-XX:+CMSClassUnloadingEnabled 激活了)。-XX:UseCMSInitatingOccupancyOnly:表示只在到达阈值的时候，才进行 CMS 回收。-XX:+CMSIncrementalMode:使用增量模式，比较适合单 CPU。 与 G1 回收器相关的参数-XX:+UseG1GC：使用 G1 回收器。-XX:+UnlockExperimentalVMOptions:允许使用实验性参数。-XX:+MaxGCPauseMills:设置最大垃圾收集停顿时间。-XX:+GCPauseIntervalMills:设置停顿间隔时间。 其他参数-XX:+DisableExplicitGC: 禁用显示 GC。 （本文完）参考： https://www.ibm.com/developerworks/cn/java/j-lo-JVMGarbageCollection/ https://github.com/pzxwhc/MineKnowContainer/issues/89 https://github.com/pzxwhc/MineKnowContainer/issues/90 http://ifeve.com/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3g1%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8/ http://chenchendefeng.iteye.com/blog/455883 http://www.jianshu.com/p/50d5c88b272d]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch5.5官方文档翻译-Zen Discovery]]></title>
      <url>%2Fpost%2Felasticsearch-zen-discovery%2F</url>
      <content type="text"><![CDATA[章节Modules » Discovery » Zen Discovery 概述Zen Discovery是内置在elasticsearch的默认发现模块。它提供单播发现，但可扩展到支持云环境和其他形式的发现。 禅发现集成了其它模块，例如，节点之间的所有通信是使用transport模块。 它被分离成多个子模块，其解释如下： Ping这是一个节点使用发现机制找到其他节点的过程。 Unicast单播发现需要一个主机列表，用于将作为GossipRouter。这些宿主可被指定为主机名或IP地址;指定主机名的主机每一轮Ping过程中解析为IP地址。 需要注意的是，因为有Java安全管理器，JVM的默认无限期缓存有效的主机名解析。这可以通过添加修改1networkaddress.cache.ttl=&lt;timeout&gt; 到你的 Java安全策略。未能解析的任何主机都将被记录。 还要注意的是，因为有Java安全管理器，JVM的默认缓存无效的主机名解析十秒钟。这可以通过添加修改1networkaddress.cache.negative.ttl=&lt;timeout&gt; 到你的Java安全策略。 建议在单播的主机列表中维护集群中的主合格节点的列表。 单播的发现提供了与以下设置discovery.zen.ping.unicast前缀： hosts用数组阵列设置或逗号分隔设置。每个值应在的形式，host:port或host（其中port默认为设置transport.profiles.default.port 如果未设定使用transport.tcp.port）。需要注意的是IPv6的主机必须用括号括起来。默认为127.0.0.1, [::1] hosts.resolve_timeout 每轮ping的DNS查找等待时间量。默认为5秒。 单播发现使用传输模块来执行发现。 Master选举作为Ping过程的一部分，集群的主节点要么当选要么加入假期。这是自动完成的。1discovery.zen.ping_timeout（默认为3s） 允许选举时间的波动去处理速度慢或拥塞网络的情况（较高的值保证失败率少）。一旦一个节点加入时，它会发出一个加入请求到主（discovery.zen.join_timeout）超时时间默认是ping timeout的20倍 当主节点停止或遇到问题，群集节点萌重新开始执行ping命令，并会选出新的主节点。这轮Ping也可对抗（部分）网络故障，其中一个节点可能认为主节点挂掉。在这种情况下，节点将从其他节点获取当前的主节点 如果1discovery.zen.master_election.ignore_non_master_pings 是true，选主期间，从没有资格的节点（当节点的node.master是false）奖杯忽略该值默认值是 false。 节点可以被排除成为主节点当node.master为false。 1discovery.zen.minimum_master_nodes 设置需要加入一个新的主选举的最小的主合格节点数量，相同的设置控制活的主合格节点的最小数量，应该是活集群的一部分。如果这一要求得不到满足，活动主节点将下台，新的主导选举将开始。 此设置必须通过一个公式算出来。建议避免仅具有两个主资格的节点中，因为任何一个主合格的节点的丢失将导致不可操作群集。 故障检测有两个故障检测运行过程。首先是由主，来ping集群中的所有其他节点，并验证它们是否还活着。而在另一端，每个节点ping 主节点，以验证它是否还活着或选举过程需要启动。 以下设置控制使用的故障检测过程 以1discovery.zen.fd 作前缀： ping_interval多久一个节点被ping。默认为1s。 ping_timeout等待多久ping响应，默认为 30s。 ping_retries有多少ping失败/超时导致被视为一个节点失败。默认为3。 集群状态主节点是群集中，可以使改变集群状态的唯一节点。主节点一次处理一个集群状态更新，状态改变和发布更新的到集群中的所有其他节点。每个节点接收发布消息，确认它，但还没有立即应用它。如果主节点没有接收来自节点确认的数量至少为discovery.zen.minimum_master_nodes，在时间（由受控discovery.zen.commit_timeout设置，默认值为30秒）内。节点集群状态改变被拒绝。 一旦足够的节点已作出回应，集群状态改变被提交然后消息将被发送到所有结点。然后节点然后进行新的群集状态适用于他们的内部状态。主节点等待所有节点响应，在去队列处理下一个状态更新之前，直到超时，超时时间是在discovery.zen.publish_timeout默认情况下设置为30秒，时间从发布开始时测量h超时设置可以通过动态的改变集群更新设置API 无主块为了让集群全面运作，必须有一个有效的主节点和一些主资格的节点。主资格的节点必须满足的数目 discovery.zen.minimum_master_nodes，如果被设置。1discovery.zen.no_master_block 设置控制什么操作被拒绝，当没有有效的主节点时。 该discovery.zen.no_master_block设置有两个有效的选项： all 对节点的所有的操作，比如读与写操作，将被拒绝。这也适用于读与写集群状态的api，获取索引设置，设置mapping和集群状态api write（默认）写操作将被拒绝。读操作会成功。基于最后已知的群集配置。这可能导致局部陈旧数据的读取，因为节点可能从集群的其余部分分离。 该discovery.zen.no_master_block设置并不适用于基于节点的API（例如群集的统计信息，节点信息和节点统计的API）。请这些API将不会被阻塞，可以在任何可用的节点上运行。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch5.5官方文档翻译-节点]]></title>
      <url>%2Fpost%2Felasticsearch-5.5-node%2F</url>
      <content type="text"><![CDATA[概述章节：Moudles&gt;&gt;Node 任何时候你启动Elasticsearch的实例时，你启动一个 节点。连接的节点的集合被称为一个 集群。如果您正在运行Elasticsearch的一个节点，那么你有一个包含一个节点的集群。 集群中的每个节点都默认的可以处理HTTP和 Transport 流量。传输层专门用于节点们和JavaTransportClient的的通信 ; HTTP层仅由外部REST客户端使用。 所有节点都了解所有集群中的其他节点，而且可以转发客户端请求到相应的节点。除此之外，每个节点提供一个或多个目的： 主合格的节点具有节点node.master设置为true（默认值），这使得它有资格被选为主节点，主节点可以控制该集群。 数据节点具有节点node.data设置为true（默认）。数据节点保存数据并进行数据相关的操作，如CRUD，搜索和聚合。 摄取节点具有节点node.ingest设置为true（默认）。摄取节点都能够运用摄取管道，以便建立索引之前转换和丰富文档。如果想拥有一个重摄取负载，可以使用专用摄取节点然后设置主数据和节点的 node.ingest: false 部落节点一个部落节点，通过tribe.*设置，是一种特殊类型的协调，可以连接到多个集群并在所有连接的集群执行搜索和其他操作。 默认情况下，一个节点是主合格的节点和数据节点，再加上它可以通过摄取管道预处理文件。这对小集群非常方便，但是，随着集群的增长，考虑从专用数据节点分离出专用主节点资格就变得很重要。 注意：协调节点如搜索请求或批量索引请求可能涉及到不同的数据节点上保存的数据。例如搜索请求，分两个阶段，这是由接收客户机请求的节点协调执行的，这个节点就叫做协调节点。 在分散阶段，协调节点将请求转发到该保存数据的数据节点。每个数据节点本地执行的请求，其结果返回到协调节点。在收集 阶段，协同节点汇集每个数据节点的结果转换成一个单一的全球结果集。 每个节点是隐含协调节点。这意味着，节点有下面三个设置node.master，node.data，node.ingest设置为false，则只充当一个协调节点，它不能被禁用。其结果是，这样一个节点需要有足够的内存和CPU以处理收集阶段。 主合格节点主节点负责轻量的集群范围的操作，如创建或删除的索引，跟踪哪些节点是群集的一部分，并且决定哪些分片分配给哪些节点，有一个稳定的主节点对集群的健康是很重要的。 任何主合格的节点（默认情况下所有节点）可能当选成为由主节点通过主选举进程。 重要主节点必须能够访问该data/目录（就像 data节点），这样集群状态会在节点重新启动后在节点之间被保持。 索引和搜索您的数据是CPU，内存 和I / O密集型的工作，这样会把压榨一个节点的资源。为了确保您的主节点是稳定的，而不是在压力下，在一个大的集群里分出专用主合格节点和专用数据节点是个好主意。 当然主节点也可以表现为协调节点 和路由搜索和索引请求从客户端到数据节点，最好不使用专用的主节点做这个。让主资格的节点做尽可能少的工作，对集群的稳定性很重要。 要创建一个专用的主合格的节点，设置：123node.master: truenode.data: falsenode.ingest: false 避免脑裂与minimum_master_nodes设置 为了防止数据丢失，配置 discovery.zen.minimum_master_nodes （缺省为1）是重要的。为了形成一个集群。 使得每个主资格节点都知道 主资格的节点的最小数量 。 为了解释，假设您有由两个主合格节点的集群。网络故障打破了这两个节点之间的通信。每个节点可以看到一个主合格的节点. 也就是他们.本身。随着minimum_master_nodes设置为默认1，这样足以形成一个集群。每个节点选择自己作为新的主（认为其他主合格的节点已经挂掉），结果形成了两个集群，或脑裂。这两个节点永远不会重新加入到一个节点被重新启动。已经写入到重新启动的节点的任何数据都将丢失。 现在，假设你有一个集群有三个主节点的资格，并 minimum_master_nodes设置为2。如果网络故障分开三个节点中的一个，一个节点这边不够的主节点的资格，并意识到它不能选择自己为master。具有两个节点的边将选举新的master（如果需要），并继续正常工作。一旦网络拆分得到解决，单个节点将重新加入集群，并重新开始服务请求。 此设置应遵循一个公式：1（master_eligible_nodes / 2）+ 1 换句话说，如果有三个主资格节点，则最小主节点应设置为(3 / 2) + 1或2：即：1discovery.zen.minimum_master_nodes：2 此设置还可以动态地改变123456PUT _cluster/settings&#123; "transient": &#123; "discovery.zen.minimum_master_nodes": 2 &#125;&#125; 小提示分开专用master节点和数据角色的一个优点是，如果你可以有只有三个主节点资格，并设置minimum_master_nodes到2。你不用再更改此设置，无论你有多少个专用的数据节点添加到集群。 数据节点数据节点保留着包含已索引文档的分片。数据节点处理相关操作，比如CRUD，搜索和汇总数据。这些操作是I / O-，内存 - 和CPU密集型。监视这些资源，以及当他们超载时添加更多的数据节点是很重要的。 具有专用数据节点的主要优点是master和数据节点角色的分离。要创建一个专用的数据节点，设置：123node.master: false node.data: true node.ingest: false 摄取节点摄取节点可以执行预处理管道，由一个或多个摄取处理器组成。根据摄取处理器和所需的资源执行的操作的类型决定，有专门的摄取节点是有意义的，只会执行这个特定的任务。要创建一个专用的采集节点，设置：1234node.master: false node.data: false node.ingest: true search.remote.connect: false 只协调节点如果你去掉 到能master责任，去掉保持数据的能力，去掉预处理文件，那么你就留下了一个协调节点，它只能路由请求，处理搜索收集阶段，并分发批量索引。从本质上讲，只协调节点表现为智能负载均衡。 只协调节点对大集群有益，他们加入集群，收到完整的群集状态，像其他的节点，它们使用群集状态直接路由请求到适当位置。 警告加入过多的协调会提高整个集群的负担，因为master节点，必须从每个节点等待集群状态更新！只协调节点的好处不应该被夸大 - 数据节点可以达到同样的目的。要创建一个专门的协调节点，设置：1234node.master: false node.data: false node.ingest: false search.remote.connect: false 节点的数据路径设置 path.data 每个数据和主合格的节点需要访问其中分片和索引和集群的元数据将被存储在这个数据目录。path.data默认是$ES_HOME/data，但可以在被配置elasticsearch.yml的配置文件的绝对路径或相对于一路径$ES_HOME如下：1path.data: /var/elasticsearch/data 像所有的节点设置，也可以在命令行上指定的：1./bin/elasticsearch -Epath.data=/var/elasticsearch/data 小提示当使用.zip或.tar.gz分发版，path.data设置应配置为定位Elasticsearch主目录之外的数据目录，从而使主目录可以删除，但不删除您的数据！rpm和Debian发行版为你已经这样做了。 node.max_local_storage_nodes path.data可以由多个节点共享，即使是不同的集群的节点。这对你在开发机器上测试故障转移和不同的配置是非常有用的。在生产环境中，建议每个服务器上运行一个节点Elasticsearch的。 默认情况下，Elasticsearch被构造成防止多于一个的节点共享同一数据路径。要允许多个节点（例如，在开发计算机上），使用设置 node.max_local_storage_nodes和将其设置为一个正整数大于一。 警告不要运行来自相同数据目录的不同的节点类型（例如master和data节点）。这会导致意外的数据丢失。 其他节点设置与X-包节点设置 略（本文完）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM性能调优经验总结]]></title>
      <url>%2Fpost%2Fjava-jvm-optimization%2F</url>
      <content type="text"><![CDATA[说明调优是一个循序渐进的过程，必然需要经历多次迭代，最终才能换取一个较好的折中方案。 在JVM调优这个领域，没有任何一种调优方案是适用于所有应用场景的，同时，切勿极端才能够达到JVM性能调优的真正目的和意义。 调优策略核心目标 GC的时间足够的小 GC的次数足够的少 发生Full GC的周期足够的长 前两个目前是相悖的，要想GC时间小必须要一个更小的堆，要保证GC次数足够少，必须保证一个更大的堆，我们只能取其平衡。 更大的年轻代必然导致更小的年老代，大的年轻代会延长普通GC的周期，但会增加每次GC的时间；小的年老代会导致更频繁的Full GC 更小的年轻代必然导致更大年老代，小的年轻代会导致普通GC很频繁，但每次的GC时间会更短；大的年老代会减少Full GC的频率 如何选择应该依赖应用程序对象生命周期的分布情况：如果应用存在大量的临时对象，应该选择更大的年轻代；如果存在相对较多的持久对象，年老代应该适当增大。 但很多应用都没有这样明显的特性，在抉择时应该根据以下两点：a、本着Full GC尽量少的原则，让年老代尽量缓存常用对象，JVM的默认比例3：8也是这个道理（B）通过观察应用一段时间，看其他在峰值时年老代会占多少内存，在不影响FullGC的前提下，根据实际情况加大年轻代，比如可以把比例控制在1：1。但应该给年老代至少预留1/3的增长空间 将新对象预留在年轻代众所周知，由于 Full GC 的成本远远高于 Minor GC，因此某些情况下需要尽可能将对象分配在年轻代，这在很多情况下是一个明智的选择。虽然在大部分情况下，JVM 会尝试在 Eden 区分配对象，但是由于空间紧张等问题，很可能不得不将部分年轻对象提前向年老代压缩。因此，在 JVM 参数调优时可以为应用程序分配一个合理的年轻代空间，以最大限度避免新对象直接进入年老代的情况发生。 通过设置一个较大的年轻代预留新对象，设置合理的 Survivor 区并且提供 Survivor 区的使用率，可以将年轻对象保存在年轻代。一般来说，Survivor 区的空间不够，或者占用量达到 50%时，就会使对象进入年老代(不管它的年龄有多大) 我们可以尝试加上1-XX:TargetSurvivorRatio=90 参数，这样可以提高 from 区的利用率，使 from 区使用到 90%时，再将对象送入年老代 让大对象进入年老代我们在大部分情况下都会选择将对象分配在年轻代。但是，对于占用内存较多的大对象而言，它的选择可能就不是这样的。因为大对象出现在年轻代很可能扰乱年轻代 GC，并破坏年轻代原有的对象结构。因为尝试在年轻代分配大对象，很可能导致空间不足，为了有足够的空间容纳大对象，JVM 不得不将年轻代中的年轻对象挪到年老代。因为大对象占用空间多，所以可能需要移动大量小的年轻对象进入年老代，这对 GC 相当不利。 基于以上原因，可以将大对象直接分配到年老代，保持年轻代对象结构的完整性，这样可以提高 GC 的效率。如果一个大对象同时又是一个短命的对象，假设这种情况出现很频繁，那对于 GC 来说会是一场灾难。原本应该用于存放永久对象的年老代，被短命的对象塞满，这也意味着对堆空间进行了洗牌，扰乱了分代内存回收的基本思路。因此，在软件开发过程中，应该尽可能避免使用短命的大对象。 可以使用参数1-XX:PetenureSizeThreshold 设置大对象直接进入年老代的阈值。当对象的大小超过这个值时，将直接在年老代分配。参数-XX:PetenureSizeThreshold 只对串行收集器和年轻代并行收集器有效，并行回收收集器不识别这个参数。 设置对象进入年老代的年龄如何设置对象进入年老代的年龄堆中的每一个对象都有自己的年龄。一般情况下，年轻对象存放在年轻代，年老对象存放在年老代。为了做到这点，虚拟机为每个对象都维护一个年龄。如果对象在 Eden 区，经过一次 GC 后依然存活，则被移动到 Survivor 区中，对象年龄加 1。以后，如果对象每经过一次 GC 依然存活，则年龄再加 1。当对象年龄达到阈值时，就移入年老代，成为老年对象。 这个阈值的最大值可以通过参数1-XX:MaxTenuringThreshold 来设置，默认值是 15。虽然-XX:MaxTenuringThreshold 的值可能是 15 或者更大，但这不意味着新对象非要达到这个年龄才能进入年老代。事实上，对象实际进入年老代的年龄是虚拟机在运行时根据内存使用情况动态计算的，这个参数指定的是阈值年龄的最大值。即，实际晋升年老代年龄等于动态计算所得的年龄与-XX:MaxTenuringThreshold 中较小的那个。 稳定的 Java 堆 VS 动荡的 Java 堆一般来说，稳定的堆大小对垃圾回收是有利的。获得一个稳定的堆大小的方法是使-Xms 和-Xmx 的大小一致，即最大堆和最小堆 (初始堆)一样 如果这样设置，系统在运行时堆大小理论上是恒定的，稳定的堆空间可以减少 GC 的次数。因此，很多服务端应用都会将最大堆和最小堆设置为相同的数值。但是，一个不稳定的堆并非毫无用处。稳定的堆大小虽然可以减少 GC 次数，但同时也增加了每次GC的时间。让堆大小在一个区间中震荡，在系统不需要使用大内存时，压缩堆空间，使 GC 应对一个较小的堆，可以加快单次 GC 的速度。基于这样的考虑，JVM 还提供了两个参数用于压缩和扩展堆空间。 1-XX:MinHeapFreeRatio 参数用来设置堆空间最小空闲比例，默认值是 40。当堆空间的空闲内存小于这个数值时，JVM 便会扩展堆空间。1-XX:MaxHeapFreeRatio 参数用来设置堆空间最大空闲比例，默认值是 70。当堆空间的空闲内存大于这个数值时，便会压缩堆空间，得到一个较小的堆。 当-Xmx 和-Xms 相等时，-XX:MinHeapFreeRatio 和-XX:MaxHeapFreeRatio 两个参数无效。 增大吞吐量提升系统性能吞吐量优先的方案将会尽可能减少系统执行垃圾回收的总时间，故可以考虑关注系统吞吐量的并行回收收集器。在拥有高性能的计算机上，进行吞吐量优先优化，可以使用参数： 12java –Xmx3800m –Xms3800m –Xmn2G –Xss128k –XX:+UseParallelGC –XX:ParallelGC-Threads=20 –XX:+UseParallelOldGC –Xmx380m –Xms3800m：设置 Java 堆的最大值和初始值。一般情况下，为了避免堆内存的频繁震荡，导致系统性能下降，我们的做法是设置最大堆等于最小堆。假设这里把最小堆减少为最大堆的一半，即 1900m，那么 JVM 会尽可能在 1900MB 堆空间中运行，如果这样，发生 GC 的可能性就会比较高； -Xss128k：减少线程栈的大小，这样可以使剩余的系统内存支持更多的线程； -Xmn2g：设置年轻代区域大小为 2GB； –XX:+UseParallelGC：年轻代使用并行垃圾回收收集器。这是一个关注吞吐量的收集器，可以尽可能地减少 GC 时间。 –XX:ParallelGC-Threads：设置用于垃圾回收的线程数，通常情况下，可以设置和 CPU 数量相等。但在 CPU 数量比较多的情况下，设置相对较小的数值也是合理的； –XX:+UseParallelOldGC：设置年老代使用并行回收收集器。 尝试使用大的内存分页CPU 是通过寻址来访问内存的。32 位 CPU 的寻址宽度是 0~0xFFFFFFFF ，计算后得到的大小是 4G，也就是说可支持的物理内存最大是 4G。但在实践过程中，碰到了这样的问题，程序需要使用 4G 内存，而可用物理内存小于 4G，导致程序不得不降低内存占用。为了解决此类问题，现代 CPU 引入了 MMU（Memory Management Unit 内存管理单元）。MMU 的核心思想是利用虚拟地址替代物理地址，即 CPU 寻址时使用虚址，由 MMU 负责将虚址映射为物理地址。MMU 的引入，解决了对物理内存的限制，对程序来说，就像自己在使用 4G 内存一样。内存分页 (Paging) 是在使用 MMU 的基础上，提出的一种内存管理机制。它将虚拟地址和物理地址按固定大小（4K）分割成页 (page) 和页帧 (page frame)，并保证页与页帧的大小相同。这种机制，从数据结构上，保证了访问内存的高效，并使 OS 能支持非连续性的内存分配。在程序内存不够用时，还可以将不常用的物理内存页转移到其他存储设备上，比如磁盘，这就是大家耳熟能详的虚拟内存。 在 Solaris 系统中，JVM 可以支持 Large Page Size 的使用。使用大的内存分页可以增强 CPU 的内存寻址能力，从而提升系统的性能。 12java –Xmx2506m –Xms2506m –Xmn1536m –Xss128k –XX:++UseParallelGC –XX:ParallelGCThreads=20 –XX:+UseParallelOldGC –XX:+LargePageSizeInBytes=256m –XX:+LargePageSizeInBytes：设置大页的大小。 过大的内存分页会导致 JVM 在计算 Heap 内部分区（perm, new, old）内存占用比例时，会出现超出正常值的划分，最坏情况下某个区会多占用一个页的大小。 使用非占有的垃圾回收器为降低应用软件的垃圾回收时的停顿，首先考虑的是使用关注系统停顿的 CMS 回收器，其次，为了减少 Full GC 次数，应尽可能将对象预留在年轻代，因为年轻代 Minor GC 的成本远远小于年老代的 Full GC。 123java –Xmx3550m –Xms3550m –Xmn2g –Xss128k –XX:ParallelGCThreads=20 –XX:+UseConcMarkSweepGC –XX:+UseParNewGC –XX:+SurvivorRatio=8 –XX:TargetSurvivorRatio=90 –XX:MaxTenuringThreshold=31 –XX:ParallelGCThreads=20：设置 20 个线程进行垃圾回收； –XX:+UseParNewGC：年轻代使用并行回收器； –XX:+UseConcMarkSweepGC：年老代使用 CMS 收集器降低停顿； –XX:+SurvivorRatio：设置 Eden 区和 Survivor 区的比例为 8:1。稍大的 Survivor 空间可以提高在年轻代回收生命周期较短的对象的可能性，如果 Survivor 不够大，一些短命的对象可能直接进入年老代，这对系统来说是不利的。 –XX:TargetSurvivorRatio=90：设置 Survivor 区的可使用率。这里设置为 90%，则允许 90%的 Survivor 空间被使用。默认值是 50%。故该设置提高了 Survivor 区的使用率。当存放的对象超过这个百分比，则对象会向年老代压缩。因此，这个选项更有助于将对象留在年轻代。 –XX:MaxTenuringThreshold：设置年轻对象晋升到年老代的年龄。默认值是 15 次，即对象经过 15 次 Minor GC 依然存活，则进入年老代。这里设置为 31，目的是让对象尽可能地保存在年轻代区域。 一些监测JVM的命令 jps 列出正在运行的虚拟机进程 jstat 监视虚拟机运行状态信息 jmap 生成堆存储快照 jstack 生成虚拟机当前时刻的线程快照，帮助定位线程出现长时间停顿的原因 一些JVM参数设定堆内存大小 -Xms：启动JVM时的堆内存空间。 -Xmx：堆内存最大限制。 -Xmn：设置年轻代大小整个堆大小=年轻代大小 + 年老代大小 + 持久代大小，Sun官方推荐配置为整个堆的3/8 -XX:PermSize=128M设置持久代大小 -XX:MaxPermSize=128M设置持久代最大值，此值可以设置与-XX:PermSize相同，防止持久代内存伸缩，持久代设置很重要，一般预留其使用空间的1/3. 设定新生代大小。 -XX:NewRatio：新生代和老年代的占比。 -XX:NewSize：新生代空间。 -XX:SurvivorRatio：伊甸园空间和幸存者空间的占比。 -XX:MaxTenuringThreshold：对象进入老年代的年龄阈值。 设定垃圾回收器 -XX:+UseSerialGC 开启串行收集器 -XX:+UseParallelGC开启年轻代并行收集器，JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值 -XX:+UseParallelOldGC开启老年代并行收集器 -XX:+UseConcMarkSweepGC开启老年代并发收集器(简称CMS)，可以和UseParallelGC一起使用 -XX:CMSInitiatingOccupancyFraction=70老年代内存使用比例到多少激活CMS收集器，这个数值的设置有很大技巧基本上满足(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn否则会出现“Concurrent Mode Failure”，promotionfailed，官方建议数值为68 -XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩 其他 -Xss： 设置每个线程的堆栈大小，设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右，这个参数对性能的影响比较大的 -XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论，linux64的java6默认值是15 -XX:ParallelGCThreads=设置并行垃圾回收的线程数。此值可以设置与机器处理器数量相等（逻辑cpu数），这个不确定是物理、还是逻辑使用默认就好 -XX:MaxGCPauseMillis=指定垃圾回收时的最长暂停时间，单位毫秒，如果指定了此值的话，堆大小和垃圾回收相关参数会进行调整以达到指定值，设定此值可能会减少应用的吞吐量 -XX:GCTimeRatio=设定吞吐量为垃圾回收时间与非垃圾回收时间的比值，公式为1/（1+N）。例如，-XX:GCTimeRatio=19时，表示5%的时间用于垃圾回收。默认情况为99，即1%的时间用于垃圾回收 -XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开 -XX:+DisableExplicitGC：禁止 java 程序中的 full gc, 如System.gc() 的调用. 最好加上， 防止程序在代码里误用了对性能造成冲击 -XX:+PrintGCDetails 打应垃圾收集的情况 -XX:+PrintGCTimeStamps 打应垃圾收集的情况 -XX:+PrintGCApplicationConcurrentTime：打印每次垃圾回收前，程序未中断的执行时间。可与上面混合使用 -XX:+PrintGCApplicationStoppedTime 打应垃圾收集时 , 系统的停顿时间 -XX:+PrintGC 打印GC情况 -XX:PrintHeapAtGC:打印GC前后的详细堆栈信息 (本文完) 参考： http://blog.csdn.net/chen77716/article/details/5695893 http://www.jianshu.com/p/c6a04c88900a https://www.ibm.com/developerworks/cn/java/j-lo-jvm-optimize-experience/index.html https://www.dexcoder.com/dexcoder/article/3159 http://buguoruci.blog.51cto.com/4104173/1287831]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM内存溢出详解（栈溢出，堆溢出，持久代溢出以及无法创建本地线程）]]></title>
      <url>%2Fpost%2Fjava-out-of-memory%2F</url>
      <content type="text"><![CDATA[写在前面内存溢出和内存泄漏的区别：内存溢出 out of memory，是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer,但给它存了long才能存下的数，那就是内存溢出。 内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。 memory leak会最终会导致out of memory 内存溢出就是你要求分配的内存超出了系统能给你的，系统不能满足需求，于是产生溢出。 内存泄漏是指你向系统申请分配内存进行使用(new)，可是使用完了以后却不归还(delete)，结果你申请到的那块内存你自己也不能再访问（也许你把它的地址给弄丢了），而系统也不能再次将它分配给需要的程序。一个盘子用尽各种方法只能装4个果子，你装了5个，结果掉倒地上不能吃了。这就是溢出！比方说栈，栈满时再做进栈必定产生空间溢出，叫上溢，栈空时再做退栈也产生空间溢出，称为下溢。就是分配的内存不足以放下数据项序列,称为内存溢出. 全文简短总结，具体内容可以看下文。栈内存溢出(StackOverflowError)：程序所要求的栈深度过大导致，可以写一个死递归程序触发。 堆内存溢出(OutOfMemoryError:java heap space) 分清内存溢出还是内存泄漏 泄露则看对象如何被 GC Root 引用。 溢出则通过 调大 -Xms，-Xmx参数。 持久带内存溢出(OutOfMemoryError: PermGen space) 持久带中包含方法区，方法区包含常量池 因此持久带溢出有可能是运行时常量池溢出，也有可能是方法区中保存的class对象没有被及时回收掉或者class信息占用的内存超过了我们配置 用String.intern()触发常量池溢出 Class对象未被释放，Class对象占用信息过多，有过多的Class对象。可以导致持久带内存溢出 无法创建本地线程总容量不变，堆内存，非堆内存设置过大，会导致能给线程的内存不足。 以下是详细内容 栈溢出(StackOverflowError)栈溢出抛出StackOverflowError错误，出现此种情况是因为方法运行的时候栈的深度超过了虚拟机容许的最大深度所致。出现这种情况，一般情况下是程序错误所致的，比如写了一个死递归，就有可能造成此种情况。 下面我们通过一段代码来模拟一下此种情况的内存溢出。 1234567891011import java.util.*; import java.lang.*; public class OOMTest&#123; public void stackOverFlowMethod()&#123; stackOverFlowMethod(); &#125; public static void main(String... args)&#123; OOMTest oom = new OOMTest(); oom.stackOverFlowMethod(); &#125; &#125; 运行上面的代码，会抛出如下的异常：12Exception in thread "main" java.lang.StackOverflowError at OOMTest.stackOverFlowMethod(OOMTest.java:6) 对于栈内存溢出，根据《Java 虚拟机规范》中文版： 如果线程请求的栈容量超过栈允许的最大容量的话，Java 虚拟机将抛出一个StackOverflow异常；如果Java虚拟机栈可以动态扩展，并且扩展的动作已经尝试过，但是无法申请到足够的内存去完成扩展，或者在新建立线程的时候没有足够的内存去创建对应的虚拟机栈，那么Java虚拟机将抛出一个OutOfMemory 异常。 堆溢出(OutOfMemoryError:java heap space)堆内存溢出的时候，虚拟机会抛出java.lang.OutOfMemoryError:java heap space,出现此种情况的时候，我们需要根据内存溢出的时候产生的dump文件来具体分析（需要增加-XX:+HeapDumpOnOutOfMemoryErrorjvm启动参数）。出现此种问题的时候有可能是内存泄露，也有可能是内存溢出了。 如果内存泄露，我们要找出泄露的对象是怎么被GC ROOT引用起来，然后通过引用链来具体分析泄露的原因。 如果出现了内存溢出问题，这往往是程序本生需要的内存大于了我们给虚拟机配置的内存，这种情况下，我们可以采用调大-Xmx来解决这种问题。下面我们通过如下的代码来演示一下此种情况的溢出：123456789import java.util.*; import java.lang.*; public class OOMTest&#123; public static void main(String... args)&#123; List&lt;byte[]&gt; buffer = new ArrayList&lt;byte[]&gt;(); buffer.add(new byte[10*1024*1024]); &#125; &#125; 我们通过如下的命令运行上面的代码：1java -verbose:gc -Xmn10M -Xms20M -Xmx20M -XX:+PrintGC OOMTest 程序输出如下的信息：12345[GC 1180K-&gt;366K(19456K), 0.0037311 secs] [Full GC 366K-&gt;330K(19456K), 0.0098740 secs] [Full GC 330K-&gt;292K(19456K), 0.0090244 secs] Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at OOMTest.main(OOMTest.java:7) 从运行结果可以看出，JVM进行了一次Minor gc和两次的Major gc，从Major gc的输出可以看出，gc以后old区使用率为134K，而字节数组为10M，加起来大于了old generation的空间，所以抛出了异常，如果调整-Xms21M,-Xmx21M,那么就不会触发gc操作也不会出现异常了。 通过上面的实验其实也从侧面验证了一个结论：对象大于新生代剩余内存的时候，将直接放入老年代，当老年代剩余内存还是无法放下的时候，触发垃圾收集，收集后还是不能放下就会抛出内存溢出异常了。 持久带溢出(OutOfMemoryError: PermGen space)我们知道Hotspot jvm通过持久带实现了Java虚拟机规范中的方法区，而运行时的常量池就是保存在方法区中的，因此持久带溢出有可能是运行时常量池溢出，也有可能是方法区中保存的class对象没有被及时回收掉或者class信息占用的内存超过了我们配置。当持久带溢出的时候抛出java.lang.OutOfMemoryError: PermGen space。可能在如下几种场景下出现： 使用一些应用服务器的热部署的时候，我们就会遇到热部署几次以后发现内存溢出了，这种情况就是因为每次热部署的后，原来的class没有被卸载掉。 如果应用程序本身比较大，涉及的类库比较多，但是我们分配给持久带的内存（通过-XX:PermSize和-XX:MaxPermSize来设置）比较小的时候也可能出现此种问题。 一些第三方框架，比如spring,hibernate都通过字节码生成技术（比如CGLib）来实现一些增强的功能，这种情况可能需要更大的方法区来存储动态生成的Class文件。我们知道Java中字符串常量是放在常量池中的，String.intern()这个方法运行的时候，会检查常量池中是否存和本字符串相等的对象，如果存在直接返回对常量池中对象的引用，不存在的话，先把此字符串加入常量池，然后再返回字符串的引用。那么我们就可以通过String.intern方法来模拟一下运行时常量区的溢出.下面我们通过如下的代码来模拟此种情况： 12345678910import java.util.*; import java.lang.*; public class OOMTest&#123; public static void main(String... args)&#123; List&lt;String&gt; list = new ArrayList&lt;String&gt;(); while(true)&#123; list.add(UUID.randomUUID().toString().intern()); &#125; &#125; &#125; 我们通过如下的命令运行上面代码：1java -verbose:gc -Xmn5M -Xms10M -Xmx10M -XX:MaxPermSize=1M -XX:+PrintGC OOMTest 运行后的输入如下图所示:123Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: PermGen space at java.lang.String.intern(Native Method) at OOMTest.main(OOMTest.java:8) 通过上面的代码，我们成功模拟了运行时常量池溢出的情况，从输出中的PermGen space可以看出确实是持久带发生了溢出，这也验证了，我们前面说的Hotspot jvm通过持久带来实现方法区的说法。 OutOfMemoryError:unable to create native thread最后我们在来看看java.lang.OutOfMemoryError:unable to create natvie thread这种错误。 出现这种情况的时候，一般是下面两种情况导致的： 程序创建的线程数超过了操作系统的限制。对于Linux系统，我们可以通过ulimit -u来查看此限制。 给虚拟机分配的内存过大，导致创建线程的时候需要的native内存太少。 我们都知道操作系统对每个进程的内存是有限制的，我们启动Jvm,相当于启动了一个进程，假如我们一个进程占用了4G的内存，那么通过下面的公式计算出来的剩余内存就是建立线程栈的时候可以用的内存。线程栈总可用内存=4G-（-Xmx的值）- （-XX:MaxPermSize的值）- 程序计数器占用的内存通过上面的公式我们可以看出，-Xmx 和 MaxPermSize的值越大，那么留给线程栈可用的空间就越小，在-Xss参数配置的栈容量不变的情况下，可以创建的线程数也就越小。因此如果是因为这种情况导致的unable to create native thread,那么要么我们增大进程所占用的总内存，或者减少-Xmx或者-Xss来达到创建更多线程的目的。 （本文完）参考： https://github.com/pzxwhc/MineKnowContainer/issues/25 http://blog.csdn.net/u011983531/article/details/63250882]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM内存管理-垃圾回收与内存分配]]></title>
      <url>%2Fpost%2Fjava-memory-manage%2F</url>
      <content type="text"><![CDATA[Java技术体系中所提倡的自动内存管理最终可以归结为自动化地解决了两个问题：给对象分配内存以及回收分配给对象的内存。 Java垃圾收集哪些内存需要回收？线程私有区的程序计数器、虚拟机栈和本地方法栈不需要，重点是共享数据区的堆和方法区部分的内存 什么时候回收？判断对象是否存活的算法？引用计数法 逻辑：给对象添加一个引用计数器，每当有一个地方引用它时，计数器值就加1，当引用失效时，计数器值就减1，任何时刻计数器为0的对象就是不可能再被使用的。 优点：实现简单，效率高 缺点：没有解决互相循环引用问题 Java虚拟机并没有选择这种算法来进行垃圾回收 可达性分析算法 逻辑：这种算法的基本思路是通过一系列名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连时，就证明此对象是不可用的。 Java语言是通过可达性分析算法来判断对象是否存活的。 在Java语言里，可作为GC Roots的对象包括下面几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象。方法区中的类静态属性引用的对象。方法区中的常量引用的对象。本地方法栈中JNI（Native方法）的引用对象。 正确理解引用，java对象有哪几种引用类型？强引用，软引用，弱引用，虚引用 对死亡的标记过程即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，至少要经历两次标记过程。 回收方法区很多人认为方法区（或者HotSpot虚拟机中的永久代）是没有垃圾收集的，Java虚拟机规范中确实说过可以不要求虚拟机在方法区实现垃圾收集，而且在方法区中进行垃圾收集的“性价比”一般比较低：在堆中，尤其是在新生代中，常规应用进行一次垃圾收集一般可以回收70%～95%的空间，而永久代的垃圾收集效率远低于此 永久代的垃圾收集主要回收两部分内容：废弃常量和无用的类。 如何回收？有哪些回收算法？ 标记-清除算法 算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。缺点：效率低,标记清除后会产生大量不连续的内存，可能会导致以后程序程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法 它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。优点：简单高效缺点：代价是将内存缩小为原来的一半，代价高 现在的商业虚拟机都采用这种收集算法来回收新生代，研究表明，新生代中的对象98%是“朝生夕死”的，所以并不需要按照1∶1的比例来划分内存空间，而是将内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一块Survivor空间上，最后清理掉Eden和刚才用过的Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也就是每次新生代中可用内存空间为整个新生代容量的90%，只有10%的内存会被“浪费”。当然，90%的对象可回收只是一般场景下的数据，我们没有办法保证每次回收都只有不多于10%的对象存活，当Survivor空间不够用时，需要依赖其他内存（这里指老年代）进行分配担保（Handle Promotion）。 标记-整理算法 标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法 当前商业虚拟机的垃圾收集都采用“分代收集”（Generational Collection）算法，这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。 在老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用”标记—清理”或者”标记—整理”算法来进行回收。 Java垃圾收集器概念理解 并发和并行 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个CPU上。 Minor GC 和 Full GC？ 新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以Minor GC非常频繁，一般回收速度也比较快。 老年代GC（Major GC / Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC（但非绝对的，在Parallel Scavenge收集器的收集策略里就有直接进行Major GC的策略选择过程）。Major GC的速度一般会比Minor GC慢10倍以上。 收集器Serial收集器 最基本、发展历史最悠久的收集器 单线程，必须暂停其他所有的工作线程，直到它收集结束 Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。 简单高效 ParNew收集器 Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外其他和Serial收集器完全一样 Parallel Scavenge收集器 是一个新生代收集器，它也是使用复制算法的收集器，又是并行的多线程收集器。 Serial Old收集器 Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记－整理算法。 Parallel Old收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。 CMS收集器CMS收集器是基于“标记—清除”算法实现的，从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 G1收集器并行与并发，分代收集， 空间整合，可预测的停顿 Java对象内存分配策略本文中的内存分配策略指的是Serial / Serial Old收集器下（ParNew / Serial Old收集器组合的规则也基本一致）的内存分配和回收的策略 对象优先在Eden分配大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC。 大对象直接进入老年代-XX:PretenureSizeThreshold参数虚拟机提供了一个-XX:PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制（复习一下：新生代采用复制算法收集内存）。 长期存活的对象将进入老年代对象年龄的判定:如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就将会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold设置。 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。 （本文完） 整理自： http://www.jianshu.com/p/424e12b3a08f http://www.jianshu.com/p/50d5c88b272d]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-ArrayList快速失败机制/CopyOnWriteArrayList/扩容]]></title>
      <url>%2Fpost%2Fjava-arrayList%2F</url>
      <content type="text"><![CDATA[1.迭代ArrayList时做add或remove操作会发生什么？答案会抛出 java.util.ConcurrentModificationException 解决方法 对JAVA集合进行遍历删除时务必要用迭代器 使用CopyOnWriteArrayList 总结： 对于ArrayList，在使用Iterator遍历时，不能使用list.add()、list.remove()等改变list的操作，只能用it.remove()原因是ArrayList不是线程安全的，需在单线程环境下使用，如果在遍历时还有别的线程做增删操作，必然会有问题，如数组下标越界ArrayList#Iterator设计的是不能在迭代时有别的线程对list修改，此种修改对当前迭代器是可能存在问题的，所以增加了对modCount的校验但当前迭代器可以remove，因为它自己删除就不是并发修改了迭代器remove会重置expectedModCount，并将cursor往前一位 CopyOnWriteArrayList在使用Iterator遍历时，可以用list.add()，list.remove()等改变list的操作，但不支持it.remove()首先CopyOnWriteArrayList是用于多线程环境下的因为CopyOnWriteArrayList的Iterator实现类COWIterator会在创建时复制一份list的副本，之后迭代的是副本所以期间怎么对list.add()，list.remove()都没事，list.remove()操作的是原始的list但不支持it.remove()因为Iterator中的本身就是副本，删除副本中的元素没意义，如果去删除原始list，在并发环境下此时list可能和创建迭代器时的副本已经完全不同了 快速失败（fast-fail）机制快速失败”即fail-fast,它是java集合的一种错误检测机制。当多钱程对集合进行结构上的改变或者集合在迭代元素时直接调用自身方法改变集合结构而没有通知迭代器时，有可能会触发fast-fail机制并抛出异常。需要注意一点，有可能触发fast-fail机制而不是肯定。触发时机是在迭代过程中，集合的结构发生了变化而此时迭代器并不知道或者还没来得及反应时便会产生fail-fast机制。这里再次强调，迭代器的快速失败行为无法得到保证，因为一般来说，不可能对是否出现不同步并发修改或者自身修改做出任何硬性保证。快速失败迭代器会尽最大努力抛出 ConcurrentModificationException。 2.CopyOnWriteArrayList概述Copy-On-Write简称COW，是一种用于程序设计中的优化策略。其基本思路是，从一开始大家都在共享同一个内容，当某个人想要修改这个内容的时候，才会真正把内容Copy出去形成一个新的内容然后再改，这是一种延时懒惰策略。从JDK1.5开始Java并发包里提供了两个使用CopyOnWrite机制实现的并发容器,它们是CopyOnWriteArrayList和CopyOnWriteArraySet。CopyOnWrite容器非常有用，可以在非常多的并发场景中使用到。 什么是CopyOnWrite容器CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。 实现原理在使用CopyOnWriteArrayList之前，我们先阅读其源码了解下它是如何实现的。以下代码是向CopyOnWriteArrayList中add方法的实现（向CopyOnWriteArrayList里添加元素），可以发现在添加的时候是需要加锁的，否则多线程写的时候会Copy出N个副本出来。1234567891011121314151617181920/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */ public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125; &#125; 读的时候不需要加锁，如果读的时候有多个线程正在向CopyOnWriteArrayList添加数据，读还是会读到旧的数据，因为写的时候不会锁住旧的CopyOnWriteArrayList。123public E get(int index) &#123; return get(getArray(), index);&#125; JDK中并没有提供CopyOnWriteMap，我们可以参考CopyOnWriteArrayList来实现一个，基本代码如下：123456789101112131415161718192021222324252627282930313233import java.util.Collection;import java.util.Map;import java.util.Set; public class CopyOnWriteMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable &#123; private volatile Map&lt;K, V&gt; internalMap; public CopyOnWriteMap() &#123; internalMap = new HashMap&lt;K, V&gt;(); &#125; public V put(K key, V value) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); V val = newMap.put(key, value); internalMap = newMap; return val; &#125; &#125; public V get(Object key) &#123; return internalMap.get(key); &#125; public void putAll(Map&lt;? extends K, ? extends V&gt; newData) &#123; synchronized (this) &#123; Map&lt;K, V&gt; newMap = new HashMap&lt;K, V&gt;(internalMap); newMap.putAll(newData); internalMap = newMap; &#125; &#125;&#125; 实现很简单，只要了解了CopyOnWrite机制，我们可以实现各种CopyOnWrite容器，并且在不同的应用场景中使用。 CopyOnWrite的应用场景CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。这些不能被搜索的关键字会被放在一个黑名单当中，黑名单每天晚上更新一次。当用户搜索时，会检查当前关键字在不在黑名单当中，如果在，则提示不能搜索。实现1234567891011121314151617181920212223242526272829303132333435package com.ifeve.book; import java.util.Map; import com.ifeve.book.forkjoin.CopyOnWriteMap; /** * 黑名单服务 * * @author fangtengfei * */public class BlackListServiceImpl &#123; private static CopyOnWriteMap&lt;String, Boolean&gt; blackListMap = new CopyOnWriteMap&lt;String, Boolean&gt;( 1000); public static boolean isBlackList(String id) &#123; return blackListMap.get(id) == null ? false : true; &#125; public static void addBlackList(String id) &#123; blackListMap.put(id, Boolean.TRUE); &#125; /** * 批量添加黑名单 * * @param ids */ public static void addBlackList(Map&lt;String,Boolean&gt; ids) &#123; blackListMap.putAll(ids); &#125; &#125; 代码很简单，但是使用CopyOnWriteMap需要注意两件事情： 1. 减少扩容开销。根据实际需要，初始化CopyOnWriteMap的大小，避免写时CopyOnWriteMap扩容的开销。 2. 使用批量添加。因为每次添加，容器每次都会进行复制，所以减少添加次数，可以减少容器的复制次数。如使用上面代码里的addBlackList方法。 CopyOnWrite的缺点CopyOnWrite容器有很多优点，但是同时也存在两个问题，即内存占用问题和数据一致性问题。所以在开发的时候需要注意一下。 内存占用问题因为CopyOnWrite的写时复制机制，所以在进行写操作的时候，内存里会同时驻扎两个对象的内存，旧的对象和新写入的对象（注意:在复制的时候只是复制容器里的引用，只是在写的时候会创建新对象添加到新容器里，而旧容器的对象还在使用，所以有两份对象内存）。如果这些对象占用的内存比较大，比如说200M左右，那么再写入100M数据进去，内存就会占用300M，那么这个时候很有可能造成频繁的Yong GC和Full GC。之前我们系统中使用了一个服务由于每晚使用CopyOnWrite机制更新大对象，造成了每晚15秒的Full GC，应用响应时间也随之变长。 针对内存占用问题，可以通过压缩容器中的元素的方法来减少大对象的内存消耗，比如，如果元素全是10进制的数字，可以考虑把它压缩成36进制或64进制。或者不使用CopyOnWrite容器，而使用其他的并发容器，如ConcurrentHashMap。 数据一致性问题CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性。所以如果你希望写入的的数据，马上能读到，请不要使用CopyOnWrite容器。 下面这篇文章验证了CopyOnWriteArrayList和同步容器的性能：http://blog.csdn.net/wind5shy/article/details/5396887下面这篇文章简单描述了CopyOnWriteArrayList的使用：http://blog.csdn.net/imzoer/article/details/9751591 扩容 以数组实现。节约空间，但数组有容量限制。超出限制时会增加50%容量，用System.arraycopy()复制到新的数组，因此最好能给出数组大小的预估值。默认第一次插入元素时创建大小为10的数组。 参考 http://www.cnblogs.com/caiyao/p/4963206.html http://www.qingpingshan.com/m/view.php?aid=219803 http://blog.csdn.net/smcwwh/article/details/7036663 http://www.cnblogs.com/dolphin0520/p/3938914.html http://yikun.github.io/2015/04/04/Java-ArrayList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/ http://www.cnblogs.com/jeffwongishandsome/archive/2012/11/06/2753054.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch5.5.0源码-编译、导入IDEA、启动]]></title>
      <url>%2Fpost%2Felasticsearch5-5-0-run%2F</url>
      <content type="text"><![CDATA[1.环境与准备工作 WINDOWS 10 IDEA 17.2 JDK 1.8.0_141 Gradle 4.0.1 2.获取代码gitlab地址：elasticsearch 复制到sourcetree中，tag选择 v5.5.0 如果不使用sourcetree可以使用git命令操作1git checkout v5.5.0 3.编译代码到代码根目录 elasticsearch下面运行：1gradle idea 等待漫长的依赖下载过程后，运行：1gradle build -x test 对源码进行编译 4.idea打开项目 配置好gradle的文件地址，其他的直接使用默认设置，直接next 文件结构是这样的： 5.启动main函数打开类：org.elasticsearch.bootstrap.Elasticsearch运行里面的main函数需要添加一些启动参数： -Des.path.home指定es的目录地址，elasticsearch在启动中会加载一些默认配置以及插件,我们直接加载elasticsearch安装目录下的配置和插件即可 如果不配置会报错：Exception in thread “main” java.lang.IllegalStateException: path.home is not configured -Dlog4j2.disable.jmx=true为elasticsearch在启动过程中使用到了jmx,我们这里禁止使用即可 如果不配置会报错：main ERROR Could not register mbeans java.security.AccessControlException: access denied (“javax.management.MBeanTrustPermission” “register”) 注意 如果报错没有找到config 如果报错没有找到 plugins 如果报错：java.lang.IllegalStateException: Unsupported transport.type [] 解决方案：去官网下个zip包：https://www.elastic.co/downloads/elasticsearch解压后把里面config、modules、plugins 复制到elasticsearch/core里面 6.运行main方法控制台看到如下说明启动成功 7.浏览器访问http://localhost:9200]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-ThreadPoolExecutor]]></title>
      <url>%2Fpost%2Fjava-ThreadPoolExecutor%2F</url>
      <content type="text"><![CDATA[概述从Java5开始，Java提供了自己的线程池。每次只执行指定数量的线程，java.util.concurrent.ThreadPoolExecutor 就是这样的线程池 构造函数1public ThreadPoolExecutor(int corePoolSize,int maximumPoolSize,long keepAliveTime,TimeUnit unit,BlockingQueue&lt;Runnable&gt; workQueue,RejectedExecutionHandler handler); 参数介绍corePoolSize核心线程数，指保留的线程池大小（不超过maximumPoolSize值时，线程池中最多有corePoolSize 个线程工作）。 maximumPoolSize指的是线程池的最大大小（线程池中最大有corePoolSize 个线程可运行）。 keepAliveTime指的是空闲线程结束的超时时间（当一个线程不工作时，过keepAliveTime 长时间将停止该线程）。 workQueue表示存放任务的队列（存放需要被线程池执行的线程队列）。 拒绝策略（添加任务失败后如何处理该任务) 线程池刚创建时，里面没有一个线程。任务队列是作为参数传进来的。不过，就算队列里面有任务，线程池也不会马上执行它们。2.当调用 execute() 方法添加一个任务时，线程池会做如下判断：2.1. 如果正在运行的线程数量小于 corePoolSize，那么马上创建线程运行这个任务；2.2. 如果正在运行的线程数量大于或等于 corePoolSize，那么将这个任务放入队列。2.3. 如果这时候队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么还是要创建线程运行这个任务；2.4. 如果队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会抛出异常，告诉调用者“我不能再接受任务了”。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做，超过一定的时间（keepAliveTime）时，线程池会判断，如果当前运行的线程数大于 corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后，它最终会收缩到 corePoolSize 的大小。 说明 这个过程说明，并不是先加入任务就一定会先执行。 假设队列大小为 4，corePoolSize为2，maximumPoolSize为6，那么当加入15个任务时，执行的顺序类似这样：首先执行任务 1、2，然后任务3~6被放入队列。这时候队列满了，任务7、8、9、10 会被马上执行，而任务 11~15 则会抛出异常。最终顺序是：1、2、7、8、9、10、3、4、5、6。 当然这个过程是针对指定大小的ArrayBlockingQueue来说，如果是LinkedBlockingQueue，因为该队列无大小限制，所以不存在上述问题。 三种方法将线程添加到线程队列对于 java.util.concurrent.BlockingQueue 类有有三种方法将线程添加到线程队列里面，然而如何区别三种方法的不同呢，其实在队列未满的情况下结果相同，都是将线程添加到线程队列里面，区分就在于当线程队列已经满的时候，此时 public boolean add(E e) 方法将抛出IllegalStateException异常，说明队列已满。 public boolean offer(E e) 方法则不会抛异常，只会返回boolean值，告诉你添加成功与否，队列已满，当然返回false。 public void put(E e) throws InterruptedException 方法则一直阻塞（即等待，直到线程池中有线程运行完毕，可以加入队列为止）。 线程池的排除策略 如果运行的线程少于 corePoolSize，则 Executor 始终首选添加新的线程，而不进行排队。 如果运行的线程等于或多于 corePoolSize，则 Executor 始终首选将请求加入队列，而不添加新的线程。 如果无法将请求加入队列，则创建新的线程，除非创建此线程超出 maximumPoolSize，在这种情况下，任务将被拒绝。 总结 线程池可立即运行的最大线程数 即maximumPoolSize 参数。 线程池能包含的最大线程数 = 可立即运行的最大线程数 + 线程队列大小 (一部分立即运行，一部分装队列里等待) 核心线程数可理解为建议值，即建议使用的线程数，或者依据CPU核数 add，offer，put三种添加线程到队列的方法只在队列满的时候有区别，add为抛异常，offer返回boolean值，put直到添加成功为止。5.同理remove，poll， take三种移除队列中线程的方法只在队列为空的时候有区别， remove为抛异常，poll为返回boolean值， take等待直到有线程可以被移除。 参考： http://blog.csdn.net/shixing_11/article/details/7109471]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-几种线程同步的方法]]></title>
      <url>%2Fpost%2Fjava-thread-synchronize%2F</url>
      <content type="text"><![CDATA[同步方法即有synchronized关键字修饰的方法。 由于java的每个对象都有一个内置锁，当用此关键字修饰方法时，内置锁会保护整个方法。在调用该方法前，需要获得内置锁，否则就处于阻塞状态。synchronized关键字也可以修饰静态方法，此时如果调用该静态方法，将会锁住整个方法 同步代码块即有synchronized关键字修饰的语句块。被该关键字修饰的语句块会自动被加上内置锁，从而实现同步注：同步是一种高开销的操作，因此应该尽量减少同步的内容。通常没有必要同步整个方法，使用synchronized代码块同步关键代码即可。 使用特殊域变量(Volatile)实现线程同步 volatile关键字为域变量的访问提供了一种免锁机制 使用volatile修饰域相当于告诉虚拟机该域可能会被其他线程更新 因此每次使用该域就要重新计算，而不是使用寄存器中的值 volatile不会提供任何原子操作，它也不能用来修饰final类型的变量 就是因为volatile不能保证原子操作导致的，因此volatile不能代替synchronized。此外volatile会组织编译器对代码优化，因此能不使用它就不适用它吧。它的原理是每次要线程要访问volatile修饰的变量时都是从内存中读取，而不是存缓存当中读取，因此每个线程访问到的变量值都是一样的。这样就保证了同步。 使用重入锁实现线程同步在JavaSE5.0中新增了一个java.util.concurrent包来支持同步。ReentrantLock类是可重入、互斥、实现了Lock接口的锁， 它与使用synchronized方法和快具有相同的基本行为和语义，并且扩展了其能力。ReenreantLock类的常用方法有： ReentrantLock() : 创建一个ReentrantLock实例 lock() : 获得锁 unlock() : 释放锁如果synchronized关键字能满足用户的需求，就用synchronized，因为它能简化代码 。如果需要更高级的功能，就用ReentrantLock类，此时要注意及时释放锁，否则会出现死锁，通常在finally代码释放锁 使用局部变量实现线程同步如果使用ThreadLocal管理变量，则每一个使用该变量的线程都获得该变量的副本，副本之间相互独立，这样每一个线程都可以随意修改自己的变量副本，而不会对其他线程产生影响。现在明白了吧，原来每个线程运行的都是一个副本，也就是说存钱和取钱是两个账户，知识名字相同而已。所以就会发生上面的效果。ThreadLocal与同步机制 ThreadLocal与同步机制都是为了解决多线程中相同变量的访问冲突问题 前者采用以”空间换时间”的方法，后者采用以”时间换空间”的方式 参考： http://www.codeceo.com/article/java-multi-thread-sync.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-线程创建的几种方式]]></title>
      <url>%2Fpost%2Fjava-create-Thread%2F</url>
      <content type="text"><![CDATA[概述 继承Thread类创建线程类 通过实现Runnable接口创建线程类 通过Callable和Future创建线程 具体继承Thread类创建线程类 定义Thread类的子类，并重写该类的run方法，该run方法的方法体就代表了线程要完成的任务。因此把run()方法称为执行体。 创建Thread子类的实例，即创建了线程对象。 调用线程对象的start()方法来启动该线程。 1234567891011121314151617181920212223242526package com.thread; public class FirstThreadTest extends Thread&#123; int i = 0; //重写run方法，run方法的方法体就是现场执行体 public void run() &#123; for(;i&lt;100;i++)&#123; System.out.println(getName()+" "+i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0;i&lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+" : "+i); if(i==20) &#123; new FirstThreadTest().start(); new FirstThreadTest().start(); &#125; &#125; &#125; &#125; 通过Runnable接口创建线程类 定义runnable接口的实现类，并重写该接口的run()方法，该run()方法的方法体同样是该线程的线程执行体。 创建 Runnable实现类的实例，并依此实例作为Thread的target来创建Thread对象，该Thread对象才是真正的线程对象。 调用线程对象的start()方法来启动该线程。 1234567891011121314151617181920212223242526272829package com.thread; public class RunnableThreadTest implements Runnable &#123; private int i; public void run() &#123; for(i = 0;i &lt;100;i++) &#123; System.out.println(Thread.currentThread().getName()+" "+i); &#125; &#125; public static void main(String[] args) &#123; for(int i = 0;i &lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+" "+i); if(i==20) &#123; RunnableThreadTest rtt = new RunnableThreadTest(); new Thread(rtt,"新线程1").start(); new Thread(rtt,"新线程2").start(); &#125; &#125; &#125; &#125; 通过Callable和Future创建线程 创建Callable接口的实现类，并实现call()方法，该call()方法将作为线程执行体，并且有返回值。 创建Callable实现类的实例，使用FutureTask类来包装Callable对象，该FutureTask对象封装了该Callable对象的call()方法的返回值。 使用FutureTask对象作为Thread对象的target创建并启动新线程。 调用FutureTask对象的get()方法来获得子线程执行结束后的返回值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.thread; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.FutureTask; public class CallableThreadTest implements Callable&lt;Integer&gt; &#123; public static void main(String[] args) &#123; CallableThreadTest ctt = new CallableThreadTest(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt); for(int i = 0;i &lt; 100;i++) &#123; System.out.println(Thread.currentThread().getName()+" 的循环变量i的值"+i); if(i==20) &#123; new Thread(ft,"有返回值的线程").start(); &#125; &#125; try &#123; System.out.println("子线程的返回值："+ft.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; @Override public Integer call() throws Exception &#123; int i = 0; for(;i&lt;100;i++) &#123; System.out.println(Thread.currentThread().getName()+" "+i); &#125; return i; &#125; &#125; 总结采用实现Runnable、Callable接口的方式创见多线程时，优势是： 线程类只是实现了Runnable接口或Callable接口，还可以继承其他类。 在这种方式下，多个线程可以共享同一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。 劣势是： 编程稍微复杂，如果要访问当前线程，则必须使用Thread.currentThread()方法。 使用继承Thread类的方式创建多线程时优势是： 编写简单，如果需要访问当前线程，则无需使用Thread.currentThread()方法，直接使用this即可获得当前线程。 劣势是： 线程类已经继承了Thread类，所以不能再继承其他父类。 参考：http://blog.csdn.net/longshengguoji/article/details/41126119]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-LinkedHashMap与LRUcache整理]]></title>
      <url>%2Fpost%2Fjava-LinkedHashMap-LRUcache%2F</url>
      <content type="text"><![CDATA[LRU 缓存介绍我们平时总会有一个电话本记录所有朋友的电话，但是，如果有朋友经常联系，那些朋友的电话号码不用翻电话本我们也能记住，但是，如果长时间没有联系了，要再次联系那位朋友的时候，我们又不得不求助电话本，但是，通过电话本查找还是很费时间的。但是，我们大脑能够记住的东西是一定的，我们只能记住自己最熟悉的，而长时间不熟悉的自然就忘记了。 其实，计算机也用到了同样的一个概念，我们用缓存来存放以前读取的数据，而不是直接丢掉，这样，再次读取的时候，可以直接在缓存里面取，而不用再重新查找一遍，这样系统的反应能力会有很大提高。但是，当我们读取的个数特别大的时候，我们不可能把所有已经读取的数据都放在缓存里，毕竟内存大小是一定的，我们一般把最近常读取的放在缓存里（相当于我们把最近联系的朋友的姓名和电话放在大脑里一样）。 LRU 缓存利用了这样的一种思想。LRU 是 Least Recently Used 的缩写，翻译过来就是“最近最少使用”，也就是说，LRU 缓存把最近最少使用的数据移除，让给最新读取的数据。而往往最常读取的，也是读取次数最多的，所以，利用 LRU 缓存，我们能够提高系统的 performance。 实现使用LinkedHashMap 好处： 它本身已经实现了按照访问顺序的存储，也就是说，最近读取的会放在最前面，最最不常读取的会放在最后（当然，它也可以实现按照插入顺序存储）。 LinkedHashMap 本身有一个方法用于判断是否需要移除最不常读取的数，但是，原始方法默认不需要移除，所以，我们需要 override 这样一个方法，使得当缓存里存放的数据个数超过规定个数后，就把最不常用的移除掉。 代码12345678910111213141516171819202122public class LRUCache &#123; private int capacity; private Map&lt;Integer, Integer&gt; cache; public LRUCache(int capacity) &#123; this.capacity = capacity; this.cache = new java.util.LinkedHashMap&lt;Integer, Integer&gt; (capacity, 0.75f, true) &#123; // 定义put后的移除规则，大于容量就删除eldest protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) &#123; return size() &gt; capacity; &#125; &#125;; &#125; public int get(int key) &#123; if (cache.containsKey(key)) &#123; return cache.get(key); &#125; else return -1; &#125; public void set(int key, int value) &#123; cache.put(key, value); &#125;&#125; 参考： http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap-lrucache.html https://yikun.github.io/2015/04/03/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AALRU-Cache%EF%BC%9F/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-HashSet/LinkedHashSet整理]]></title>
      <url>%2Fpost%2Fjava-Hashset-LinkedHashSet%2F</url>
      <content type="text"><![CDATA[HashSet概述 HashSet 是基于 HashMap 实现的，HashSet 底层采用 HashMap 来保存所有元素 所有放入 HashSet 中的集合元素实际上由 HashMap 的 key 来保存，而 HashMap 的 value 则存储了一个 PRESENT，它是一个静态的 Object 对象。 HashSet 的绝大部分方法都是通过调用 HashMap 的方法来实现的，因此 HashSet 和 HashMap 两个集合在实现本质上是相同的。 我们应该为保存到 HashSet 中的对象覆盖 hashCode() 和 equals() 方法 我们先通过 HashSet 最简单的构造函数和几个成员变量来看一下，证明其底层是 HashMap：123456789101112private transient HashMap&lt;E,Object&gt; map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); /** * Constructs a new, empty set; the backing &lt;tt&gt;HashMap&lt;/tt&gt; instance has * default initial capacity (16) and load factor (0.75). */ public HashSet() &#123; map = new HashMap&lt;&gt;(); &#125; 默认情况下采用的是 initial capacity为16，load factor 为 0.75。 构造方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 默认的无参构造器，构造一个空的HashSet。 * * 实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。 */public HashSet() &#123; map = new HashMap&lt;E,Object&gt;();&#125;/** * 构造一个包含指定collection中的元素的新set。 * * 实际底层使用默认的加载因子0.75和足以包含指定collection中所有元素的初始容量来创建一个HashMap。 * @param c 其中的元素将存放在此set中的collection。 */public HashSet(Collection&lt;? extends E&gt; c) &#123; map = new HashMap&lt;E,Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c);&#125;/** * 以指定的initialCapacity和loadFactor构造一个空的HashSet。 * * 实际底层以相应的参数构造一个空的HashMap。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125;/** * 以指定的initialCapacity构造一个空的HashSet。 * * 实际底层以相应的参数及加载因子loadFactor为0.75构造一个空的HashMap。 * @param initialCapacity 初始容量。 */public HashSet(int initialCapacity) &#123; map = new HashMap&lt;E,Object&gt;(initialCapacity);&#125;/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。此构造函数为包访问权限，不对外公开， * 实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125; add方法12345678/** * @param e 将添加到此set中的元素。 * @return 如果此set尚未包含指定元素，则返回true。 */public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 由于 HashMap 的 put() 方法添加 key-value 对时，当新放入 HashMap 的 Entry 中 key 与集合中原有 Entry 的 key 相同（hashCode()返回值相等，通过 equals 比较也返回 true），新添加的 Entry 的 value 会将覆盖原来 Entry 的 value（HashSet 中的 value 都是PRESENT），但key不会有任何改变。 该方法如果添加的是在 HashSet 中不存在的，则返回 true；如果添加的元素已经存在，返回 false。其原因在于我们之前提到的关于 HashMap 的 put 方法。该方法在添加 key 不重复的键值对的时候，会返回 null。 contains方法1234567891011/** * 如果此set包含指定元素，则返回true。 * 更确切地讲，当且仅当此set包含一个满足(o==null ? e==null : o.equals(e))的e元素时，返回true。 * * 底层实际调用HashMap的containsKey判断是否包含指定key。 * @param o 在此set中的存在已得到测试的元素。 * @return 如果此set包含指定元素，则返回true。 */ public boolean contains(Object o) &#123; return map.containsKey(o); &#125; remove方法 /** * 如果指定元素存在于此set中，则将其移除。更确切地讲，如果此set包含一个满足(o==null ? e==null : o.equals(e))的元素e， * 则将其移除。如果此set已包含该元素，则返回true * * 底层实际调用HashMap的remove方法删除指定Entry。 * @param o 如果存在于此set中则需要将其移除的对象。 * @return 如果set包含指定元素，则返回true。 */ public boolean remove(Object o) { return map.remove(o)==PRESENT; } clone方法1234567891011121314/** * 返回此HashSet实例的浅表副本：并没有复制这些元素本身。 * * 底层实际调用HashMap的clone()方法，获取HashMap的浅表副本，并设置到HashSet中。 */ public Object clone() &#123; try &#123; HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; &#125; catch (CloneNotSupportedException e) &#123; throw new InternalError(); &#125; &#125; 注意 对于 HashSet 中保存的对象，请注意正确重写其 equals 和 hashCode 方法，以保证放入的对象的唯一性。这两个方法是比较重要的，希望大家在以后的开发过程中需要注意一下 LinkedHashSet概述LinkedHashSet 首先我们需要知道的是它是一个 Set 的实现，所以它其中存的肯定不是键值对，而是值。此实现与 HashSet 的不同之处在于，LinkedHashSet 维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 LinkedHashSet 底层使用 LinkedHashMap 来保存所有元素，它继承与 HashSet，其所有的方法操作上又与 HashSet 相同，因此 LinkedHashSet 的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个 LinkedHashMap 来实现，在相关操作上与父类 HashSet 的操作相同，直接调用父类 HashSet 的方法即可。LinkedHashSet 的源代码如下： 构造方法123456789101112/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合。 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持。 * * 实际底层会以指定的参数构造一个空LinkedHashMap实例来实现。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 * @param dummy 标记。 */HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;E,Object&gt;(initialCapacity, loadFactor);&#125; LinkedHashSet 通过继承 HashSet，底层使用 LinkedHashMap，以很简单明了的方式来实现了其自身的所有功能。 其他代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class LinkedHashSet&lt;E&gt; extends HashSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable &#123; private static final long serialVersionUID = -2851667679971038690L; /** * 构造一个带有指定初始容量和加载因子的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个有指定初始容量和加载因子的LinkedHashMap实例。 * @param initialCapacity 初始容量。 * @param loadFactor 加载因子。 */ public LinkedHashSet(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor, true); &#125; /** * 构造一个带指定初始容量和默认加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带指定初始容量和默认加载因子0.75的LinkedHashMap实例。 * @param initialCapacity 初始容量。 */ public LinkedHashSet(int initialCapacity) &#123; super(initialCapacity, .75f, true); &#125; /** * 构造一个带默认初始容量16和加载因子0.75的新空链接哈希set。 * * 底层会调用父类的构造方法，构造一个带默认初始容量16和加载因子0.75的LinkedHashMap实例。 */ public LinkedHashSet() &#123; super(16, .75f, true); &#125; /** * 构造一个与指定collection中的元素相同的新链接哈希set。 * * 底层会调用父类的构造方法，构造一个足以包含指定collection * 中所有元素的初始容量和加载因子为0.75的LinkedHashMap实例。 * @param c 其中的元素将存放在此set中的collection。 */ public LinkedHashSet(Collection&lt;? extends E&gt; c) &#123; super(Math.max(2*c.size(), 11), .75f, true); addAll(c); &#125;&#125; 总结 LinkedHashSet 是 Set 的一个具体实现，其维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可为插入顺序或是访问顺序。 LinkedHashSet 继承与 HashSet，并且其内部是通过 LinkedHashMap 来实现的。有点类似于我们之前说的LinkedHashMap 其内部是基于 Hashmap 实现一样，不过还是有一点点区别的（具体的区别大家可以自己去思考一下） 如果我们需要迭代的顺序为插入顺序或者访问顺序，那么 LinkedHashSet 是需要你首先考虑的 参考： http://alex09.iteye.com/blog/539549 http://wiki.jikexueyuan.com/project/java-collection/hashset.html http://wiki.jikexueyuan.com/project/java-collection/linkedhashset.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-LinkedHashMap整理]]></title>
      <url>%2Fpost%2Fjava-LinkedHashMap%2F</url>
      <content type="text"><![CDATA[概述 HashMap 是无序的，HashMap 在 put 的时候是根据 key 的 hashcode 进行 hash 然后放入对应的地方。所以遍历 HashMap 的顺序跟 put 的顺序不同 JAVA 在 JDK1.4 以后提供了 LinkedHashMap 来帮助我们实现了有序的 HashMap LinkedHashMap 是 HashMap的一个子类，它保留了元素插入的顺序，如果需要输出的顺序和输入时的相同，那么就选用 LinkedHashMap。 LinkedHashMap允许使用 null 值和 null 键 LinkedHashMap 实现与 HashMap 的不同之处在于，LinkedHashMap 维护着一个运行于所有条目的双重链接列表。此链接列表定义了迭代顺序，该迭代顺序可以是插入顺序或者是访问顺序。 根据链表中元素的顺序可以分为：按插入顺序的链表，和按访问顺序(调用 get 方法)的链表。默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。 例子HashMap12345678910111213public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); map.put("apple", "苹果"); map.put("watermelon", "西瓜"); map.put("banana", "香蕉"); map.put("peach", "桃子"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125;&#125; 输出： banana=香蕉apple=苹果peach=桃子watermelon=西瓜 可以发现，遍历HashMap 是没有顺序的 LinkedHashMap12345678910111213public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(); map.put("apple", "苹果"); map.put("watermelon", "西瓜"); map.put("banana", "香蕉"); map.put("peach", "桃子"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125;&#125; 输出： apple=苹果watermelon=西瓜banana=香蕉peach=桃子 可以发现，其输出顺序是完成按照插入顺序的！也就是我们上面所说的保留了插入的顺序 接下来验证按照访问顺序12345678910111213141516public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new LinkedHashMap&lt;String, String&gt;(16,0.75f,true); map.put("apple", "苹果"); map.put("watermelon", "西瓜"); map.put("banana", "香蕉"); map.put("peach", "桃子"); map.get("banana"); map.get("apple"); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); System.out.println(entry.getKey() + "=" + entry.getValue()); &#125;&#125; 输出： watermelon=西瓜peach=桃子banana=香蕉apple=苹果 可以发现，访问了banana和apple之后，遍历，他们排在了后面 再来看一个例子：123456789101112LinkedHashMap&lt;String, Integer&gt; lmap = new LinkedHashMap&lt;String, Integer&gt;();lmap.put("语文", 1);lmap.put("数学", 2);lmap.put("英语", 3);lmap.put("历史", 4);lmap.put("政治", 5);lmap.put("地理", 6);lmap.put("生物", 7);lmap.put("化学", 8);for(Entry&lt;String, Integer&gt; entry : lmap.entrySet()) &#123; System.out.println(entry.getKey() + ": " + entry.getValue());&#125; 输出： 语文: 1数学: 2英语: 3历史: 4政治: 5地理: 6生物: 7化学: 8 我们来看看LinkedHashMap的内部结构： 看图应该很清楚了，LinkedHashMap是Hash表和链表的实现，并且依靠着双向链表保证了迭代顺序是插入的顺序。 实现对于 LinkedHashMap 而言，它继承与 HashMap(public class LinkedHashMap extends HashMap implements Map)、底层使用哈希表与双向链表来保存所有元素。其基本操作与父类 HashMap 相似，它通过重写父类相关的方法，来实现自己的链接列表特性。下面我们来分析 LinkedHashMap 的源代码： 在HashMap中提到了下面的定义：1234// Callbacks to allow LinkedHashMap post-actionsvoid afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125;void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; LinkedHashMap继承于HashMap，因此也重新实现了这3个函数，顾名思义这三个函数的作用分别是：节点访问后、节点插入后、节点移除后做一些事情。 afterNodeAccess函数:12345678910111213141516171819202122232425void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // 如果定义了accessOrder，那么就保证最近访问节点放到最后 if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 就是说在进行put之后就算是对节点的访问了，那么这个时候就会更新链表，把最近访问的放到最后 afterNodeInsertion函数12345678void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 如果定义了移除规则，则执行相应的溢出 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; 成员变量LinkedHashMap 采用的 hash 算法和 HashMap 相同，但是它重新定义了数组中保存的元素 Entry，该 Entry 除了保存当前对象的引用外，还保存了其上一个元素 before 和下一个元素 after 的引用，从而在哈希表的基础上又构成了双向链接列表。看源代码：12345678910111213141516171819/*** The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt;* for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order.* 如果为true，则按照访问顺序；如果为false，则按照插入顺序。*/private final boolean accessOrder;/*** 双向链表的表头元素。 */private transient Entry&lt;K,V&gt; header;/*** LinkedHashMap的Entry元素。* 继承HashMap的Entry元素，又保存了其上一个元素before和下一个元素after的引用。 */private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; ……&#125; LinkedHashMap 中的 Entry 集成于 HashMap 的 Entry，但是其增加了 before 和 after 的引用，指的是上一个元素和下一个元素的引用。 初始化通过源代码可以看出，在 LinkedHashMap 的构造方法中，实际调用了父类 HashMap 的相关构造方法来构造一个底层存放的 table 数组，但额外可以增加 accessOrder 这个参数，如果不设置，默认为 false，代表按照插入顺序进行迭代；当然可以显式设置为 true，代表以访问顺序进行迭代。如：1234public LinkedHashMap(int initialCapacity, float loadFactor,boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; 我们已经知道 LinkedHashMap 的 Entry 元素继承 HashMap 的 Entry，提供了双向链表的功能。在上述 HashMap 的构造器中，最后会调用 init() 方法，进行相关的初始化，这个方法在 HashMap 的实现中并无意义，只是提供给子类实现相关的初始化调用。但在 LinkedHashMap 重写了 init() 方法，在调用父类的构造方法完成构造后，进一步实现了对其元素 Entry 的初始化操作。12345678910/*** Called by superclass constructors and pseudoconstructors (clone,* readObject) before any entries are inserted into the map. Initializes* the chain.*/@Overridevoid init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header;&#125; 储存LinkedHashMap 并未重写父类 HashMap 的 put 方法，而是重写了父类 HashMap 的 put 方法调用的子方法void recordAccess(HashMap m) ，void addEntry(int hash, K key, V value, int bucketIndex) 和void createEntry(int hash, K key, V value, int bucketIndex)，提供了自己特有的双向链接列表的实现。我们在之前的文章中已经讲解了HashMap的put方法，我们在这里重新贴一下 HashMap 的 put 方法的源代码：12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; 重写的方法：1234567891011121314151617181920212223242526272829303132333435363738void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 调用create方法，将新元素以双向链表的的形式加入到映射中。 createEntry(hash, key, value, bucketIndex); // 删除最近最少使用元素的策略定义 Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) &#123; removeEntryForKey(eldest.key); &#125; else &#123; if (size &gt;= threshold) resize(2 * table.length); &#125;&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;K,V&gt;(hash, key, value, old); table[bucketIndex] = e; // 调用元素的addBrefore方法，将元素加入到哈希、双向链接列表。 e.addBefore(header); size++;&#125;private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123; after = existingEntry; before = existingEntry.before; before.after = this; after.before = this;&#125; 读取LinkedHashMap 重写了父类 HashMap 的 get 方法，实际在调用父类 getEntry() 方法取得查找的元素后，再判断当排序模式 accessOrder 为 true 时，记录访问顺序，将最新访问的元素添加到双向链表的表头，并从原来的位置删除。由于的链表的增加、删除操作是常量级的，故并不会带来性能的损失。12345678910111213141516171819202122232425262728293031323334public V get(Object key) &#123; // 调用父类HashMap的getEntry()方法，取得要查找的元素。 Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) return null; // 记录访问顺序。 e.recordAccess(this); return e.value;&#125;void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; // 如果定义了LinkedHashMap的迭代顺序为访问顺序， // 则删除以前位置上的元素，并将最新访问的元素添加到链表表头。 if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125;&#125;/*** Removes this entry from the linked list.*/private void remove() &#123; before.after = after; after.before = before;&#125;/**clear链表，设置header为初始状态*/public void clear() &#123; super.clear(); header.before = header.after = header;&#125; 排序模式LinkedHashMap 定义了排序模式 accessOrder，该属性为 boolean 型变量，对于访问顺序，为 true；对于插入顺序，则为 false。一般情况下，不必指定排序模式，其迭代顺序即为默认为插入顺序。 这些构造方法都会默认指定排序模式为插入顺序。如果你想构造一个 LinkedHashMap，并打算按从近期访问最少到近期访问最多的顺序（即访问顺序）来保存元素，那么请使用下面的构造方法构造 LinkedHashMap：public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) 该哈希映射的迭代顺序就是最后访问其条目的顺序，这种映射很适合构建 LRU 缓存。LinkedHashMap 提供了 removeEldestEntry(Map.Entry eldest) 方法。该方法可以提供在每次添加新条目时移除最旧条目的实现程序，默认返回 false，这样，此映射的行为将类似于正常映射，即永远不能移除最旧的元素。 对比下几种Mapjava为数据结构中的映射定义了一个接口java.util.Map;它有四个实现类,分别是HashMap Hashtable LinkedHashMap 和TreeMap. Map主要用于存储健值对，根据键得到值，因此不允许键重复(重复了覆盖了),但允许值重复。 HashMapHashmap 是一个最常用的Map,它根据键的HashCode值存储数据,根据键可以直接获取它的值，具有很快的访问速度，遍历时，取得数据的顺序是完全随机的。 HashMap最多只允许一条记录的键为Null;允许多条记录的值为 Null;HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。如果需要同步，可以用 Collections的synchronizedMap方法使HashMap具有同步的能力，或者使用ConcurrentHashMap。 HashtableHashtable与 HashMap类似,它继承自Dictionary类，不同的是:它不允许记录的键或者值为空;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了 Hashtable在写入时会比较慢。 LinkedHashMapLinkedHashMap 是HashMap的一个子类，保存了记录的插入顺序，在用Iterator遍历LinkedHashMap时，先得到的记录肯定是先插入的.也可以在构造时用带参数，按照应用次数排序。在遍历的时候会比HashMap慢，不过有种情况例外，当HashMap容量很大，实际数据较少时，遍历起来可能会比 LinkedHashMap慢，因为LinkedHashMap的遍历速度只和实际数据有关，和容量无关，而HashMap的遍历速度和他的容量有关。 TreeMapTreeMap实现SortMap接口，能够把它保存的记录根据键排序,默认是按键值的升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。 总结其实 LinkedHashMap 几乎和 HashMap 一样：从技术上来说，不同的是它定义了一个 Entry header，这个 header 不是放在 Table 里，它是额外独立出来的。LinkedHashMap 通过继承 hashMap 中的 Entry,并添加两个属性 Entry before,after,和 header 结合起来组成一个双向链表，来实现按插入顺序或访问顺序排序。 参考： http://wiki.jikexueyuan.com/project/java-collection/linkedhashmap.html https://yikun.github.io/2015/04/02/Java-LinkedHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/ http://www.cnblogs.com/hubingxu/archive/2012/02/21/2361281.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TIME_WAIT和CLOSE_WAIT]]></title>
      <url>%2Fpost%2FtimeWaitCloseWait%2F</url>
      <content type="text"><![CDATA[TIME_WAIT和CLOSE_WAIT在服务器的日常维护过程中，会经常用到下面的命令： netstat -n | awk ‘/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}’ 它会显示例如下面的信息： TIME_WAIT 814 CLOSE_WAIT 1 FIN_WAIT1 1 ESTABLISHED 634 SYN_RECV 2 LAST_ACK 1 常用的三个状态是： ESTABLISHED 表示正在通信 TIME_WAIT 表示主动关闭 CLOSE_WAIT 表示被动关闭。 这几种状态什么意思呢，看看图 tcp断开时候也可能是图中的情况，反过来所以说服务端可能处于TIME_WAIT状态，也有可能处于CLOSE_WAIT状态 一般不到万不得已的情况也不会去查看网络状态，如果服务器出了异常，百分之八九十都是下面两种情况： 服务器保持了大量TIME_WAIT状态 服务器保持了大量CLOSE_WAIT状态 因为Linux分配给一个用户的文件句柄是有限的，而TIME_WAIT和CLOSE_WAIT两种状态如果一直被保持，那么意味着对应数目的通道就一直被占着，而且是“占着茅坑不使劲”，一旦达到句柄数上限，新的请求就无法被处理了，接着就是大量Too Many Open Files异常，tomcat崩溃 如何解决存在大量TIME_WAIT和CLOSE_WAIT的问题减少TIME_WAIT状态解决思路很简单，就是让服务器能够快速回收和重用那些TIME_WAIT的资源。对/etc/sysctl.conf文件修改12345678910111213141516171819202122#对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃,不应该大于255，默认值是5，对应于180秒左右时间 net.ipv4.tcp_syn_retries=2 #net.ipv4.tcp_synack_retries=2 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为300秒 net.ipv4.tcp_keepalive_time=1200 net.ipv4.tcp_orphan_retries=3 #表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间 net.ipv4.tcp_fin_timeout=30 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_syn_backlog = 4096 #表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭 net.ipv4.tcp_syncookies = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭 net.ipv4.tcp_tw_reuse = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭 net.ipv4.tcp_tw_recycle = 1 ##减少超时前的探测次数 net.ipv4.tcp_keepalive_probes=5 ##优化网络设备接收队列 net.core.netdev_max_backlog=3000 修改完之后执行/sbin/sysctl -p让参数生效。 要注意的几个参数： net.ipv4.tcp_tw_reuse net.ipv4.tcp_tw_recycle net.ipv4.tcp_fin_timeout net.ipv4.tcpkeepalive* net.ipv4.tcp_tw_reuse和net.ipv4.tcp_tw_recycle的开启都是为了回收处于TIME_WAIT状态的资源。 net.ipv4.tcp_fin_timeout这个时间可以减少在异常情况下服务器从FIN-WAIT-2转到TIME_WAIT的时间 net.ipv4.tcpkeepalive*一系列参数，是用来设置服务器检测连接存活的相关配置 减少CLOSE_WAIT状态TIME_WAIT状态可以通过优化服务器参数得到解决，因为发生TIME_WAIT的情况是服务器自己可控的，要么就是对方连接的异常，要么就是自己没有迅速回收资源，总之不是由于自己程序错误导致的。 但是CLOSE_WAIT就不一样了，从上面的图可以看出来，如果一直保持在CLOSE_WAIT状态，那么只有一种情况，就是在对方关闭连接之后服务器程序自己没有进一步发出ack信号。换句话说，就是在对方连接关闭之后，程序里没有检测到，或者程序压根就忘记了这个时候需要关闭连接，于是这个资源就一直被程序占着。 个人觉得这种情况，通过服务器内核参数也没办法解决，服务器对于程序抢占的资源没有主动回收的权利，除非终止程序运行。 所以如果将大量CLOSE_WAIT的解决办法总结为一句话那就是：查代码。因为问题出在服务器程序里头啊。 参考: http://blog.csdn.net/shootyou/article/details/6622226]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TCP协议三次握手四次挥手整理]]></title>
      <url>%2Fpost%2Ftcp-shakehands%2F</url>
      <content type="text"><![CDATA[TCP协议中的三次握手直接上图 解释 第一次握手：客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认; 第二次握手：服务器收到syn包，必须确认客户的SYN(ack=j+1)，同时自己也发送一个SYN包(syn=k)，即SYN+ACK包，此时服务器进入SYN_RECV状态; 第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。来个更清晰的图为什么要三次握手谢希仁的《计算机网络》中是这样说的： 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误 说白了就是：防止了服务器端的一直等待而浪费资源 SYN攻击在三次握手过程中，服务器发送SYN-ACK之后，收到客户端的ACK之前的TCP连接称为半连接(half-open connect).此时服务器处于Syn_RECV状态.当收到ACK后，服务器转入ESTABLISHED状态.Syn攻击就是 攻击客户端 在短时间内伪造大量不存在的IP地址，向服务器不断地发送syn包，服务器回复确认包，并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直 至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。Syn攻击是一个典型的DDOS攻击。检测SYN攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击.在Linux下可以如下命令检测是否被Syn攻击netstat -n -p TCP | grep SYN_RECV一般较新的TCP/IP协议栈都对这一过程进行修正来防范Syn攻击，修改tcp协议实现。主要方法有SynAttackProtect保护机制、SYN cookies技术、增加最大半连接和缩短超时时间等.但是不能完全防范syn攻击。 TCP协议中的四次挥手直接上图 注意：中断连接端可以是Client端，也可以是Server端，那么就是图上的流程反过来操作 解释（以客户端发起断开为例子） 假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说”我Client端没有数据要发给你了“ 但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息“。– 这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了“。 Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“ Server端收到ACK后，”就知道可以断开连接了“。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ 来个更清晰的图 为什么要四次分手那四次分手又是为何呢？TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。 来个完整的图 问题为什么连接的时候是三次握手，关闭的时候却是四次握手？答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。 为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 参考： http://blog.csdn.net/whuslei/article/details/6667471/ http://www.jellythink.com/archives/705 http://www.jianshu.com/p/9968b16b607e http://www.cnblogs.com/zmlctt/p/3690998.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网络分层、TCP、IP、Http、Socket整理]]></title>
      <url>%2Fpost%2Fnet-tcp%2F</url>
      <content type="text"><![CDATA[网络分层 网络分层就是将网络节点所要完成的数据的发送或转发、打包或拆包，控制信息的加载或拆出等工作，分别由不同的硬件和软件模块去完成。这样可以将往来通信和网络互连这一复杂的问题变得较为简单。 五层因特网协议栈（从上到下） 应用层 传输层 网络层 链路层 物理层 五层因特网协议栈介绍应用层用于支持支持网络应用，运行在不同主机上的进程则使用应用层协议进行通信。主要的协议有：http、ftp、telnet、smtp、pop3等。 传输层负责为信源和信宿提供应用程序进程间的数据传输服务，这一层上主要定义了两个传输协议，传输控制协议即TCP和用户数据报协议UDP。 网络层负责将数据报独立地从信源发送到信宿，主要解决路由选择、拥塞控制和网络互联等问题。 数据链路层负责将IP数据报封装成合适在物理网络上传输的帧格式并传输，或将从物理网络接收到的帧解封，取出IP数据报交给网络层。 物理层负责将比特流在结点间传输，即负责物理传输。该层的协议既与链路有关也与传输介质有关 七层因特网协议栈（从上到下） 应用层(Application) 表示层(Presentation) 会话层(Session) 传输层(Transport) 网络层(Network) 数据链路层(Data Link) 物理层(Physical) 和五层结构有什么区别个人理解五层结构就是把7层结构的 应用层/表示层/会话层 合为一个应用层 7层因特网协议栈介绍一句话概述 应用层 指网络操作系统和具体的应用程序，对应WWW服务器、FTP服务器等应用软件 表示层 数据语法的转换、数据的传送等 会话层 建立起两端之间的会话关系，并负责数据的传送 传输层 负责错误的检查与修复，以确保传送的质量，是TCP协议工作的地方。（报文） 网络层 提供了编址方案,IP协议工作的地方(数据包） 数据链路层 将由物理层传来的未经处理的位数据包装成数据帧 物理层 对应网线、网卡、接口等物理设备(位)。 物理层物理层(Physical layer)是参考模型的最低层。该层是网络通信的数据传输介质，由连接不同结点的电缆与设备共同构成。主要功能是：利用传输介质为数据链路层提供物理连接，负责处理数据传输并监控数据出错率，以便数据流的透明传输。 数据链路层数据链路层(Data link layer)是参考模型的第2层。 主要功能是：在物理层提供的服务基础上，在通信的实体间建立数据链路连接，传输以“帧”为单位的数据包，并采用差错控制与流量控制方法，使有差错的物理线路变成无差错的数据链路。 网络层网络层(Network layer)是参考模型的第3层。主要功能是：为数据在结点之间传输创建逻辑链路，通过路由选择算法为分组通过通信子网选择最适当的路径，以及实现拥塞控制、网络互联等功能。 传输层传输层(Transport layer)是参考模型的第4层。主要功能是向用户提供可靠的端到端(End-to-End)服务，处理数据包错误、数据包次序，以及其他一些关键传输问题。传输层向高层屏蔽了下层数据通信的细节，因此，它是计算机通信体系结构中关键的一层。 会话层会话层(Session layer)是参考模型的第5层。主要功能是：负责维护两个结点之间的传输链接，以便确保点到点传输不中断，以及管理数据交换等功能。 表示层表示层(Presentation layer)是参考模型的第6层。主要功能是：用于处理在两个通信系统中交换信息的表示方式，主要包括数据格式变换、数据加密与解密、数据压缩与恢复等功能。 应用层应用层(Application layer)是参考模型的最高层。主要功能是：为应用软件提供了很多服务，例如文件服务器、数据库服务、电子邮件与其他网络软件服务。 总结图 问题汇总常用的协议位于那一层 IP协议对应于网络层 TCP协议对应于传输 HTTP协议对应于应用层 TCP/IP和HTTP的关系关于TCP/IP和HTTP协议的关系，网络有一段比较容易理解的介绍： “我们在传输数据时，可以只使用(传输层)TCP/IP协议，但是那样的话，如果没有应用层，便无法识别数据内容。 如果想要使传输的数据有意义，则必须使用到应用层协议。应用层协议有很多，比如HTTP、FTP、TELNET等，也可以自己定义应用层协议。WEB使用HTTP协议作应用层协议，以封装HTTP文本信息，然后使用TCP/IP做传输层协议将它发到网络上。 Socket注意：Socket本身并不是协议，而是一个调用接口(API)，它只是提供了一个针对TCP或者UDP编程的接口。 socket是对TCP/IP协议的封装和应用,TPC/IP协议是传输层协议，主要解决数据如何在网络中传输 通过Socket，我们才能使用TCP/IP协议。实际上，Socket跟TCP/IP协议没有必然的联系。Socket编程接口在设计的时候，就希望也能适应其他的网络协议。所以说，Socket的出现只是使得程序员更方便地使用TCP/IP协议栈而已，是对TCP/IP协议的抽象，从而形成了我们知道的一些最基本的函数接口，比如create、listen、connect、accept、send、read和write等等。 Socket和TCP/IP协议的关系网络有一段关于socket和TCP/IP协议关系的说法比较容易理解： “TCP/IP只是一个协议栈，就像操作系统的运行机制一样，必须要具体实现，同时还要提供对外的操作接。这个就像操作系统会提供标准的编程接口，比如win32编程接口一样，TCP/IP也要提供可供程序员做网络开发所用的接口，这就是Socket编程接口。” HTTP和Socket的关系CSDN上有个比较形象的描述： HTTP是轿车，提供了封装或者显示数据的具体形式;Socket是发动机，提供了网络通信的能力。 利用Socket建立网络连接的步骤 建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。 套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。 1、服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。 2、客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。 为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。 3、连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端，一旦客户端确认了此描述，双方就正式建立连接。 而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 HTTP链接的特点 HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。 HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。 TCP和UDP的区别 1、TCP是面向链接的，虽然说网络的不安全不稳定特性决定了多少次握手都不能保证连接的可靠性，但TCP的三次握手在最低限度上(实际上也很大程度上保证了)保证了连接的可靠性; 而UDP不是面向连接的，UDP传送数据前并不与对方建立连接，对接收到的数据也不发送确认信号，发送端不知道数据是否会正确接收，当然也不用重发，所以说UDP是无连接的、不可靠的一种数据传输协议。 2、也正由于1所说的特点，使得UDP的开销更小数据传输速率更高，因为不必进行收发数据的确认，所以UDP的实时性更好。 知道了TCP和UDP的区别，就不难理解为何采用TCP传输协议的MSN比采用UDP的QQ传输文件慢了，但并不能说QQ的通信是不安全的， 因为程序员可以手动对UDP的数据收发进行验证，比如发送方对每个数据包进行编号然后由接收方进行验证啊什么的， 即使是这样，UDP因为在底层协议的封装上没有采用类似TCP的“三次握手”而实现了TCP所无法达到的传输效率。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-BIO/NIO/AIO整理]]></title>
      <url>%2Fpost%2Fjava-niobioaio%2F</url>
      <content type="text"><![CDATA[参考http://qindongliang.iteye.com/blog/2018539 一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作。 同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO。 阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 同步和异步是针对应用程序和内核的交互而言的，同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪，而异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知。 而阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。 举个例子,如果你想吃一份宫保鸡丁盖饭： 同步阻塞：你到饭馆点餐，然后在那等着，还要一边喊：好了没啊！ 同步非阻塞：在饭馆点完餐，就去遛狗了。不过溜一会儿，就回饭馆喊一声：好了没啊！ 异步阻塞：遛狗的时候，接到饭馆电话，说饭做好了，让您亲自去拿。 异步非阻塞：饭馆打电话说，我们知道您的位置，一会给你送过来，安心遛狗就可以了。 所以,IO操作可以分为3类：同步阻塞（即早期的IO操作）、同步非阻塞（NIO）、异步（AIO）。 同步阻塞：在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式。 同步非阻塞：在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。 异步：此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序。” 同步和异步是针对应用程序和内核的交互而言的。 -阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。 由上描述基本可以总结一句简短的话，同步和异步是目的，阻塞和非阻塞是实现方式。 编号 名词 解释 举例 1 同步 指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪 自己上街买衣服，自己亲自干这件事，别的事干不了。 2 异步 异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知（异步的特点就是通知） 告诉朋友自己合适衣服的尺寸，大小，颜色，让朋友委托去卖，然后自己可以去干别的事。（使用异步IO时，Java将IO读写委托给OS处理，需要将数据缓冲区地址和大小传给OS） 3 阻塞 所谓阻塞方式的意思是指, 当试图对该文件描述符进行读写时, 如果当时没有东西可读,或者暂时不可写, 程序就进入等待 状态, 直到有东西可读或者可写为止 去公交站充值，发现这个时候，充值员不在（可能上厕所去了），然后我们就在这里等待，一直等到充值员回来为止。（当然现实社会，可不是这样，但是在计算机里确实如此。） 4 非阻塞 非阻塞状态下, 如果没有东西可读, 或者不可写, 读写函数马上返回, 而不会等待 银行里取款办业务时，领取一张小票，领取完后我们自己可以玩玩手机，或者与别人聊聊天，当轮我们时，银行的喇叭会通知，这时候我们就可以去了。 下面我们再来理解组合方式的IO类型，就好理解多了。 同步阻塞IO（JAVA BIO）： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 同步非阻塞IO(Java NIO) ：同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。用户进程也需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问。 异步阻塞IO（Java NIO）： 此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄（如果从UNP的角度看，select属于同步操作。因为select之后，进程还需要读写数据），从而提高系统的并发性！ （Java AIO(NIO.2)）异步非阻塞IO: 在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。 BIO、NIO、AIO适用场景分析: BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 搞清楚了以上概念以后，我们再回过头来看看，Reactor模式和Proactor模式 （其实阻塞与非阻塞都可以理解为同步范畴下才有的概念，对于异步，就不会再去分阻塞非阻塞。对于用户进程，接到异步通知后，就直接操作进程用户态空间里的数据好了。） 首先来看看Reactor模式，Reactor模式应用于同步I/O的场景。我们分别以读操作和写操作为例来看看Reactor中的具体步骤读取操作： 应用程序注册读就绪事件和相关联的事件处理器 事件分离器等待事件的发生 当发生读就绪事件的时候，事件分离器调用第一步注册的事件处理器 事件处理器首先执行实际的读取操作，然后根据读取到的内容进行进一步的处理 写入操作类似于读取操作，只不过第一步注册的是写就绪事件。 下面我们来看看Proactor模式中读取操作和写入操作的过程读取操作： 应用程序初始化一个异步读取操作，然后注册相应的事件处理器，此时事件处理器不关注读取就绪事件，而是关注读取完成事件，这是区别于Reactor的关键。 事件分离器等待读取操作完成事件 在事件分离器等待读取操作完成的时候，操作系统调用内核线程完成读取操作（异步IO都是操作系统负责将数据读写到应用传递进来的缓冲区供应用程序操作，操作系统扮演了重要角色），并将读取的内容放入用户传递过来的缓存区中。这也是区别于Reactor的一点，Proactor中，应用程序需要传递缓存区。 事件分离器捕获到读取完成事件后，激活应用程序注册的事件处理器，事件处理器直接从缓存区读取数据，而不需要进行实际的读取操作。 Proactor中写入操作和读取操作，只不过感兴趣的事件是写入完成事件。 从上面可以看出，Reactor和Proactor模式的主要区别就是真正的读取和写入操作是有谁来完成的，Reactor中需要应用程序自己读取或者写入数据，而Proactor模式中，应用程序不需要进行实际的读写过程，它只需要从缓存区读取或者写入即可，操作系统会读取缓存区或者写入缓存区到真正的IO设备. 综上所述，同步和异步是相对于应用和内核的交互方式而言的，同步 需要主动去询问，而异步的时候内核在IO事件发生的时候通知应用程序，而阻塞和非阻塞仅仅是系统在调用系统调用的时候函数的实现方式而已。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM内存模型整理]]></title>
      <url>%2Fpost%2Fjava-memory%2F</url>
      <content type="text"><![CDATA[一般地大家讲到的Java内存其实就是Jvm内存 Java代码是运行在Java虚拟机之上的，由Java虚拟机通过解释执行(解释器)或编译执行(即时编译器)来完成，故Java内存模型，也就是指Java虚拟机的运行时内存模型。 内存模型图 详细介绍程序计数器PC 程序计数器PC，当前线程所执行的字节码行号指示器。每个线程都有自己计数器，是私有内存空间，该区域是整个内存中较小的一块。 当线程正在执行一个Java方法时，PC计数器记录的是正在执行的虚拟机字节码的地址；当线程正在执行的一个Native方法时，PC计数器则为空（Undefined）。 虚拟机栈虚拟机栈，生命周期与线程相同，是Java方法执行的内存模型。每个方法(不包含native方法)执行的同时都会创建一个栈帧结构，方法执行过程，对应着虚拟机栈的入栈到出栈的过程。 栈帧(Stack Frame)结构栈帧是用于支持虚拟机进行方法执行的数据结构，是属性运行时数据区的虚拟机站的栈元素。见上图， 栈帧包括： 局部变量表 (locals大小，编译期确定)，一组变量存储空间， 容量以slot为最小单位。 操作栈(stack大小，编译期确定)，操作栈元素的数据类型必须与字节码指令序列严格匹配 动态连接， 指向运行时常量池中该栈帧所属方法的引用，为了 动态连接使用。前面的解析过程其实是静态解析；对于运行期转化为直接引用，称为动态解析。 方法返回地址正常退出，执行引擎遇到方法返回的字节码，将返回值传递给调用者异常退出，遇到Exception,并且方法未捕捉异常，那么不会有任何返回值。 额外附加信息，虚拟机规范没有明确规定，由具体虚拟机实现。 异常(Exception)Java虚拟机规范规定该区域有两种异常： StackOverFlowError：当线程请求栈深度超出虚拟机栈所允许的深度时抛出 OutOfMemoryError：当Java虚拟机动态扩展到无法申请足够内存时抛出 本地方法栈本地方法栈则为虚拟机使用到的Native方法提供内存空间，而前面讲的虚拟机栈式为Java方法提供内存空间。有些虚拟机的实现直接把本地方法栈和虚拟机栈合二为一，比如非常典型的Sun HotSpot虚拟机。 异常(Exception)Java虚拟机规范规定该区域可抛出StackOverFlowError和OutOfMemoryError。 方法区方法区主要存放的是已被虚拟机加载的类信息、常量、静态变量、编译器编译后的代码等数据。GC在该区域出现的比较少。 异常(Exception)Java虚拟机规范规定该区域可抛出OutOfMemoryError。 运行时常量池运行时常量池也是方法区的一部分，用于存放编译器生成的各种字面量和符号引用。运行时常量池除了编译期产生的Class文件的常量池，还可以在运行期间，将新的常量加入常量池，比较常见的是String类的intern()方法。 字面量：与Java语言层面的常量概念相近，包含文本字符串、声明为final的常量值等。 符号引用：编译语言层面的概念，包括以下3类： 类和接口的全限定名 字段的名称和描述符 方法的名称和描述符但是该区域不会抛出OutOfMemoryError异常。 Java堆Java堆，是Java虚拟机管理的最大的一块内存，也是GC的主战场，里面存放的是几乎所有的对象实例和数组数据。JIT编译器有栈上分配、标量替换等优化技术的实现导致部分对象实例数据不存在Java堆，而是栈内存。 从内存回收角度，Java堆被分为新生代和老年代；这样划分的好处是为了更快的回收内存； 从内存分配角度，Java堆可以划分出线程私有的分配缓冲区(Thread Local Allocation Buffer,TLAB)；这样划分的好处是为了更快的分配内存； 对象创建的过程是在堆上分配着实例对象，那么对象实例的具体结构如下： 对于填充数据不是一定存在的，仅仅是为了字节对齐。HotSpot VM的自动内存管理要求对象起始地址必须是8字节的整数倍。对象头本身是8的倍数，当对象的实例数据不是8的倍数，便需要填充数据来保证8字节的对齐。该功能类似于高速缓存行的对齐。 另外，关于在堆上内存分配是并发进行的，虚拟机采用CAS加失败重试保证原子操作，或者是采用每个线程预先分配TLAB内存. 异常(Exception)Java虚拟机规范规定该区域可抛出OutOfMemoryError。 参考：http://gityuan.com/2016/01/09/java-memory/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-WeakHashMap整理]]></title>
      <url>%2Fpost%2FWeakHashMap%2F</url>
      <content type="text"><![CDATA[参考： http://mikewang.blog.51cto.com/3826268/880775 http://www.wanghd.com/%E5%90%8E%E7%AB%AF/2012/08/01/weakhashmapzuo-yong-he-shi-li.html 介绍 以弱键 实现的基于哈希表的 Map。在 WeakHashMap 中，当某个键不再正常使用时，将自动移除其条目。更精确地说，对于一个给定的键，其映射的存在并不阻止垃圾回收器对该键的丢弃，这就使该键成为可终止的，被终止，然后被回收。丢弃某个键时，其条目从映射中有效地移除 WeakHashMap 类的行为部分取决于垃圾回收器的动作。因为垃圾回收器在任何时候都可能丢弃键，WeakHashMap 就像是一个被悄悄移除条目的未知线程。特别地，即使对 WeakHashMap 实例进行同步，并且没有调用任何赋值方法，在一段时间后 size 方法也可能返回较小的值，对于 isEmpty 方法，返回 false，然后返回true，对于给定的键，containsKey 方法返回 true 然后返回 false，对于给定的键，get 方法返回一个值，但接着返回 null，对于以前出现在映射中的键，put 方法返回 null，而 remove 方法返回 false，对于键 set、值 collection 和条目 set 进行的检查，生成的元素数量越来越少。 WeakHashMap 中的每个键对象间接地存储为一个弱引用的指示对象。因此，不管是在映射内还是在映射之外，只有在垃圾回收器清除某个键的弱引用之后，该键才会自动移除。 作用WeekHashMap 的这个特点特别适用于需要缓存的场景。在缓存场景下，由于内存是有限的，不能缓存所有对象；对象缓存命中可以提高系统效率，但缓存MISS也不会造成错误，因为可以通过计算重新得到。 代码解释测试一下保持引用会不会被自动释放1234567891011121314151617181920212223242526272829303132package wanghuida.test;import java.util.WeakHashMap;import java.util.Map;import java.util.List;import java.util.ArrayList;public class Entry &#123; /** * @param args */ public static void main(String[] args) &#123; // TODO Auto-generated method stub List&lt;String[]&gt; templist = new ArrayList&lt;String[]&gt;(); //设的多一点，可以让GC真实发挥 for(int i=0; i &lt; 1000000; i++)&#123; String[] tempstr = new String[2]; templist.add(tempstr); &#125; Map&lt;String[], String[]&gt; map = new WeakHashMap&lt;String[], String[]&gt;(); for(int i=0; i &lt; 100; i++)&#123; map.put(templist.get(i), new String[2]); System.gc(); System.out.println(map.size()); &#125; &#125;&#125; 输出1，2，3，4。。。递增，OK没有问题，有一个引用，就不会释放 再测试一下删除引用后的总数1234567891011121314151617181920212223242526272829package wanghuida.test;import java.util.WeakHashMap;import java.util.Map;import java.util.List;import java.util.ArrayList;public class Entry &#123; public static void main(String[] args) &#123; List&lt;String[]&gt; templist = new ArrayList&lt;String[]&gt;(); //设的多一点，可以让GC真实发挥 for(int i=0; i &lt; 1000; i++)&#123; String[] tempstr = new String[2]; templist.add(tempstr); &#125; Map&lt;String[], String[]&gt; map = new WeakHashMap&lt;String[], String[]&gt;(); for(int i=0; i &lt; 100; i++)&#123; map.put(templist.get(i), new String[2]); templist.set(i, null); //删除掉引用 System.gc(); System.out.println(map.size()); &#125; &#125;&#125; 输出0，1，0，1，1，1。。。保持下去，OK也没问题，删除引用后就会释放 再测试一下有引用但没有使用的情况1234567891011121314151617181920212223242526272829303132package wanghuida.test;import java.util.WeakHashMap;import java.util.Map;import java.util.List;import java.util.ArrayList;public class Entry &#123; public static void main(String[] args) &#123; List&lt;String[]&gt; templist = new ArrayList&lt;String[]&gt;(); //新增一个引用 List&lt;String[]&gt; list = new ArrayList&lt;String[]&gt;(); //设的多一点，可以让GC真实发挥 for(int i=0; i &lt; 1000000; i++)&#123; String[] tempstr = new String[2]; templist.add(tempstr); list.add(tempstr); &#125; Map&lt;String[], String[]&gt; map = new WeakHashMap&lt;String[], String[]&gt;(); for(int i=0; i &lt; 100; i++)&#123; map.put(templist.get(i), new String[2]); templist.set(i, null); //删除掉引用 System.gc(); System.out.println(map.size()); &#125; &#125;&#125; 输出0，1，0，1，1，1。。。保持下去，OK也没问题，有引用但不使用也就会释放 最后对比一下有使用的情况12345678910111213141516171819202122232425262728293031323334package wanghuida.test;import java.util.WeakHashMap;import java.util.Map;import java.util.List;import java.util.ArrayList;public class Entry &#123; public static void main(String[] args) &#123; List&lt;String[]&gt; templist = new ArrayList&lt;String[]&gt;(); //新增一个引用 List&lt;String[]&gt; list = new ArrayList&lt;String[]&gt;(); //设的多一点，可以让GC真实发挥 for(int i=0; i &lt; 1000000; i++)&#123; String[] tempstr = new String[2]; templist.add(tempstr); list.add(tempstr); &#125; Map&lt;String[], String[]&gt; map = new WeakHashMap&lt;String[], String[]&gt;(); for(int i=0; i &lt; 100; i++)&#123; map.put(templist.get(i), new String[2]); templist.set(i, null); //删除掉引用 System.gc(); System.out.println(map.size()); &#125; System.out.println(list.size()); &#125;&#125; 输出1，2，3，4。。。递增，OK了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《程序员健康指南》书摘]]></title>
      <url>%2Fpost%2FThe-Healthy-Programmer%2F</url>
      <content type="text"><![CDATA[给自己称重，但不要执着于此。健康是个多面体，单单专注于其中的一面，无法给你的健康情况做出一个准确的描述。 看看本章开头的问题清单，把答案写下来。时不时回顾答案，看看情况是否有改善。 把车停在停车场最里边，这样就能多走一会儿路去办公室了。 走楼梯，不坐电梯。 玩电视游戏时，尽量选择需要四肢协作的游戏，而不是使用只需控制器的游戏。 每两个星期测量一次静息心率。 记录每天散步的步数，给自己设定一个目标。 练习步行技巧，改掉不正确的步行方式。这肯定能帮你提速，而且还能让你避免受伤或过度疲劳。 每小时要休息5分钟。 每天至少用三种不同的姿势工作。 同一个姿势最好别持续20分钟以上。 在工作时，最好做一些灵活性锻炼。 定期测量血压，了解你适合用哪种测量方式，并注意血压有无产生很大变化。 办公室应该备有不止一种椅子：一张常规的办公室椅子、一个健身球、一张高脚凳，甚至还可以有一张“自行车式办公桌”。 尝试几种不同的饮食方案，别害怕失败，尽管去试试吧。 把办公室的糖果罐子和软性饮料换成水果、蔬菜和水果冰沙。 在蔬菜中加入沙拉酱或芝士，让它们变得更可口。 试着每个周一不吃肉。 计算BMR值。别忘了把通过锻炼燃烧的热量加进去。更加明确每天身体消耗的热量。 为饮食方案制定一些规则。你确实可以把某些项目排除在外，但如果把它们加到规则里，就更容易实现饮食目标了。例如，每天吃五份水果或者蔬菜。 不要每天都计算热量，这事儿算不清楚的。相反，你只要认真计算某一天的热量值，将之作为参考数值，制定一个有效的系统或者其他能启发你的系统就行了。 制定两份不同的饮食方案：一份是减肥用的，一份是能让你一生受用的。这两份都要能使你愉悦。 对饮食方案使用迭代法。循序渐进地减少热量的摄入。对食物的不同分量进行测试。对客观条件做出反应。要学会灵活变通。脸部和显示屏的距离应该保持在51至101厘米之间。 调节显示器和办公室室内的亮度，达到平衡。 经常眨眼，保持眼睛湿润，预防电脑视觉综合征。 使用“20-20-20法则”来锻炼眼睛。 留意一下，你在头痛前吃了什么东西。两者之间可能有关联。 使用含有消炎药成分的药物（如果医生同意的话）来对抗头痛。但是别经常使用，头痛停止后也应该尽快停用药物。 经常检查眼睛，以防出现某些没有症状的眼部疾病。 在每次锻炼中，轮换进行不同的无器械锻炼。如果你做腻了这些运动，可以试试一些普拉提或瑜伽动作。 通过测试找到适合自己的坐姿。也许一开始保持正确的姿势很难，但你一定会找到最适合自己的坐姿。 如果感觉背部疼痛，请马上起立。如果已经对办公桌进行了改良，那就继续站着写代码吧。 用K-W测试进行自我评估，但别把测试内容作为日常锻炼。如果不断重复某些动作，比如单腿仰卧起坐，会加重背部疼痛。进行一些神经滑动运动，来润滑手臂的肌腱和神经，最终缓解手腕疼痛。 如果很喜欢瑜伽课，就把它变成一项日常活动吧。你还可以把它安排到日常清单中的20分钟锻炼里。 使用亚历山大疗法来观察、抑制和引导你每天重复无数次的坏习惯。 使用番茄锻炼法，为日常生活增加一些锻炼活动。 在开始检查代码或者结对编程之前进行一些锻炼，以加强大脑的认知弹性。 记录锻炼情况，写下自己每天做了什么（或者没做什么）。你可以使用本书的辅助iPhone应用，写锻炼日志轻而易举。 玩游戏。不管是Wii的网球游戏还是现实的网球，游戏都能让健身变得充满乐趣。 为自己设定目标，作为对自己的挑战。这是激励自己健身的最好方法。每天花至少10分钟时间晒晒太阳，以使身体合成足够的维生素D。如果肤色较深，那么晒太阳的时间应该要更长些。 了解家族病史，这会影响你为自己的健康所做的决定。 不在都市，而是在森林进行一次步行。这种方式已经被证明能够舒缓压力、降低血压和心率，同时增强免疫系统。把步行所及的地点记录在锻炼日志中。 到室外去工作。如果恰逢天气晴朗，那就别闷在办公室里，到公园里安排一次会议吧。 放下工作，闻闻玫瑰花香——说真的。一些像常青树这样的自然气息能促进抗癌蛋白的生成。 关掉iPod。森林环境对人体健康大有益处，很可能跟小鸟的鸣唱和森林的声音有关。 安排一次到国家公园或者其他户外场所的旅行。尽量在户外待上三天，经证实，这能够促进某些血液白细胞的生成。 把自己的身体当作一个系统。只有让健康保持平衡，身体才能正常运转。良好的膳食结构和大量的运动都是这个系统需要的。 在高强度锻炼前进行热身运动，能够避免受伤，让表现更加出色。 参加一些需要复杂肢体动作的运动。这样有助于增强运动记忆，从而提高你在其他任务的表现，如编程。 了解自己的BMI值。这种方法单独使用时并没有多少参考价值，但只要结合其他数据，就能为预测疾病风险提供可信的参考。 使用本章推荐的跑步/步行计划来增强有氧运动能力。七周后，你就能一口气跑完2.4公里了。 每周做两到三个循环训练，并在循环训练中加入本章介绍的高强度动作。 在锻炼计划中加入无器械锻炼。如果长时间重复进行某种锻炼，肌肉就会产生适应性，那么锻炼效果就会越来越不明显。 在锻炼活动的尾声进行静态伸展运动。 谈论与健康有关的话题。问问你的同事，他们如何保持健康。告诉你的同事你如何保持健康。为自己的成果感到骄傲，并且与他人分享你的成功经验。 表现出热情。你应该为自己的健康感到自豪，如果身边的人感受到你的自豪，他们势必会跟随你的脚步。 跟老板讨论健康话题。如果你的公司非常关注员工的工作效率，那么他们也许会很乐意参考本书提供的建议。 与同事互动。组织那些大家都可以参与的比赛或者运动小组。不过，要注意尽量选择能适应不同体能条件的运动，尽量让更多人参与进来]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java引用类型]]></title>
      <url>%2Fpost%2Fjava-Reference%2F</url>
      <content type="text"><![CDATA[在JDK 1.2以前的版本中，若一个对象不被任何变量引用，那么程序就无法再使用这个对象。也就是说，只有对象处于可触及（reachable）状态，程序才能使用它。从JDK 1.2版本开始，把对象的引用分为4种级别，从而使程序能更加灵活地控制对象的生命周期。这4种级别由高到低依次为：强引用、软引用、弱引用和虚引用。 强引用（StrongReference）强引用是使用最普遍的引用。如果一个对象具有强引用，那垃圾回收器绝不会回收它。当内存空间不足，Java虚拟机宁愿抛出OutOfMemoryError错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题 ps：强引用其实也就是我们平时A a = new A()这个意思。 代码12345678910111213141516171819@Test public void strongReference() &#123; Object referent = new Object(); /** * 通过赋值创建 StrongReference */ Object strongReference = referent; assertSame(referent, strongReference); referent = null; System.gc(); /** * StrongReference 在 GC 后不会被回收 */ assertNotNull(strongReference); &#125; 软引用（SoftReference）如果一个对象只具有软引用，则内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现内存敏感的高速缓存（下文给出示例）。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。 为什么需要使用软引用我们知道，访问磁盘文件、访问网络资源、查询数据库等操作都是影响应用程序执行性能的重要因素，如果能重新获取那些尚未被回收的Java对象的引用，必将减少不必要的访问，大大提高程序的运行速度。 代码123456789101112131415@Test public void softReference() &#123; Object referent = new Object(); SoftReference&lt;Object&gt; softRerference = new SoftReference&lt;Object&gt;(referent); assertNotNull(softRerference.get()); referent = null; System.gc(); /** * soft references 只有在 jvm OutOfMemory 之前才会被回收, 所以它非常适合缓存应用 */ assertNotNull(softRerference.get()); &#125; 以下程序创建了一个String对象、ReferenceQueue对象和WeakReference对象：1234567//创建一个强引用String str = new String("hello");//创建引用队列, &lt;String&gt;为范型标记，表明队列中存放String对象的引用ReferenceQueue&lt;String&gt; rq = new ReferenceQueue&lt;String&gt;();//创建一个弱引用，它引用"hello"对象，并且与rq引用队列关联//&lt;String&gt;为范型标记，表明WeakReference会弱引用String对象WeakReference&lt;String&gt; wf = new WeakReference&lt;String&gt;(str, rq); 以上程序代码执行完毕，内存中引用与对象的关系如图所示，”hello”对象同时具有强引用和弱引用：`带实线的箭头表示强引用，带虚线的箭头表示弱引用。从图中可以看出，此时”hello”对象被str强引用，并且被一个WeakReference对象弱引用，因此”hello”对象不会被垃圾回收。 在以下程序代码中，把引用”hello”对象的str变量置为null，然后再通过WeakReference弱引用的get()方法获得”hello”对象的引用： 1234567String str = new String("hello"); //① ReferenceQueue&lt;String&gt; rq = new ReferenceQueue&lt;String&gt;(); //② WeakReference&lt;String&gt; wf = new WeakReference&lt;String&gt;(str, rq); //③str=null; //④取消"hello"对象的强引用String str1=wf.get(); //⑤假如"hello"对象没有被回收，str1引用"hello"对象//假如"hello"对象没有被回收，rq.poll()返回nullReference&lt;? extends String&gt; ref=rq.poll(); //⑥ 执行完以上第④行后，内存中引用与对象的关系下图所示： 此时”hello”对象仅仅具有弱引用，因此它有可能被垃圾回收。假如它还没有被垃圾回收，那么接下来在第⑤行执行wf.get()方法会返回 “hello”对象的引用，并且使得这个对象被str1强引用。再接下来在第⑥行执行rq.poll()方法会返回null，因为此时引用队列中没有任何 引用。ReferenceQueue的poll()方法用于返回队列中的引用，如果没有则返回null。 ，在以下程序代码中，执行完第④行后，”hello”对象仅仅具有弱引用。接下来两次调用System.gc()方法，催促垃圾回收器工作，从而提高 “hello”对象被回收的可能性。假如”hello”对象被回收，那么WeakReference对象的引用被加入到ReferenceQueue中， 接下来wf.get()方法返回null，并且rq.poll()方法返回WeakReference对象的引用。 123456789String str = new String("hello"); //①ReferenceQueue&lt;String&gt; rq = new ReferenceQueue&lt;String&gt;(); //② WeakReference&lt;String&gt; wf = new WeakReference&lt;String&gt;(str, rq); //③str=null; //④//两次催促垃圾回收器工作，提高"hello"对象被回收的可能性System.gc(); //⑤System.gc(); //⑥String str1=wf.get(); //⑦ 假如"hello"对象被回收，str1为nullReference&lt;? extends String&gt; ref=rq.poll(); //⑧ 弱引用（WeakReference）弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 代码123456789101112131415@Test public void weakReference() &#123; Object referent = new Object(); WeakReference&lt;Object&gt; weakRerference = new WeakReference&lt;Object&gt;(referent); assertSame(referent, weakRerference.get()); referent = null; System.gc(); /** * 一旦没有指向 referent 的强引用, weak reference 在 GC 后会被自动回收 */ assertNull(weakRerference.get()); &#125; 虚引用（PhantomReference）“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收虚引用主要用来跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之 关联的引用队列中。ReferenceQueue queue = new ReferenceQueue ();PhantomReference pr = new PhantomReference (object, queue);程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取必要的行动。 代码12345678910@Test public void phantomReferenceAlwaysNull() &#123; Object referent = new Object(); PhantomReference&lt;Object&gt; phantomReference = new PhantomReference&lt;Object&gt;(referent, new ReferenceQueue&lt;Object&gt;()); /** * phantom reference 的 get 方法永远返回 null */ assertNull(phantomReference.get()); &#125; 参考： http://blog.csdn.net/mxbhxx/article/details/9111711 http://blog.csdn.net/u011936381/article/details/11709245 https://www.dexcoder.com/selfly/article/313]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-HashMap整理]]></title>
      <url>%2Fpost%2FhashMap-note%2F</url>
      <content type="text"><![CDATA[参考： http://blog.csdn.net/vking_wang/article/details/14166593 http://wiki.jikexueyuan.com/project/java-collection/hashmap.html HashMap数据结构数组数组存储区间是连续的，占用内存严重，故空间复杂的很大。但数组的二分查找时间复杂度小，为O(1)；数组的特点是：寻址容易，插入和删除困难； 链表链表存储区间离散，占用内存比较宽松，故空间复杂度很小，但时间复杂度很大，达O（N）。链表的特点是：寻址困难，插入和删除容易。 哈希表首先HashMap里面实现一个静态内部类Entry，其重要的属性有 key , value, next，从属性key,value我们就能很明显的看出来Entry就是HashMap键值对实现的一个基础bean，我们上面说到HashMap的基础就是一个线性数组，这个数组就是Entry[]，Map里面的内容都保存在Entry[]里面。 1234/** * The table, resized as necessary. Length MUST Always be a power of two. */transient Entry[] table; 那么我们能不能综合两者的特性，做出一种寻址容易，插入删除也容易的数据结构？答案是肯定的，这就是我们要提起的哈希表。哈希表（(Hash table）既满足了数据的查找方便，同时不占用太多的内容空间，使用也十分方便。 哈希表是由数组+链表组成的，一个长度为16的数组中，每个元素存储的是一个链表的头结点。那么这些元素是按照什么样的规则存储到数组中呢。一般情况是通过hash(key)%len获得，也就是元素的key的哈希值对数组长度取模得到 简单地说，HashMap 在底层将 key-value 当成一个整体进行处理，这个整体就是一个 Entry 对象。HashMap 底层采用一个 Entry[] 数组来保存所有的 key-value 对，当需要存储一个 Entry 对象时，会根据 hash 算法来决定其在数组中的存储位置，在根据 equals 方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry 时，也会根据 hash 算法找到其在数组中的存储位置，再根据 equals 方法从该位置上的链表中取出该Entry。 put12345678910111213141516171819202122232425262728 public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); //null总是放在数组的第一个链表中 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); //遍历链表 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //如果key在链表中已存在，则替换为新value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null; &#125; void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); //参数e, 是Entry.next //如果size超过threshold，则扩充table大小。再散列 if (size++ &gt;= threshold) resize(2 * table.length);&#125; get1234567891011121314 public V get(Object key) &#123; if (key == null) return getForNullKey(); int hash = hash(key.hashCode()); //先定位到数组元素，再遍历该元素处的链表 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null;&#125; 初始大小12345678910public HashMap(int initialCapacity, float loadFactor) &#123; ..... // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)(capacity * loadFactor); table = new Entry[capacity]; init(); &#125; rehash当 HashMap 中的元素越来越多的时候，hash冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对 HashMap 的数组进行扩容，数组扩容这个操作也会出现在 ArrayList 中，这是一个常用的操作，而在 HashMap 数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是 resize。 那么 HashMap 什么时候进行扩容呢？当 HashMap 中的元素个数超过数组大小 loadFactor时，就会进行数组扩容，loadFactor的默认值为 0.75，这是一个折中的取值。也就是说，默认情况下，数组大小为 16，那么当 HashMap 中元素个数超过 160.75=12 的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，而这是一个非常消耗性能的操作，所以如果我们已经预知 HashMap 中元素的个数，那么预设元素的个数能够有效的提高 HashMap 的性能。 Fail-Fast 机制我们知道 java.util.HashMap 不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了 map，那么将抛出 ConcurrentModificationException，这就是所谓 fail-fast 策略。ail-fast 机制是 java 集合(Collection)中的一种错误机制。 当多个线程对同一个集合的内容进行操作时，就可能会产生 fail-fast 事件。例如：当某一个线程 A 通过 iterator去遍历某集合的过程中，若该集合的内容被其他线程所改变了；那么线程 A 访问集合时，就会抛出 ConcurrentModificationException 异常，产生 fail-fast 事件。 由所有 HashMap 类的“collection 视图方法”所返回的迭代器都是快速失败的：在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器本身的 remove 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException HashMap 的两种遍历方式1234567 Map map = new HashMap(); Iterator iter = map.entrySet().iterator(); while (iter.hasNext()) &#123; Map.Entry entry = (Map.Entry) iter.next(); Object key = entry.getKey(); Object val = entry.getValue(); &#125; 效率高 12345 Map map = new HashMap(); Iterator iter = map.keySet().iterator(); while (iter.hasNext()) &#123; Object key = iter.next(); Object val = map.get(key); 效率低]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java-ConcurrentHashMap整理]]></title>
      <url>%2Fpost%2Fjava-concurrentHashMap%2F</url>
      <content type="text"><![CDATA[http://onxkn9cbz.bkt.clouddn.com/concurrentHashMap.png参考： http://blog.csdn.net/qq_27093465/article/details/52279473 http://blog.csdn.net/u010723709/article/details/48007881 https://yq.aliyun.com/articles/36781 先说说HashMap HashTableHashMap 线程不安全，并发情况下不能使用HashTable: 线程安全，但是效率低下 HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。 线程1使用put方法时，线程2不但不能使用put方法，也不能使用get方法 ConcurrentHashMap(jdk1.8之前)简介 假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁ReentrantLock，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含多个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构，一个Segment里包含多个HashEntry数组，每个HashEntry是一个链表结构的元素，,当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 ConcurrentHashMap(jdk1.8)简介 与JDK6的版本有很大的差异。实现线程安全的思想也已经完全变了，它摒弃了Segment（锁段）的概念，而是启用了一种全新的方式实现,利用CAS算法。它沿用了与它同时期的HashMap版本的思想，底层依然由“数组”+链表+红黑树的方式思想，但是为了做到并发，又增加了很多辅助的类，例如TreeBin，Traverser等对象内部类。 重要属性sizeCtl可以说它是ConcurrentHashMap中出镜率很高的一个属性，因为它是一个控制标识符，在不同的地方有不同用途，而且它的取值不同，也代表不同的含义。 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。实际容量&gt;=sizeCtl，则扩容 concurrencyLevelconcurrencyLevel，能够同时更新ConccurentHashMap且不产生锁竞争的最大线程数，在Java8之前实际上就是ConcurrentHashMap中的分段锁个数，即Segment[]的数组长度。正确地估计很重要，当低估，数据结构将根据额外的竞争，从而导致线程试图写入当前锁定的段时阻塞；相反，如果高估了并发级别，你遇到过大的膨胀，由于段的不必要的数量; 这种膨胀可能会导致性能下降，由于高数缓存未命中。在Java8里，仅仅是为了兼容旧版本而保留。唯一的作用就是保证构造map时初始容量不小于concurrencyLevel。 重要内部类NodeNode是最核心的内部类，它包装了key-value键值对，所有插入ConcurrentHashMap的数据都包装在这里面。它与HashMap中的定义很相似，但是有一些差别它对value和next属性设置了volatile同步锁，它不允许调用setValue方法直接改变Node的value域，它增加了find方法辅助map.get()方法。 TreeNode 链表&gt;8，才可能转为TreeNode. HashMap的TreeNode继承至LinkedHashMap.Entry；而这里继承至自己实现的Node，将带有next指针，便于treebin访问。 树节点类，另外一个核心的数据结构。当链表长度过长的时候，会转换为TreeNode。但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。而且TreeNode在ConcurrentHashMap集成自Node类，而并非HashMap中的集成自LinkedHashMap.Entry类，也就是说TreeNode带有next指针，这样做的目的是方便基于TreeBin的访问。TreeBin TreeBin用于封装维护TreeNode，包含putTreeVal、lookRoot、UNlookRoot、remove、balanceInsetion、balanceDeletion等方法 这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别。另外这个类还带有了读写锁。 Unsafe与CAS在ConcurrentHashMap中，随处可以看到U, 大量使用了U.compareAndSwapXXX的方法，这个方法是利用一个CAS算法实现无锁化的修改值的操作，他可以大大降低锁代理的性能消耗。这个算法的基本思想就是不断地去比较当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。 3个原子操作 tabAt // 获取索引i处Node casTabAt // 利用CAS算法设置i位置上的Node节点（将c和table[i]比较，相同则插入v）。 setTabAt // 设置节点位置的值，仅在上锁区被调用 put相关这个put方法依然沿用HashMap的put方法的思想，根据hash值计算这个新插入的点在table中的位置i，如果i位置是空的，直接放进去，否则进行判断，如果i位置是树节点，按照树的方式插入新的节点，否则把i插入到链表的末尾。ConcurrentHashMap中依然沿用这个思想，有一个最重要的不同点就是ConcurrentHashMap不允许key或value为null值。另外由于涉及到多线程，put方法就要复杂一点。在多线程中可能有以下两个情况 如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入forward节点，如果检测到需要插入的位置被forward节点占有，就帮助进行扩容 如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多 流程： 判空：null直接抛空指针异常 hash：计算哈希值 遍历table 若table为空，则初始化，仅设置相关参数； 计算当前key存放位置，即table的下标i=(n - 1) &amp; hash； 若待存放位置为null，casTabAt无锁插入； 若是forwarding nodes（检测到正在扩容），则helpTransfer（帮助其扩容）； else（待插入位置非空且不是forward节点，即碰撞了），将头节点上锁（保证了线程安全）：区分链表节点和树节点，分别插入（遇到hash值与key值都与新节点一致的情况，只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点）； 若链表长度&gt;8，则treeifyBin转树（Note：若length&lt;64,直接tryPresize,两倍table.length;不转树）。 addCount resize相关当ConcurrentHashMap容量不足的时候，需要对table进行扩容。这个方法的基本思想跟HashMap是很像的，但是由于它是支持并发扩容的，所以要复杂的多。原因是它支持多线程进行扩容操作，而并没有加锁。我想这样做的目的不仅仅是为了满足concurrent的要求，而是希望利用并发处理去减少扩容带来的时间影响。因为在扩容的时候，总是会涉及到从一个“数组”到另一个“数组”拷贝的操作，如果这个操作能够并发进行，那真真是极好的了。整个扩容操作分为两个部分: 第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。这个单线程的保证是通过RESIZE_STAMP_SHIFT这个常量经过一次运算来保证的，这个地方在后面会有提到； 第二个部分就是将原来table中的元素复制到nextTable中，这里允许多线程进行操作。 多线程又是如何实现的呢？ 遍历到ForwardingNode节点((fh = f.hash) == MOVED)，说明此节点被处理过了，直接跳过。这是控制并发扩容的核心 。由于给节点上了锁，只允许当前线程完成此节点的操作，处理完毕后，将对应值设为ForwardingNode（fwd），其他线程看到forward，直接向后遍历。如此便完成了多线程的复制工作，也解决了线程安全问题。 Size相关由于ConcurrentHashMap在统计size时可能正被多个线程操作，而我们又不可能让他停下来让我们计算，所以只能计量一个估计值。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java多线程简介之休眠、优先级、让步等]]></title>
      <url>%2Fpost%2Fjava-concurrent02%2F</url>
      <content type="text"><![CDATA[休眠影响任务的一种简单方式是调用sleep（），这将使任务中止执行给定的时间。 例程：123456789101112131415161718192021222324252627//: concurrency/SleepingTask.java// Calling sleep() to pause for a while.import java.util.concurrent.*;public class SleepingTask extends LiftOff &#123; public void run() &#123; try &#123; while(countDown-- &gt; 0) &#123; System.out.print(status()); // Old-style: // Thread.sleep(100); // Java SE5/6-style: TimeUnit.MILLISECONDS.sleep(100); &#125; &#125; catch(InterruptedException e) &#123; System.err.println("Interrupted"); &#125; &#125; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for(int i = 0; i &lt; 5; i++) exec.execute(new SleepingTask()); exec.shutdown(); &#125;&#125; /* Output:#0(9), #1(9), #2(9), #3(9), #4(9), #0(8), #1(8), #2(8), #3(8), #4(8), #0(7), #1(7), #2(7), #3(7), #4(7), #0(6), #1(6), #2(6), #3(6), #4(6), #0(5), #1(5), #2(5), #3(5), #4(5), #0(4), #1(4), #2(4), #3(4), #4(4), #0(3), #1(3), #2(3), #3(3), #4(3), #0(2), #1(2), #2(2), #3(2), #4(2), #0(1), #1(1), #2(1), #3(1), #4(1), #0(Liftoff!), #1(Liftoff!), #2(Liftoff!), #3(Liftoff!), #4(Liftoff!),*///:~ 对sleep的调用可以抛出InterruptedException异常，并且你可以看到，它在main()中被捕获，因为异常不能跨越线程传播回main()，所以你必须在本地处理所有在任务内部产生的异常。 优先级线程的优先级先线程的重要性传递给调度器，尽管cpu处理现有线程集的顺序是不确定的，但是调度器更倾向于让优先级更高的线程先执行。线程优先级较低的线程不是不执行，仅仅是执行的频率较低。 getPriority()读取现有的优先级setPriority()来修改它 例程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//: concurrency/SimplePriorities.java// Shows the use of thread priorities.import java.util.concurrent.*;public class SimplePriorities implements Runnable &#123; private int countDown = 5; private volatile double d; // No optimization private int priority; public SimplePriorities(int priority) &#123; this.priority = priority; &#125; public String toString() &#123; return Thread.currentThread() + ": " + countDown; &#125; public void run() &#123; Thread.currentThread().setPriority(priority); while(true) &#123; // An expensive, interruptable operation: for(int i = 1; i &lt; 100000; i++) &#123; d += (Math.PI + Math.E) / (double)i; if(i % 1000 == 0) Thread.yield(); &#125; System.out.println(this); if(--countDown == 0) return; &#125; &#125; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for(int i = 0; i &lt; 5; i++) exec.execute( new SimplePriorities(Thread.MIN_PRIORITY)); exec.execute( new SimplePriorities(Thread.MAX_PRIORITY)); exec.shutdown(); &#125;&#125; /* Output: (70% match)Thread[pool-1-thread-6,10,main]: 5Thread[pool-1-thread-6,10,main]: 4Thread[pool-1-thread-6,10,main]: 3Thread[pool-1-thread-6,10,main]: 2Thread[pool-1-thread-6,10,main]: 1Thread[pool-1-thread-3,1,main]: 5Thread[pool-1-thread-2,1,main]: 5Thread[pool-1-thread-1,1,main]: 5Thread[pool-1-thread-5,1,main]: 5Thread[pool-1-thread-4,1,main]: 5...*///:~ 通过Thread.currentThread()来获取对驱动该任务的Thread对象的引用 尽管JDK有10个优先级，但它与多数操作系统都不能映射得很好，唯一可移植的方法是当调整优先级的时候。只使用 MAX_PRIORITY NORM_PRIORITY MIN_PRIORITY 三种级别 让步如果知道已经完成了在run()方法的循环的一次迭代过程中所需的工作，就可以给线程调度机制一个暗示：你的工作已近做的差不多了，可以让别的线程使用CPU了，这个暗示将通过调用yield()方法作出（不过这只是一个暗示，没有任何机制保证它将被采纳），当调用yield()时，你也是在建议具有相同优先级的其他线程可以运行 后台线程后台线程(deamon)线程，又叫守护线程，是指程序运行的时候在后台提供的一种通用服务线程。这种线程不属于程序中不可或缺的部分，因此，当所有非后台线程结束时，程序也就终止了，同时会杀死进程中的所有后台线程 必须在线程启动调用setDeamon()方法，才能把它设置为后台线程 例程：123456789101112131415161718192021222324252627282930313233343536373839//: concurrency/SimpleDaemons.java// Daemon threads don't prevent the program from ending.import java.util.concurrent.*;import static net.mindview.util.Print.*;public class SimpleDaemons implements Runnable &#123; public void run() &#123; try &#123; while(true) &#123; TimeUnit.MILLISECONDS.sleep(100); print(Thread.currentThread() + " " + this); &#125; &#125; catch(InterruptedException e) &#123; print("sleep() interrupted"); &#125; &#125; public static void main(String[] args) throws Exception &#123; for(int i = 0; i &lt; 10; i++) &#123; Thread daemon = new Thread(new SimpleDaemons()); daemon.setDaemon(true); // Must call before start() daemon.start(); &#125; print("All daemons started"); TimeUnit.MILLISECONDS.sleep(175); &#125;&#125; /* Output: (Sample)All daemons startedThread[Thread-0,5,main] SimpleDaemons@530daaThread[Thread-1,5,main] SimpleDaemons@a62fc3Thread[Thread-2,5,main] SimpleDaemons@89ae9eThread[Thread-3,5,main] SimpleDaemons@1270b73Thread[Thread-4,5,main] SimpleDaemons@60aeb0Thread[Thread-5,5,main] SimpleDaemons@16caf43Thread[Thread-6,5,main] SimpleDaemons@66848cThread[Thread-7,5,main] SimpleDaemons@8813f2Thread[Thread-8,5,main] SimpleDaemons@1d58aaeThread[Thread-9,5,main] SimpleDaemons@83cc67...*///:~ 可以调用isDeamon()方法来确定线程是否是一个后台线程，如果是一个后台线程，那么它创建的任何线程将被自动设置成后台线程 当最后一个非后台线程终止时，后台线程会“突然”终止。 加入一个线程一个线程在其他线程上调用join()方法，其效果是等待一段时间直到第二个线程结束才继续执行，如果某个线程在另一个线程t上调用t.join()，此线程将被挂起。直到目标线程t结束才恢复 也可以在调用join()时带上一个超时参数，这样如果目标线程在这段时间到期时还没有结束的话，join()方法总能返回。 对join()方法的调用可以被中断，做法是在线程上调用interrupt()方法，这需要用到try-catch子句 异常捕获由于线程的本质特性，使得你不能捕获从线程中逃逸的异常，一旦异常逃逸任务的run()方法，他就会向外传播到控制台，除非你采取特殊的步骤捕获这种错误的异常 Thread.UncaughtExceptionHandler是Java SE5中的新接口 ，它允许你在每个Thread对象上附着一个异常处理器 Thread.UncaughtExceptionHandler的uncaughtException()方法会在线程因未捕获的异常而临近死亡时被调用。 示例代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//: concurrency/CaptureUncaughtException.javaimport java.util.concurrent.*;class ExceptionThread2 implements Runnable &#123; public void run() &#123; Thread t = Thread.currentThread(); System.out.println("run() by " + t); System.out.println( "eh = " + t.getUncaughtExceptionHandler()); throw new RuntimeException(); &#125;&#125;class MyUncaughtExceptionHandler implementsThread.UncaughtExceptionHandler &#123; public void uncaughtException(Thread t, Throwable e) &#123; System.out.println("caught " + e); &#125;&#125;class HandlerThreadFactory implements ThreadFactory &#123; public Thread newThread(Runnable r) &#123; System.out.println(this + " creating new Thread"); Thread t = new Thread(r); System.out.println("created " + t); t.setUncaughtExceptionHandler( new MyUncaughtExceptionHandler()); System.out.println( "eh = " + t.getUncaughtExceptionHandler()); return t; &#125;&#125;public class CaptureUncaughtException &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool( new HandlerThreadFactory()); exec.execute(new ExceptionThread2()); &#125;&#125; /* Output: (90% match)HandlerThreadFactory@de6ced creating new Threadcreated Thread[Thread-0,5,main]eh = MyUncaughtExceptionHandler@1fb8ee3run() by Thread[Thread-0,5,main]eh = MyUncaughtExceptionHandler@1fb8ee3caught java.lang.RuntimeException*///:~ 共享受限资源对于并发工作，你需要某种方式来防止两个任务访问相同的资源，至少关键阶段不能出现这种现象 基本上所有的并发模式在解决线程冲突问题的时候，都是采取序列化访问共享资源的方案 synchronizedJava以提供关键字synchronized的形式，为防止资源冲突提供了内置支持要控制对共享资源的访问，得先把它包装进一个对象 LockLock对象必须显示的创建、锁定和释放。与synchronized的形式的相比，代码缺乏优雅性。但是，对于解决某些类型的问题来说。它更加灵活。 如果使用synchronized关键字，某些事物失败了，那么就会抛出一个异常。但是你没有机会去做任何清理工作。以维护系统使其处于良好状态。有了显示的Lock对象，你就可以使用finally子句将系统维护在正常的状态了。 volatilezJVM可以将64位（long和double变量）读取和写入当做两个分离的32来执行，这就产生了一个在读取和写入操作中间发生上下文切换，从而导致不同任务可以看到不正确的结果的可能性（这有时被称为字撕裂） 当定义long和double变量时，如果使用volatile关键字，就会获取原子性 如果一个域完全由synchronized方法或语句块来保护，那就不必将其设置为是volatile的 当一个域的值依赖于它之前的值时，volatile就无法工作了。如果某个域的值受到其他域的值的限制。那么volatile也无法工作。使用volatile而不是synchronized的唯一安全的情况是类中只有一个可变的域。 同步控制块有时我们只希望防止多个线程同时访问方法内部的部分代码而不是防止访问整个方法，通过这种方式分离出来的代码段被称为临界区，她也使用synchronized关键字，这里synchronized被用来指定某对象，此对象的锁被用来对花括号内的代码进行同步控制 123synchronized(syncObject)&#123;&#125; 在进入此段代码前，必须得到syncObject对象的锁。如果其他线程也已经得到这个锁，那么就要等到锁被释放以后，才能进入临界区。 使用它的好处是，可以使多个任务访问对象的时间性能得到显著提高 ThreadLocal防止任务在共享资源上产生冲突的第二种方式是根除对变量的共享，线程本地是一种自动化机制，可以使用相同变量的每个不同线程都创建不同的存储， get()方法将返回与其线程相关联的对象的副本，而set()会将参数数据插入到为其线程储存的对象中。 （注：内容整理自《Thinking in Java》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《软技能-代码之外的生存指南》书摘]]></title>
      <url>%2Fpost%2Fread-softSkill%2F</url>
      <content type="text"><![CDATA[你所犯的最大的错误就是相信自己是在为别人工作，这样一来你对工作的安全感已然尽失，职业发展的驱动力一定是来自个体本身，记住：工作是属于公司的，而职业生涯是属于你自己的–厄尔 南丁格尔 我们中大多数人现在在为公司工作，但是我们的技能和生意都还是自己的，我们随时都能换个地方另起炉找 你只是在为某家公司打工。尽管在你的职业生涯的某个特定时间段里，你可能确实是在为某家公司打工，但是千万不要让那个特定的角色固化了你和你的整个职业生涯——这一点非常重要！ 把雇主当作是你的软件开发企业的一个客户吧。 这是你职业生涯中必须要做的第一要务：转变你的心态，从被一纸“卖身契”束缚住的仆人转变为一名拥有自己生意的商人。在起步阶段就具备这种心态会改变你对职业生涯的思维方式，将此铭记在心，并积极主动地管理自己的职业生涯。 通常软件开发人员售卖的就是他们把一个想法变成一个数字化的现实产品的能力。 集中精力成为一位专家，专门为某一特定类型的客户提供专业的整体服务（记住，作为一个软件开发人员，你只有真正专注于一类客户，才能找到非常好的工作 每个人都是独一无二的，你为自己设立的职业目标肯定与我的不同。但是，要实现任何目标，都必须先知道目标是什么。当然，说比做要容易得多。我发现大多数人，包括软件开发人员在内，要么缺乏对自己人生目标的具体认知，要么没有尽力去实现自己的人生目标，浑浑噩噩混过一生。这是大多数人的自然状态。我们通常不会充分思考自己该关注什么，因而我们的行动也就漫无目标、无的放矢。 为了安逸，我们倾向于遵循已经设计好的路线。要创造自己的路线非常艰难，所以我们不会去做。事实上，我们会接受第一份录用通知提供的工作，一直待下去，直到有更好的机会出现，或者被解雇（我的意思是“下岗”）。 对我来说，我的目标一直都是最终能凭自己的能力走出去，为我自己工作。 在我的例子中，我找到了已经在这家公司工作的开发人员的博客，并与他们建立了联系。于是当有新工作岗位时，获得他们的推荐也就轻而易举。 你应该做的第一件事是确保自己仍旧保持技术能力。如果你力所不及，那么世界上所有的面试技巧都不能帮助你找到工作。确保自己一直阅读技术书籍和博客文章，并会花些时间提升自己的技能。 落入俗套很容易，循规蹈矩也很容易，只要跟其他人做一样的事情就是了。 有大量的软件开发人员并没有具体的专业方向。事实上，大部分软件开发人员完全以自己使用的编程语言来定义自己的专业性。你经常会听到有人说“我是C#开发人员”，或者“我是Java开发人员”，等等。这种专业分工太宽泛了，并不足以说明你能胜任哪种类型的软件开发工作。一门编程语言并不能让我了解你是哪类软件开发人员，也不能告诉我你真正能做什么。它只是让我知道你在工作中使用哪一种工具。 从表面上看，身为“专才”后，潜在雇主和客户群都变小了，但是实际上你对他们更具吸引力了。只要你专业能力雄厚，市场没有过渡饱和，与那些自称为“软件开发人员”的人相比，你能更轻松地找到工作或者赢得客户。 这里有一些技巧来帮你选择自己的专业。在你现在或以前工作的公司里，有哪些主要的痛点？你能成为一名专门解决这些痛点的专家吗？有没有一种特定的工作是无人能做，或者缺乏经验丰富的人？成为这个领域的专家，你就会获得大量业务。在各种会议上和用户组中哪些话题最常出现？哪类问题你回复的最多，无论是针对同事还是在Stack Overflow这样的网站上？ 学富五车，或者灵活变通并同时仍有所专长让自己卓尔不群。如果你非要二选一，那先从专业化开始，再拓展分支。列出 中等规模的公司往往要比大公司还稳定，因为大公司往往还有大裁员或者周期性重组。如果你喜欢稳定，那你会发现中等规模的公司最适合你。 在决定自己要去哪种公司工作的时候，另一个需要考虑的重要因素是下面两类公司之间的区别：一种是软件开发人员只负责内部软件或他们正在生产的部分产品的公司，另一种是生产软件或者做软件开发就是核心业务的公司。 我认识的IT行业人士里有不少人似乎从来就没晋升过。年复一年，他们工作在同一岗位上，停留在同一职位上。我不知道他们是否得到过晋升机会。你认识这样的人吗？这事居然出人意料地常见。如果你不想终老在这条死胡同上，就得做点什么。 在任何公司里能让你脱颖而出的最重要法宝就是承担更多的责任。没有人愿意涉足的领域是搜寻机会最好的地方。另一种间接承担责任的方式是成为团队中其他人的导师， 如果找不到方法让你的老板或高层管理人员知道你在做什么，那你的所有努力都是徒劳的。 另外，不要只学软件开发。如果你把目标设定为更高级别的岗位甚至是行政岗位，你还需要学习领导力、管理和商科的相关知识。 成为专业人士是一种心态。如果我们总是与恐惧、自毁、拖延和自我怀疑作斗争，那么问题就是：我们正在像外行那样思考问题。外行毫不起眼，外行人废话连篇，外行屈从于逆境。专业人士可不这么想。不管怎样，他引人注目，他恪尽职守，他始终如一。 辞掉工作之前，很重要的一点是你对自己实际承担的工作量有一个符合实际的预期，并训练自己提前处理更高强度的工作负荷。在当前工作中，你可以每天追踪自己的时间，看看能不能坚持富有成效地工作6小时。同时，晚上加班做你的那些副业，也会让你做好准备，迎接未来每天8小时或者更长时间的满负荷工作。 如果你想找一份新工作，你能做的最好的投资就是写一份专业的简历。 但对于软件开发人员，最突出也是我个人推荐的还是博客。我认为博客就是你在互联网上的大本营。这是一个你完全能够控制信息的地方，不像在其他的平台上你还要仰人鼻息。 这个策略需要时间，需要持之以恒。随着时间的推移，你写的每一篇博客，你采访的每一期播客，还有你写下的每一本书、每一篇文章，都有助于营销自己，提升你的个人品牌的认知度。最终你在这个领域就成为了权威，拥有了追随者。这些声望转化为更好的机会，最终成就你的事业。 选择某个细分市场，然后以它为核心建立你的品牌，越有针对性越好。如果能充分聚焦，你就可以直接向受众传达信息，也能更轻松地建立品牌的认知度。 即使你对上面提及的博客能带给你的所有好处都打了折扣，有一个好处是你无法轻易抹杀的——提高你的沟通技巧。组织自己的思想，并将其转化为文字，是一项颇具难度却也极具价值的技能。定期写作能帮助你打磨此技能，有了很好的沟通能力会让你在生活的诸多领域受益。此外，如果你能约束自己定期更新博客，你也就在持续刷新自己的技能，保证自己处于自己所在专业领域的前沿。 打造成功博客的最大秘诀有且仅有一个——持之以恒。 另外一些在初始阶段获得流量的好办法就是：在社交网络上分享你的博客文章，在你的电子邮件签名的底部以及所有的在线个人主页中添加你的博客链接。这种方法可能不会产生如你预期的流量，但仍然值得一试。 如果你能帮助足够多的人们得到他们想要的东西，你就会得到自己想要的东西。” 但是，如果你还疏于此道，那么你要去探究一下人们对什么感兴趣。在网络论坛上与你选定的细分领域相关的话题有哪些？你认为行业的整体趋势是什么？以及，或许是最重要的，人们都在害怕什么，对此你又该如何应对？ 教育就是当一个人把在学校所学全部忘光之后剩下的东西。——阿尔伯特•爱因斯坦 这也同样适用于我们做的事情。如果我告诉你该怎么做，你可能会忘掉，但如果你自己动手做一次，你可能就记住了。如果你能将自己所学的东西教给别人，你不仅能记住，还能理解得更深刻 我觉得学习知识的最好方式就是立即将其用于实践 十步学习法： 生活中，特别是在软件开发的职业生涯中，如果你想看到成果，你就必须要学会坐下来，做好自己并不想做的工作——并且要坚持不懈。 你必须认识到，工作最终必须要被完成，所以还不如现在就做，而不是拖到以后；你必须认识到，你要想实现目标，要想发挥出自己的全部潜力，唯一的途径就是自愿咬紧牙关、硬着头皮、开始工作。 当你持有这样的观点之后，你的思维方式可能更倾向于长期而不是短期。你每张工资单上的钱都是辛苦工作赚来的，但是每个月资产为你赚的钱则不需要辛苦工作。如果你用自己工作赚来的钱去购买不需要辛苦工作就能够增值的资产，那么你最终只需要做相同或更少的工作就能赚到更多的钱。如果你每个月把自己工作赚来的钱花在负债上，则会背道而驰，你被迫更辛苦地工作去赚更多的钱，以便继续支付保有这些负债的费用。 也就在那一天，我深刻地意识到，要想有朝一日真正成为富人，我不仅要学会“节流”——不把自己的薪水浪费在负债上，还要学会“开源”——拿出薪水中的很大一部分进行投资，让这些资产帮我赚更多的钱。如果想在财务上获得成功，就必须学会如何投资，别无选择。即使你工作一辈子，竭尽全力存钱，如果找不到理财的方法，你也永远不会变得富有，更遑论财务自由。 自我营销做得越好，声望越高，薪酬谈判就越容易。这一点甚至有可能是最重要的因素。曾经和我一起工作过的一些软件开发人员，他们仅凭自己打造的个人品牌和网络声望就已经能让自己薪水翻一番。 获得工作的另一种方法是通过他人推荐 不要让自己醒悟得太晚，从现在起就认真对待。不要等到出了健康问题才开始关心自己的健康。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《软件随想录-Joel on Software》书摘]]></title>
      <url>%2Fpost%2Fread-Joel-on-Software%2F</url>
      <content type="text"><![CDATA[比尔盖茨对技术的了解令人惊叹，他理解可变数据类型，COM对象，IDispathch接口以及Automation与虚表有何不同，他明白这种不同可能会导致双重接口，因此他担心日期 函数并非心血来潮，如果他信任那个干事的人，他就不会干涉软件，但是，你不要糊弄他，哪怕是一分钟，因为他也是一个程序员，一个真正的，现实的程序员 不懂编程的人管理软件公司，就好像不懂冲浪的人硬要去冲浪 优秀的人才从不在市场上求职 如果你不尊重程序员，你就不会得到优秀的程序员 老实说，只要有两个以上的人待在一起，就会有政治，这很自然，我说“不搞政治”的真正意思是“不搞恶性的政治”程序员早就练出了对公正有非常良好的判断力，代码要么能运行，要么不能，坐在那里争论代码是否有问题，是毫无意义的，因为你可以运行代码，答案自然就有了，代码的世界是非常公正的，也是非常严格有序的，许许多多人选择编程，首要原因就是，他们宁愿将自己的时间花在一个公平有序的地方，一个严格的能者上庸者下的地方，一个只要你是对的就能赢得争论的地方 当你引入新的绩效测量方法时，会有两个阶段的发展，第一阶段，你实际上得到了你想要的东西，因为人们还没有想出作弊的方法，但是，到了第二阶段，你实际上让事情变得比原来更糟，因为每一个人都想出可如何将你测量的指标值最大化的对策，即使代价是毁掉公司，他们也在所不惜 如果不是授课老师SE教授让一些人明白自己其实不具备编程的能力，他们就会有悲惨的职业生涯，一生忙于复制和粘贴大量他人编写的java代码 我职业生涯中的一个重大发现，周而复始得，你会注意到，当程序员遇到问题的时候，他们会把问题重新定义，使得这些问题可以用算法解决，这样一来，问题转化他们可以解决的形式，但是实际上，那些问题是一种“琐碎”问题，也就是说，程序员解决的只是问题的某种外在形式，而并没有解决真正的问题，原因是这些问题非常难，不是表面的算法可以概括的 一个普通的程序员与一个优秀的程序员的区别，不在于他们懂得的编程语言谁多谁少，也不在于他们喜欢用Python语言还是喜欢用java语言，而在于他们能否与他人交流思想，如果你能说服其他人，你的力量就可以得到放大，如果你能写出清晰的注释和技术规格说明书，其他程序员就能够理解你的代码，因此他们就能在自己的代码中使用，而不必重写，如果你做不到这一点，你的代码对其他人就没有价值，如果你能为最终用户写出清晰的使用手册，其他人就能明白你的代码是用来干什么的，这是唯一让别人明白你代码有何价值的方法 为什么计算机系的学生应该学经济学，因为，从经营一家公司的角度来看，比起那些不懂的程序员，一个理解基本商业规则的程序员将会更有价值 计算机科学与软件开发不是一回事 你为麻烦的事情找到了解决方法市场就会向你支付报酬，解决轻而易举的事情是拿不到钱的]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸-财务自由之路》书摘]]></title>
      <url>%2Fpost%2Frich-father-pool-father-financial-free%2F</url>
      <content type="text"><![CDATA[随着富爸爸的成功,他的闲暇却越来越多。我之所以学习到很多关于金钱、财务、商业和生活的知识,原因之一在于我的富爸爸有越来越多的自由时间陪伴我和他的孩子们。 “如果你想成为人群的领导者,那么你首先需要成为语言的主人。”所以,要成为一位伟大的企业主“B”,必要的技能之一就是做语言的主人,学会对不同的人说不同的话。他训练我们先仔细地听别人使用过的词语,然后让我们明白我们是否应该使用这些词语,以及应该何时使用它们以便达到最好的效果。 富爸爸强调说,作为伟大的领导者,我们首先必须是伟大的倾听者,如果你没有听清别人使用的语言,那么你将无法感受他们的灵魂,如果你没有感受到他们的灵魂,那么你永远也不会知道你在跟谁交谈。 真正的企业主“B”可以离开他们的企业一年多,当他们回来时,发现他们的企业比他们离开时更能赢利,运营得也更好。在真正的“S”型企业中,如果“S”离开他的企业一年多,等他回来时,就会发现他的企业已没有什么生意可做了。 要想做一个成功的“B”,需要有： A．对系统的所有权或控制权； B．领导他人的能力。 事实上,我们的企业也可以被看成是一项资产,因为它能够产生收入,而且无需太多的实际投入就能运作。但出于我们个人对财富的认识,我们想确信自己拥有另外一些有形资产,诸如不动产和股票,以带来比我们的支出更多的稳定的收入。因此,我们能够实事求是地说,我们很富有。当资产所带来的收入超过了企业所产生的收入时,我们把企业卖给了我们的合伙人,现在我们真的是很富有了。 每当他们赚到一些钱时,他们就会去购物。他们通常会买一所更大的房子或一辆新车,这些导致了长期负债和更辛苦的工作,并且他们没剩下任何钱流入到自己的资产项目中去。他们挣到的钱消失得如此之快,以致于你可以认为他们是服用了某种财务泻药。 说这些话的人将把增加收入的一半交给政府,并且为此更加努力和更长时间地工作。 迅速变富 我妻子和我要想比较快速地从无家可归的境况达到财务自由,就需要在“B”和“I”象限中工作挣钱。只有在右侧的象限中,你才有可能迅速变富,因为在这里你能合法避税,从而留下更多的钱,通过让这些钱为我们工作,我们很快获得了财务上的自由。 如何获得财务自由 税收和负债是大多数人永远感受不到财务安全或财务自由的两个主要原因。通向财务安全或自由的路径位于现金流象限的右侧,但是在那里你会远离职业的保障。现在你需要了解职业保障,财务安全与财务自由之间的区别。 经理通常把他们的下属看成是不如他们的人,而领导者必须指挥那些通常是更为聪明的人。 1．要想成功,你需要克服对被拒绝的恐惧,并且不去考虑他人对你的评价。我经常遇到一些人,他们畏缩不前,只是因为他们担心如果他们做不同的事情,他的朋友们会怎么说。我很了解他们,因为过去我也是这样。在一个小城镇里,每个人都知道其他人要做什么,如果有人不喜欢你正在做的事情,整个城镇都会知道,并且会有人跳出来干涉你。 这个等级的人经常浪费他们最宝贵的资产——时间,却去试图节省某一分钱。他们花儿个小时从报纸上剪下赠券,然后在超市中,排着长队,笨拙地找到那些赠品。 然而,如果你不愿意学习投资,害怕金融风险,那么储蓄是比其他投资更好的选择。如果你把钱放在银行里,你就不必想很多问题……你的银行家将会善待你。他们为什么不这样做呢？你储蓄1美元,由于派生存款,银行最后实际可向外贷出10到20美元,并收取高达19%的利息,反过来它只付给你不到5%的利息。我想,我们都应该试着成为银行家。 “大多数人投资,”富爸爸说,“95%是用他们的眼睛,而仅有5%是用他们的大脑。”富爸爸继续解释道,当人们购买一项不动产,或者一只股票时,通常是根据他们眼睛所看到的,或者经纪人所告诉他们的,或者一位同事的热情暗示来做出他们的决策。他们通常是用情感而不是理智进行购买。 ,当某人对你说“你不能这样做”时,他可能正用一只手指着你……但是你要用三只手指反过来指向他。 这就好像说：“我股票的p/e值是12,我的公寓的顶利率是12。这是我需要知道的有关我的财富的全部信息吗？答案还是否定的,但这同样意味着一个开始。至少我们开始说同样的语言,使用同样的数字,而这就是金融学即财务知识基础的开始。开始时就是要认识这些语言和数字。”医生说的话来自“S”象限,而后一类人说的语言和数字则出自“I”象限。它们可能也是不同的外语。 我不同意人们说“挣钱首先要花钱”。 依我的观点,用钱挣钱的能力起源于对这些词语和数字的理解。就像我的富爸爸一直说的：“如果钱在你的头脑中没有处于第一位,那么它不会粘到你的手上。” “如果你一无所知,那么任何财务建议都比没有财务建议好。但是如果你不能区分出好建议和坏建议,那么任何财务建议都将是非常危险的。” ,富爸爸告诉我一条重要的规则,这条规则他一直使用,“你的利润是在你购买时…… 而不是卖出时产生的。” 我在家里听到的大部分有关金钱的事情事实上都是人们的建议,而不一定是事实。 实际上,惟一的资产或负债是你自己……因为最终是你把黄金变成资产,而同样你也能把黄金变成负债。这就是为什么财务教育如此重要的原因 关键是大多数人的生活都是由他人的建议而不是事实决定的。要改变一个人的生活,首先要改变他们建议……然后开始观察事实 关键是在这个过程中,你内心所经历的变化和你究竟想成为谁。对于某些人,这个过程很容易,而对于另一些人,这个过程永远不可能实现。 “小心钱瘾的威力,”他经常说,“一旦你习惯了它,就无法脱离得到它的途径。”换句话说,如果你做为雇员挣钱,那么你就倾向于这种获得钱的方式；如果你习惯于作为自由职业者挣钱,那么你就很难改变这种挣钱方式；如果你习惯于政府施舍,那么这也将是一个难于打破的方式。 你任何时候都可以停下来,所以为什么现在停呢？”我决定拖延停止,直到我让事情发生。 关于教育的著名引语丘吉尔曾说：“我时刻准备学习,但是我不是时刻准备着被教导。”伽利略说：“你不能教会一个人任何东西,你只能帮助他找到做事的方法。”马克．吐温说：“我从来不让学校干涉我对自己的核育。”爱因斯坦说：“现在有太多的教育,尤其是在美国的学校里。” 许多大公司通常由财务总监控制运营的原因。”富爸爸继续说,“如果你想在象限右侧获得成功,当涉及到钱时,你必须知道事实和建议之间的区别。你不能盲目听从象限左侧的人的意见。你必须了解你的数字,你必须清楚事实,而数字告诉你事实。” 将明白为什么用寻找职业保障来回避风险是你所做的最危险的事情。你将发展你自己的财务视觉,而不是盲目地接受别人的建议,而且仅仅因为建议者有一个工作头衔,如银行家、股票经纪人、会计师或者其他的什么。你将有能力亲自观察,并知道财务事实和财务建议之间的区别。”这是很有意义的一天。事实上,这是我上过的课程中最令人难忘的一课,因为它开始拓宽了我的视野,使我看到我过去无法看到的东西。 例如,有些事情对我而言没有任何意义,包括为钱财损失提供的税收减免,以及终生负债的做法。再有就是诸如当你的房子是一项负债并且每天还要耗费你的现金时,你也称它为一项资产。或者认为政府花费的开支多于它所获得的税收。或者把孩子送到学校,希望他找到好工作,而不教给他任何的财务知识。 实际上,最努力工作的人最终绝不会富有。如果你想变富,你需要“思考”,独立思考而不是盲从他人。我认为,富人最大的一项资产就是他们的思考方式与别人不同。如果你做别人做的事,你最终只会拥有别人拥有的东西。而对大部分人来说,他们拥有的是多年的辛苦工作,高额的税收和终生的债务。 当有人问我“要从象限左侧转到右侧,我必须做什么”时,我的回答通常是：“关键不是你要做什么,而是你‘思考’做什么以实现你希望的改变,也就是说,为了去‘做’需要被做的事情,你首先需要‘成为’谁。” 老师在黑板上写下三个词：成为—→做—→拥有。 然后说,“目标是这三个词中的‘拥有’。这些目标包括拥有优美的身材、拥有完美的人际关系、拥有几百万美元、拥有健康、拥有名誉等。大部分人一旦确定了他们想要拥有的东西,即他们的目标,他们便开始列出他们要‘做’的事情。因此很多人都有‘要做的事情’的单子。他们确定目标然后开始‘做’。” 她先举了个减肥的例子。“大部分想拥有完美身材的人都节食,而后去健身房。坚持几个星期后,大部分人又开始吃薯条和比萨饼,并且不再去健身房,而是坐在家里看电视。这就是只追求‘做’而不重视‘成为’的例子。” “这不单纯是节食的问题,这是你按照有效的食谱必须成为谁的问题。每年都有很多的人为了保持身材到处寻找完美的食谱。然而遗憾的是他们都把注意力集中在他们必须做的事情上,而不是他们必须成为什么样的人上,这种思想若得不到改变,再好的食谱也不会起作用。” 她又举了另外一个例子：“许多人寄希望于通过购买新的高尔夫球用具来改进他们的技能,而不是用一个职业高尔夫球手的态度、思维方式和信念进行训练。拥有一套新高尔夫球用具后的烦躁的高尔夫球手仍是一个烦躁的高尔夫球手。” 接着,她讨论起投资：“许多人认为买股票或共同基金能使他们变富。然而,事实是仅仅购买股累、共同基金、不动产和债券并不能使你变富。单纯地做职业投资者做的事情并不能保证财务上的成功,一个拥有失败者心理状态的人将一直失败,无论他们购买什么样的股票、债券、不动产或共同基金。” 另外一个案例是关子人际关系的：“在婚姻中,很多人试图改变对方,以使婚姻变得更幸福。而当他们试图改变对方时,这种做法往往会引起争斗,最好的做法应是先改变你自己。”她说,“不要在对方身上下功夫,要在你对对方的看法上下功夫。” 在那个周末班结束之后,我意识到,许多人在“做”富人应该做的事情,尽力“拥有”富人拥有的东西。他们通常会购买大房子,投资于股市,因为这是他们认为富人才做的事情。然而,富爸爸尽力想告诉我的是,如果他们仍旧持有穷人或中产阶级持有的信念和思想,难做着富人做的事情,那么他们最终将仍旧拥有穷人和中产阶级拥有的东西——财务困境,并且肯定会变得更加困窘。“成为—→做—→拥有”开始变得有意义。 。作为一个成年人就意味着你知道你必须做什么并且去做,即使你可能感觉并不喜欢做这件事。 财商与情商紧密相连。我认为,大多数遭受财务痛苦的人是因为他们的情感控制着他们的思想。 作为人类,我们都有相同的情感。决定我们在生活中“做”不同的事情和“拥有”不同的东西的主要原因是我们如何对待这些情感。 例如,恐惧感能使一些人成为懦夫,同样的恐惧感可以使另一些人变得有勇气,不幸的是,在金钱方面,我们社会中的大多数人被限定为财务方面的懦夫。当对赔钱的恐惧上升时,大多数人的脑海里会自动地回响起这样一些话： 1．“安全”,而不是“自由”。 2．“避开风险”,而不是“学会管理风险”。 3．“平稳地做事”,而不是,聪明地做事”。 4．“我支付不起”,而不是“我怎样能支付得起”。 5．“太贵了”,而不是“长期看,它值多少钱呢”。 6．“多样化”,而不是“集中化”。 7．“我的朋友们会怎样想”,而不是“我怎样想”。 我们都听说过这句话：“千里之行,始于足下。”我把这句话略微修改了一下,我将说：“千里之行,始于初步。”我强调这点是因为我见过太多的人试图向前迈出“伟大的一步”而不是采取初级步骤进行。我们都看见过这样一些人,他们的身体非常肥胖,于是决定减掉20磅的体重,并恢复好的体型。 他们开始疯狂节食,每天去两个小时的健身房,然后慢跑10英里。这样持续了可能1个星期后,他们减轻了几磅重量,然而这时痛苦、烦躁和饥饿开始消磨他们的意志力和决心。到了第三周,他们的老毛病——过度饮食、缺乏锻炼和看电视再次失控。 我的建议是：不要总想向前迈出“伟大的一步”,而是要采用初级步骤法前进。长期的财务成功不是用你的步子有多大来衡量的,而是用步数。前进的方向和年数来衡量的。事实上,这是适用于任何成功或失败的公式。对于金钱,我见过太多的人,包括我自己,试图用太少的付出做出太多的事情……然而这样做的后果只能是崩溃和灭亡。在你首先需要一把梯子把自己从你亲手挖的财务深坑里解救出来时,你很难向前迈出哪怕是一小步。 该研究发现,这些人无论生活在哪个国家,都具备三种特性： 1．他们持有长期的展望和计划。 2．他们相信延迟的回报。 3．他们以有利于自己的方式运用复利力量。 。该研究发现这些人拥有以下三种特征： 1．他们目光短浅。 2,他们渴望即时回报。 3．他们滥用复利力量。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《算法图解》书摘-狄克斯特拉算法贪婪算法等]]></title>
      <url>%2Fpost%2Falgorithm-illustration-04%2F</url>
      <content type="text"><![CDATA[第七章 狄克斯特拉算法 前一章使用了广度优先搜索，它找出的是段数最少的路径（如第一个图所示）。如果你要找出最快的路径（如第二个图所示），该如何办呢？为此，可使用另一种算法——狄克斯特拉算法（Dijkstra’s algorithm）。 狄克斯特拉算法包含4个步骤。 找出最便宜的节点，即可在最短时间内前往的节点。 对于该节点的邻居，检查是否有前往它们的更短路径，如果有，就更新其开销。 重复这个过程，直到对图中的每个节点都这样做了。 计算最终路径。（下一节再介绍！） 小结 广度优先搜索用于在非加权图中查找最短路径。 狄克斯特拉算法用于在加权图中查找最短路径。 仅当权重为正时狄克斯特拉算法才管用。 如果图中包含负权边，请使用贝尔曼福德算法。 贪婪算法/动态规划/k最近邻算法 就是你每步都选择局部最优解，最终得到的就是全局最优解 有时候，你只需找到一个能够大致解决问题的算法，此时贪婪算法正好可派上用场，因为它们实现起来很容易，得到的结果又与正确结果相当接近。 贪婪算法寻找局部最优解，企图以这种方式获得全局最优解。 对于NP完全问题，还没有找到快速解决方案。 面临NP完全问题时，最佳的做法是使用近似算法。 贪婪算法易于实现、运行速度快，是不错的近似算法。 需要在给定约束条件下优化某种指标时，动态规划很有用。 问题可分解为离散子问题时，可使用动态规划来解决。 每种动态规划解决方案都涉及网格。 单元格中的值通常就是你要优化的值。 每个单元格都是一个子问题，因此你需要考虑如何将问题分解为子问题。 没有放之四海皆准的计算动态规划解决方案的公式。 KNN用于分类和回归，需要考虑最近的邻居。 分类就是编组。 回归就是预测结果（如数字）。 特征抽取意味着将物品（如水果或用户）转换为一系列可比较的数字。 能否挑选合适的特征事关KNN算法的成败。 其他一些算法树反向索引傅里叶变换 Better Explained是一个杰出的网站，致力于以通俗易懂的语言阐释数学，它就傅里叶变换做了一个绝佳的比喻：给它一杯冰沙，它能告诉你其中包含哪些成分。换言之，给定一首歌曲，傅里叶变换能够将其中的各种频率分离出来。-JPG也是一种压缩格式，也采用了刚才说的工作原理。傅里叶变换还被用来地震预测和DNA分析。 并行算法MapReduce MapReduce是一种流行的分布式算法，你可通过流行的开源工具Apache Hadoop来使用它。 分布式算法非常适合用于在短时间内完成海量工作，其中的MapReduce基于两个简单的理念：映射（map）函数和归并（reduce）函数。 MapReduce使用这两个简单概念在多台计算机上执行数据查询。数据集很大，包含数十亿行时，使用MapReduce只需几分钟就可获得查询结果，而传统数据库可能要耗费数小时。 布隆过滤器和HyperLogLogSHA算法-安全散列算法（secure hash algorithm，SHA）函数。 Diffie-Hellman 密钥交换线性规划]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《算法图解》书摘-散列表/广度优先搜索]]></title>
      <url>%2Fpost%2Falgorithm-illustration-03%2F</url>
      <content type="text"><![CDATA[第五章 散列表 散列函数“将输入映射到数字” 散列函数总是将同样的输入映射到相同的索引 散列函数将不同的输入映射到不同的索引 散列函数知道数组有多大，只返回有效的索引 而散列表也使用数组来存储数据，因此其获取元素的速度与数组一样快。 散列表适合用于 模拟映射关系； 防止重复； 缓存/记住数据，以免服务器再通过处理来生成它们。 如果两个键映射到了同一个位置，就在这个位置储存一个链表 经验 散列函数很重要。前面的散列函数将所有的键都映射到一个位置，而最理想的情况是，散列函数将键均匀地映射到散列表的不同位置。 如果散列表存储的链表很长，散列表的速度将急剧下降。然而，如果使用的散列函数很好，这些链表就不会很长！ 在最糟情况下，散列表所有操作的运行时间都为O(n)——线性时间 在平均情况下，散列表的查找（获取给定索引处的值）速度与数组一样快，而插入和删除速度与链表一样快，因此它兼具两者的优点！但在最糟情况下，散列表的各种操作的速度都很慢。 因此，在使用散列表时，避开最糟情况至关重要。为此，需要避免冲突。而要避免冲突，需要有1.较低的填装因子2.良好的散列函数。 填装因子大于1意味着商品数量超过了数组的位置数。一旦填装因子开始增大，你就需要在散列表中添加位置，这被称为调整长度（resizing）。 一个不错的经验规则是：一旦填装因子大于0.7，就调整散列表的长度。 什么样的散列函数是良好的呢？你根本不用操心——天塌下来有高个子顶着。如果你好奇，可研究一下SHA函数 小结 你几乎根本不用自己去实现散列表，因为你使用的编程语言提供了散列表实现。你可使用Python提供的散列表，并假定能够获得平均情况下的性能：常量时间。 散列表是一种功能强大的数据结构，其操作速度快，还能让你以不同的方式建立数据模型。你可能很快会发现自己经常在使用它。 你可以结合散列函数和数组来创建散列表。 冲突很糟糕，你应使用可以最大限度减少冲突的散列函数。 散列表的查找、插入和删除速度都非常快。 散列表适合用于模拟映射关系。 一旦填装因子超过0.7，就该调整散列表的长度。 散列表可用于缓存数据（例如，在Web服务器上）。 散列表非常适合用于防止重复。 第六章 广度优先搜索 广度优先搜索指出是否有从A到B的路径。 如果有，广度优先搜索将找出最短路径。 面临类似于寻找最短路径的问题时，可尝试使用图来建立模型，再使用广度优先搜索来解决问题。 有向图中的边为箭头，箭头的方向指定了关系的方向，例如，rama→adit表示rama欠adit钱。 无向图中的边不带箭头，其中的关系是双向的，例如，ross - rachel表示“ross与rachel约会，而rachel也与ross约会”。 队列是先进先出（FIFO）的。 栈是后进先出（LIFO）的。 你需要按加入顺序检查搜索列表中的人，否则找到的就不是最短路径，因此搜索列表必须是队列。 对于检查过的人，务必不要再去检查，否则可能导致无限循环。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《算法图解》书摘-递归/快速排序]]></title>
      <url>%2Fpost%2Falgorithm-illustration-02%2F</url>
      <content type="text"><![CDATA[第三章 递归 递归只是让解决方案更清晰，并没有性能上的优势。实际上，在有些情况下，使用循环的性能更好。 “如果使用循环，程序的性能可能更高；如果使用递归，程序可能更容易理解。如何选择要看什么对你来说更重要。” 编写递归函数时，必须告诉它何时停止递归。正因为如此，每个递归函数都有两部分：基线条件（base case）和递归条件（recursivecase）。递归条件指的是函数调用自己，而基线条件则指的是函数不再调用自己，从而避免形成无限循环。 小结 递归指的是调用自己的函数。 每个递归函数都有两个条件：基线条件和递归条件。 栈有两种操作：压入和弹出。 所有函数调用都进入调用栈。 调用栈可能很长，这将占用大量的内存。 第四章 快速排序 我们将探索分而治之（divide and conquer，D&amp;C）——一种著名的递归式问题解决方法。你将学习第一个重要的D&amp;C算法——快速排序 快速排序代码 123456789def quicksort(array)： if len(array) &lt; 2: return array else: pivot = array[0] less = [i for i array[1:] if i &lt;= pivot] greater = [i for i in array[i:] if i &gt; pivot] return quicksort(less) + [pivot] + quicksort(greater) print quicksort([10,5,2,3]) 整个算法需要的时间为O(n) O(log n) = O(n log n)。这就是最佳情况。在最糟情况下，有O(n)层，因此该算法的运行时间为O(n) O(n) = O(n2)。知道吗？这里要告诉你的是，最佳情况也是平均情况。只要你每次都随机地选择一个数组元素作为基准值，快速排序的平均运行时间就将为O(n logn)。快速排序是最快的排序算法之一 小结 D&amp;C将问题逐步分解。使用D&amp;C处理列表时，基线条件很可能是空数组或只包含一个元素的数组。 实现快速排序时，请随机地选择用作基准值的元素。快速排序的平均运行时间为O(n log n)。 大O表示法中的常量有时候事关重大，这就是快速排序比合并排序快的原因所在。 比较简单查找和二分查找时，常量几乎无关紧要，因为列表很长时，O(log n)的速度比O(n)快得多。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《算法图解》书摘-算法介绍/选择排序]]></title>
      <url>%2Fpost%2Falgorithm-illustration-01%2F</url>
      <content type="text"><![CDATA[第一章 算法介绍二分查找Python代码1234567891011121314def binary_search(list,item): low = 0 high = len(list-1) while low &lt;= high: mid =(low+high) guess = list[mid] if guess == item: return mid if guess &gt; item： high = mid -1 else: low = mid + 1 return None 小结 二分查找的速度比简单查找快得多 O(log n)比O(n)快，需要搜索的元素越多，前者比后者就快得越多 算法运行时间并不是以秒为单位 算法运行时间是从其增速的角度度量的 算法运行时间用大O表示法表示 第二章 选择排序 很多算法仅在数据经过排序后才管用.选择排序是下一章介绍的快速排序的基石 数组和链表操作运行时间 # 数组 链表 读取 o(1) o(n) 输入 o(n) o(1) 删除 o(n) o(1) 需要随机读取元素时，数组效率高，当需要中间插入元素时，链表是更好的选择，链表只能随机访问 选择排序，需要的总时间是O(nxn) 示例代码 123456789101112131415def findSmallest(arr): smallest = arr[0] smallest_index = 0 for i in range(1,len(arr)): if arr[i] &lt; smallest; smallest = arr[i] smallest_index = i return smallest_index def selectionSort(arr): newArr = [] for i in range(len(arr)): smallest = findSmallest(arr) newArr.append(arr.pop(smallest))return newArr 小结 计算机内存犹如一大堆抽屉。 需要存储多个元素时，可使用数组或链表。 数组的元素都在一起。 链表的元素是分开的，其中每个元素都存储了下一个元素的地址。 数组的读取速度很快。 链表的插入和删除速度很快。 在同一个数组中，所有元素的类型都必须相同（都为int、double等）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《HeadFirst设计模式》书摘-状态模式/代理模式]]></title>
      <url>%2Fpost%2FheadFirstDesignPatterns05%2F</url>
      <content type="text"><![CDATA[状态模式定义允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它的类 get到的oo原则类应该只有一个改变的理由 要点 状态模式允许一个对象基于内部状态而拥有不同的行为 和程序状态机(PSM)不同，状态模式用类代表状态 context会将行为委托给当前状态对象 通过将每个状态封装进一个类，我们把以后需要做的任何改变局部化了 状态模式和策略模式有相同的类图，但是他们意图不同 策略模式通常会用行为或算法来配置context类 状态模式允许随着状态的改变而改变行为 状态转换可以由state类或context类控制 使用状态模式通常会导致设计中类的数目大量增加 状态类可以被多个context实例共享 代理模式定义为另一个对象提供一个替身或占位符以访问这个对象 要点 代理模式为另一个对象提供代表，以便控制客户对对象的访问，管理访问的方式有许多种 远程代理管理客户和远程对象之间的交互 虚拟代理控制访问实例化开销大的对象 保护代理基于调用者控制对象方法的访问 代理模式有许多变体，例如：缓存代理，同步代理，防火墙代理和写入时复制代理 代理在结构上类似装饰者，但是目的不同 装饰者模式为对象加上行为，而代理则是控制访问 java内置的代理支持，可以根据需要建立动态代理，并将所要调用分配到所选的处理器 就和其他包装者一样，代理会造成你的设计中的类数目增加]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《HeadFirst设计模式》书摘-模板方法/模式迭代器/组合]]></title>
      <url>%2Fpost%2FheadFirstDesignPatterns04%2F</url>
      <content type="text"><![CDATA[模板方法模式定义在一个方法中定义一个算法的骨架，而将一些步骤延迟到子类中，模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤 get到的oo原则 别找我，我会找你 要点 “模板方法”定义了算法的步骤，把这些步骤的实现延迟到子类 模板方法模式为我们提供了一种代码复用的重要技巧 模板方法的抽象类可以定义具体方法，抽象方法 抽象方法由子类实现 钩子是一种方法，它在抽象类中不做事，或者只做默认的事情，子类可以选择不要去覆盖它 为了防止子类改变模板方法中的算法，可以将模板方法声明为final 好莱坞原则告诉我们，将决策权放在高层模块中，以便决定如何以及何时调用低层模块 你将在真实世界代码中看到模板方法模式的许多变体，不要期待他们全都是一眼就可以被认出来的 策略模式和模板方法模式都是封装算法，一个用组合，一个用继承 工厂方法时模板方法一种特殊版本 迭代器模式/组合模式迭代器模式定义提供一种方法顺序访问一个聚合对象的各个元素，而不暴露内部的表示 组合模式定义允许你将对象组成树形结构来表现“整体/部分”的层次结构，组合能让客户以一致的方式处理个别对象和对象组合 要点 迭代器允许访问聚合的元素，而不需要暴露它的内部结构 迭代器将遍历聚合的工作封装进一个对象中 当使用迭代器的时候，我们依赖聚合提供遍历 迭代器提供了一个通用的接口，让我们遍历聚合的项，当我们编码使用聚合的项时，就可以使用多态机制 我们应该努力让一个类只分配一个责任 组合模式提供一个结构可同时包容个别对象和组合对象 组合模式允许客户对个别对象以及组合对象一视同仁 组合结构内的任意对象称为组件，组件可以是组合，也可以是叶子节点]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《HeadFirst设计模式》书摘-单例/命令/适配器/外观]]></title>
      <url>%2Fpost%2FheadFirstDesignPatterns03%2F</url>
      <content type="text"><![CDATA[单例模式定义确保一个类只有一个实例，并提供全局访问点 要点 单例模式也提供访问这个实例的全局点 在java中实现单例模式需要私有的构造器，一个静态方法和一个静态变量 确定在性能和资源上的限制，然后小心地选择适当的方案来实现单例模式，以解决多线程问题 小心，如果使用多个类加载器，可能导致单件失效而产生多个实例 如果使用jvm1 2或之前的版本，你必须建立单例注册表，以免垃圾收集器将单例回收 命令模式定义将请求封装成对象，这可以让你使用不用的请求，队列，或者日志请求来参数化其他对象，命令模式也可以支持撤销操作 要点 命令模式将发出请求的对象和执行请求的对象解耦 在被解耦的两者之间是通过命令对象进行沟通的，命令对象封装了接受者和一个或一组动作 调用者通过调用命令对象的execute()发出请求，这会使得接收者的动作被调用 调用者可以接受命令当做参数甚至在运行时动态的进行 命令可以支持撤销，做法是实现一个undo方法来来回到execute()地执行的状态 宏命令是命令的一种简单的延伸，允许调用多个命令，宏方法也可以支持注销 实际操作时，很常见用“聪明”命令模式，就是直接实现了请求，而不是将工作委托给接收者 命令模式也可以用来实现日志和实务系统 适配器模式/外观模式适配器模式定义将一个类的接口，转换成客户期望另一个接口，适配器让原来不兼容的类可以工作无间 外观模式定义提供一个统一的接口，用来访问子系统中的一群接口，外观定义了一个高层接口，让子系统更容易使用 get到的oo原则只和朋友交谈 要点 当需要使用一个现有的类而其接口并不符合你的需要时，就使用适配器 当需要简化并统一一个很大的接口或者一群复杂的接口时，使用外观模式 适配器改变接口以符合客户的期望 外观将客户从一个复杂的子系统中解耦 实现一个适配器可能需要一番功夫，也可能不费功夫，视目标接口的大小和复杂度而定 实现一个外观，需要将子系统组合进外观中，然后将工作委托给子系统执行 适配器模式有两种形式：对象适配器和类适配器，类适配器需要用到多重继承 你可以为一个子系统实现一个以上的外观 适配器将一个对象包装起来以改变其接口，装饰者将一个对象包装起来以增加新的行为和责任，而外观将一群对象包装起来以简化其接口]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《HeadFirst设计模式》书摘-装饰者模式/抽象工厂模式/工厂方法模式]]></title>
      <url>%2Fpost%2FheadFirstDesignPatterns02%2F</url>
      <content type="text"><![CDATA[装饰者模式定义动态地将责任附加到对象上，想要扩展功能，装饰者提供有别于继承的另一种选择 get到的oo原则 对扩展开放，对修改关闭 要点 继承属于扩展形式之一，但不见得是达到弹性设计的最佳方式。 在我们的设计中，应该允许行为可以被扩展，而无须修改现有的代码。 组合和委托可用于在运行时动态地加上新的行为。 除了继承，装饰者模式也可以让我们扩展行为。 装饰者模式意味着一群装饰者类， 这些类用来包装具体组件。 装饰者类反映出被装饰的组件类型（事实上，他们具有相同的类型，都经过接口或继承实现）。 装饰者可以在被装饰者的行为前面与/或后面加上自己的行为， 甚至将被装饰者的行为整个取代掉，而达到特定的目的。 你可以用无数个装饰者包装一个组件。 装饰者一般对组件的客户是透明的，除非客户程序依赖于组件的具体类型。 装饰者会导致设计中出现许多小对象，如果过度使用，会让程序变得很复杂。 抽象工厂模式/工厂方法模式抽象工厂模式定义提供一个接口，用于创建相关或依赖对象的家族，而不需要明确指定具体类 工厂方法模式定义定义一个创建对象的接口，但由子类绝对实例化的类是哪一个，工厂方法让类把实例化推迟到子类 get到的oo原则 依赖抽象，不是依赖具体类 要点 所有的工厂都是用来对象的创建 简单工厂，虽然不是真正的设计模式，但仍不失一个简单的方法，可以将客户端程序从具体类解耦 工厂方法使用继承，把对象的创建委托给子类，子类实现工厂方法来创建对象 抽象工厂使用对象组合，对象的创建被实现在工厂接口所暴露出来的方法中 所有工厂模式都通过减少应用程序和具体类之间的依赖促进松耦合 工厂方法允许类将实例化延迟到子类进行 抽象工厂创建相关的对象家族，而不需要依赖他们的具体类 依赖倒置原则，指导我们避免依赖具体类型，而要尽量依赖抽象 工厂是很有威力的技巧，帮助我们针对抽象变成，而不是针对具体类编程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《HeadFirst设计模式》书摘-策略模式/观察者模式]]></title>
      <url>%2Fpost%2FheadFirstDesignPatterns01%2F</url>
      <content type="text"><![CDATA[策略模式定义：定义算法族，分别封装起来，让他们之间可以互相替换，此模式让算法的变化独立于使用算法的客户 oo基础：抽象 封装 多态 继承 get到的oo原则： 封装变化 多用组合，少用继承 针对接口编程，不针对实现编程 要点 知道O O基础，并不足以让你设计出良好的O O系统。 良好的O O设计必须具备可复用、可扩充、可维护三个特性 模式可以让我们建造出具有良好O O 设计质量的系统。 模式被认为是历经验证的O O设计经验。 模式不是代码，而是针对设计问题的通用解决方案。你把它们应用到特定的应用中。 模式不是被发明，而是被发现 大多数的模式和原则，都着眼于软件变化的主题。 大多数的模式都允许系统局部改变独立于其他部分。 我们常把系统中，会变化的部分抽出来封装 模式让开发人员之间有共享的语言， 最大化沟通的价值。 观察者模式定义在对象之间定义一对多的依赖，这样一来，当一个对象改变状态，依赖它的对象都会收到通知，并自动更新。 get到的oo原则为交互对象之间的松耦合设计而努力 要点 观察者模式定义了对象之间一对多的关系。 主题（也就是可观察者）用一个共同的接口来更新观察者 观察者和可观察者之间用松耦合方式结合（loosecoupling），可观察者不知道观察者的细节，只知道观察者实现了观察者接口。 使用此模式时，你可从被观察者处推（push）或拉（pull）数据（然而，推的方式被认为更“正确”）。 有多个观察者时，不可以依赖特定的通知次序。 J a v a有多种观察者模式的实现，包括了通用的java.util.Observable。 要注意java.util.Observable实现上所带来的一些问题。 如果有必要的话，可以实现自己的Observable，这并不难，不要害怕。 Swing大量使用观察者模式，许多GUI框架也是如此。 此模式也被应用在许多地方，例如：JavaBeans、RMI。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-开始行动]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father10%2F</url>
      <content type="text"><![CDATA[我相信,我们每个人都拥有内在的理财天赋,问题是,这种理财天赋一直处于休眠状态。这种天赋处于休眠状态的原因,是因为我们的文化把对金钱的需要视为万恶之源,并把这种观念灌输给了我们,这种观念促使我们学习某种技能,并为金钱而工作,却没能教给我们如何让金钱来为我们而工作。我们被告知不必去担忧将来的财务状况,因为一旦我们退休了,公司或者政府会照顾我们。然而,现在在同样的学校体制下受教育的我们的孩子们,他们将来却有可能不再向公司和政府支付提供这种照顾所需要的钱。可现有的信息仍告诉我们努力工作,挣钱维生,当缺钱时,我们总能借到钱。 我建议你采取以下十个步骤来开发上帝赐予你的才能,这种才能只有你才能控制：1.我需要一个超现实的理由：精神的力量。如果你问别人是否愿意致富或者获得财务上的自由,大部人会说“愿意”。可是一想到现实,前进的道路似乎就变得很漫长而崎岖,相比之下,为了钱工作并把剩余的钱托付给经纪人看管似乎要更容易一些。 首先投资于教育。实际上,当你还是一个穷人时,你所拥有的惟一真正的资产就是你的头脑,这是我们所控制的最强有力的工具。就像我说的选择的力量那样,当我们逐渐长大时,每个人都要选择向自己的大脑里注人些什么样的知识。你可以整天看电视,也可以阅读高尔夫球杂志、上陶艺辅导班或者上财务计划培训班,你可以进行选择。在投资方面,大部分人选择的是直接投资于某种项目,而不是首先投资于学习自己所要投资项目的有关知识 傲慢自大或吹毛求疵的人往往是缺乏自信而不敢冒风险的人。如果你想学习某些新东西,那么你就需要犯些错误,只有这样才能充分理解你所学习的知识。如果你能读到这里,你就不存在傲慢自大的问题,因为傲慢的人很少读书或买磁带。为什么？因为他们自以为是宇宙的中心。 一个真正聪明的人欢迎新思想,因为新思想能够增加已日积月累的思想库中的内容。听比说更重要,否则,上帝就不会给我们安排两只耳朵,却只安排一张嘴巴了。有太多的人爱说而不爱听,这样就等于放弃了吸收更多新思想和可能性的机会,他们爱争论问题而不是提出问题并倾听别人的见解。 我愿意以长远的眼光来看待我的财富,我并不相信那些买彩票的人或赌博者“快速致富”的观念。我可能会做短期股票买卖,但从长远考虑我更重视教育。如果你想驾驶飞机,我建议你首先去听有关飞行原理的课程而不是直接坐进驾驶舱。有的人投资于股票或房地产,却从不投资于他们最重要的资产——头脑,对此我常常感到震惊。因为,你买一两套房地产并不能让你成为房地产方面的专家。“3.慎重地选择朋友：关系的力量。首先,我不会把财务状况作为挑选朋友的标准。我既有穷困潦倒的朋友,也有家财万贯的朋友,因为我相信”三人行,必有我师“,而我也愿意努力地去向他们每个人学习。 我想说,在积累财富的过程中,最困难的事情莫过于坚持自己的选择而不盲目从众。 精明的投资者不会抱怨市场时机不对,如果错过了这一拨,他们就会去寻找下一个机会,并且在其中找到自己的位置。对大多数投资者来说,做到这一点之所以非常困难,是因为一旦他们买人的东西不那么流行,他们就会感到害怕。胆小的投资者总是亦步亦趋地跟在众人后面,当欲望终于驱使他们冒险投资时,精明的投资者此刻已经获利退出了。明智的投资者往往购买一项不太流行的投资,他们懂得盈利是在购买时就已获得,而不是在出售时获得的,他们会耐心地等待时机实现投资增值。正如我所说,他们并不计较市场时机,就像一位冲浪者,他们时刻等待着下一个大浪来将他们高高托起。 在钱的问题上,大多数人一般只知道一个基本的挣钱公式,这个公式是他们从学校学来的,就是为了金钱而工作。在我看来,这个公式是在全世界占支配地位的一个公式：千百万人每天起床,上班,挣钱,支付账单,平衡支票簿,购买共同基金,然后再回去工作。 我总是在寻找赚钱更迅速的公式,这就是为什么在条件差不多的情况下,我每天所挣的钱总是比许多人在一生当中所挣的钱还要多。 为金钱而工作是人类在穴居时代产生的一个公式,它早已过时了。 开创你自己的事业所必备的最重要的三种管理技能是：1.现金流量管理；2.人事管理；3.个人时间管理。 有胆量不随大溜才能致富。你可能并不是一个软弱的人,但是一旦涉及到金钱,许多人往往会怯懦起来。 穷人有不好的习惯,一个普遍的坏习惯是随便“动用储蓄”。富人知道储蓄只能用于创造更多的钱,而不是用来支付账单。我知道这样说听起来很刺耳,但是正如我说过的那样,如果你意志不够坚定,那么无论如何,你只能让世界推着你转。 确实,在许多情况下我曾损失过资金,但我总是能负担得起损失的资金。我想,在平均每十项投资中,我会有2～3项赢利,同时5—6项不赚不赔,2～3项亏本。但是我会将自己可能发生的损失限制在那个时期我所拥有的资金量这一范围内。 因此明智的投资者必定不光看到投资回报率,而且还要看到一旦收回投资,你因此所拥有的资产就如同白得。这也是财商。 在“首先支付你自己”一节中,我说到如果一个人没有自律的能力,最好别想着去致富。因为从理论上来讲,一项资产产生现金流量的过程是容易的,但是拥有控制金钱的坚强意志却是困难的。由于种种外在的诱惑,在今天的消费者世界里,在支出项目上挥霍金钱更加容易。因为意志薄弱,金钱的流出简直会无遮无拦,这就是大多数人贫困和财务困窘的原因。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-克服困难]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father09%2F</url>
      <content type="text"><![CDATA[仅掌握财务知识的人很多时候仍然不能拥有充裕的资产项目,其主要原因有五个：1.恐惧心理；2.愤世嫉俗；3.懒惰；4.不良习惯；5.自负 对可能损失金钱的畏惧心理。我从来没有遇到过喜欢损失金钱的人,但在我的一生中,也从来没有遇到过一位从未损失过金钱的富人。可我曾经遇到过许多从未损失过一毫的穷人——我是说在投资活动中 不仅是对金钱,对生活中的任何事情的处理都是这样。富人和穷人之间的主要差别在于他们处理恐惧心理的方式不同 这也是为什么银行建议你在年轻时把储蓄当作一种习惯的原因。如果你在年轻的时候就开始积累了,你就更容易致富。当然,我并不认为储蓄是一种好的财富积累方法,我不愿意在这里详细讨论这个问题,但应该看到在那些从20岁开始储蓄与从30岁开始储蓄的人之间,的确存在着巨大的差异。 这是他们驾驭生活的方式,他们活得很大度,不像这儿的大部分人碰到金钱问题时,生活态度象斜齿编一样。斜齿编在有人用光照到它们时会非常害怕,而这种人在杂货店职员少找给他们两毛五分钱时,便会抱怨不停。 富爸爸经常告诉我和迈克,在财务上不能获得成功的最大原因是大部分人的做法过于安全……“人们因为太害怕失败,所以才会失败。”这是他常说的话。对此前全美橄榄球联赛的杰出球员弗朗。塔肯顿还有另一种说法：“胜利意味着不害怕失败。”在我的生活中,我注意到失败常常伴随着成功。在我最终学会骑自行车之前,我曾经跌倒过许多次,我从来没有遇到过不曾打丢球的高尔夫球手,也从未见过不曾伤心过的恋人,更未曾见过从不损失金钱的富人。因此,对大多数人来说,他们在财务上不能获胜的原因是因为对他们而言损失金钱所造成的痛苦远远大于致富所带来的乐趣。得克萨斯人的另一句谚语讲道：“人人都想上天堂,却没有人想死。”可是不死怎么能进入天堂呢,这就如同大部分人梦想发财,但却害怕在投资过程中损失金钱,所以他们永远进不了“天堂”。 富爸爸知道失败只会使他更加强大,更加精明。他并不愿意损失,但他清楚自己是什么样的人,知道该怎样去面对损失。他会接受损失并将它变成赢利,而这也是他最终成为赢家而别人成为失败者的最根本的原因；同时这也是当别人退出时,他依然有勇气去冲过终点线的原因。“这就是我为什么非常喜欢得克萨斯人的原因。他们接受失败的现实并把它转变成通向成功道路上的一个个插曲 失败会激励胜利者,也会去垮失败者,这是胜者之所以胜利的最大秘密,也是失败者所不知道的秘密。重复一下弗朗。塔肯顿的话：“胜利意味着不畏惧失败”。像弗朗。塔肯顿这样的人不害怕失败,因为他们了解自己,他们和所有人一样讨厌失败,但失败只会激发他们做得更好。要知道讨厌失败和害怕失败之间有着巨大的差异,大部分人因为太害怕失败而失败,他们甚至会因一幢两套房的复式公寓而完全破产,财务上他们做得过于安全 他们中的大部人将大量现金以大额存单、低收益债券,可以在共同基金内部买卖的共同基金,以及一点私人股票的形式进行投资。这是一个安全而合理的投资组合,却并不是一个赢利的投资组合,说到底这是人们为避免损失而做的一种投资组合。比起其他超过70％的人来,这可能还算是一个较好的投资组合。对于另外70％的人来说,即便是这种组合可能依然让他们感到担心,因此除了储蓄他们根本不做任何投资。毕竟,一个安全的投资计划要比什么投资计划都没有强得多 如果你有致富的愿望,你必须集中精力。把很多鸡蛋放在较少的篮子里（当然你还要确信篮子的结实程度）。不要把很少的鸡蛋放在许多篮子里 我们眼睁睁地看着时光流逝,心中的结使我们无所作为。在生活之中或多或少我们都会产生这样的状态。 他们总是批评而不是去分析,总是看到细节上的麻烦而看不到解决麻烦之后总体上的巨大收益。 当我们到家时,他给我列举了所有数据,以说明为什么在即将到来的几年里油价会趋于上涨。我以前从未读过这些数据,即使是我已拥有一家营运中的石油公司的主要股份后也是如此。根据这些信息,我立即开始寻找并找到了一家新的价值被低估了的石油公司,这家公司正在勘探新的地下石油储备,这使我的经纪人对这家新公司感到很兴奋。我后来买下了它的65％的股份,共1.5万股。在1997年2月,还是这位朋友,驾车和我经过同一座加油站。的确没错,每加仑汽油的价格上升了几乎15％,这位忧心忡忡的人非常担忧并且不停地抱怨。我笑了,因为在1997年1月,那家小型石油公司找到了石油,自从他第一次给我分析了那些数据以后,我买下了1.5万股股票,现在每股价格已上升到3美元以上。如果我的朋友所说的是正确的话,石油价格还会持续上扬,我的收益还会增加。 一定很响。“小鸡”掌管了他们的思维,正在叫喊“天要塌下来了,厕所坏了”。于是,他们避开了自己的“不想要”,却为此付出了巨大的代价——他们可能将永远得不到自己在生活中想要的东西。 忙碌的人常常是最懒惰的人 他们常常会忙着看电视、钓鱼、打高尔夫球或者购物。总之,把问题掩盖起来使他们逃避了一些重要的事情。这是最普遍的一种懒惰形式,一种通过忙碌来表现的懒惰。 还有许多这样的话,这些话影响了我和其他和我一样的孩子们。还有另外一种父母,他们采取的方式是另一种极端,他们常会这样说：“我牺牲自己的生活去买来这个给你,我给你买这个是因为在我小时候从未得到过这些东西。”我有一个邻居身无分文,但他的车库里却满是他孩子的玩具,以至于不能将车停进车库里。受溺爱的孩子们得到了他们要求的任何东西,“我不想让他们尝到贫困的滋味”是他每天都要说的话。他没有为孩子上大学或自己退休留下任何东西,可他的孩子却拥有市场上出售的每种玩具。他最近刚得到一张信用卡,就带上孩子去拉斯维加斯玩了。“我这么做全是为了孩子。”临走时他带着深深的自我牺牲的神情对我说。在我看来,以上这两种父母常用的教育方式都不能培养孩子正确的金钱观念和投资意识。富爸爸从不使用“我不能支付这个”这类的话。在我自己的家里,这可是我常听到的。但富爸爸要求他的孩子们说：“我怎样才能够支付这个？”。他的理由是：“我不能支付这个”这句话禁锢了你的思想,使你不再去作进一步的思考。“我怎样才能支付这个？”则开启了你的头脑,迫使你去思索并寻求答案。 我感到今天的问题是成千上万的人对自己的“贪婪”感到罪过,这是他们在少年时代就因袭到的陈旧思想。他们渴望拥有生活所提供的那些更美好的东西,但面对困难,大部分人却下意识地调整自己并借口说：“你不能拥有这个”,或者“你可支付不起这个”。 要记住迈克尔。道格拉斯在电影《华尔街》中所说的：“欲望是好事”。富爸爸以另一种方式说：“负罪感比欲望要糟,因为负罪感从身体里抢走了灵魂。”而对我来说,埃连娜。罗斯福说的好：“做你心里认为正确的事——因为你不管怎么做总会受到批评。如果你做的话,会受到指责；而你不做的话,还是会受到指责。”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-不要为钱而工作]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father08%2F</url>
      <content type="text"><![CDATA[世界上到处都是精明、才华横溢、受过良好教育以及很有天赋的人,我们每天都会碰到他们,他们就在我们的周围。几天前,我的汽车不大灵便。我把它开进维修厂,一位年轻的机械工几分钟之内就把它修好了。他仅凭倾听发动机的声音就能确定哪儿有毛病,这使我感到非常惊讶。然而遗憾的是,真正能够很好地利用这种非凡才华的人总是太少。 我常常吃惊为什么有些人才华过人却只挣到很低的收入,我听说只有不到5％的美国人年收人在10万美元以上。一位对药品贸易很精通的商务顾问曾经告诉我,有许多医生、牙医和按摩师在财务上困难重重。以前我总是以为他们一毕业,美元就会滚滚而来。这位商务顾问最后告诉了我一句话,“他们只有一项技能,所以他们挣不到大钱。” 以前我提到过,财商是会计、投资、市场营销和法律等各方面能力的综合。将上述四种专业技能结合起来,以钱生钱就会容易得多。为了赚钱,只有一项技能的人只能努力工作。 对于受过良好教育的爸爸来说,稳定的工作就是一切。而对于富爸爸来说,不断学习才是一切。 有一句古老的格言说,“工作的意义就是‘比破产强一点’”。然而,不幸的是,这句话确实适用于千百万人,因为学校没有把财商看作是一种智慧,大部分工人都“按他们的方式活着”,这些方式就是：干活挣钱,支付账单。 我劝告年轻人在寻找工作时要看看能从中学到什么,而不是只看能挣到多少。 一旦人们为支付生活的账单而整天疲于奔命,就和那些蹬着小铁笼子不停转圈的小老鼠一样了。老鼠的小毛腿蹬得飞快,小铁笼也转得飞快,可到了第二天早上醒来,他们发现自己依然困在老鼠笼里。 我知道,受到良好教育的爸爸每年都在期望加薪,但每年他都十分失望。于是他不得不回到学校去获得更高的学历资格,以便能得到另一次加薪的机会。但是很快,他又会再次失望。 当我提出这些建议时,我常常听到这样的反应,“这太麻烦了”,或者“我只想做我感兴趣的事情”。对于“太麻烦了”的说法,我问：“因此,你宁可辛苦工作一一生,并把挣来的50％的收入交给政府？”对于另一种说法说“我只想做我感兴趣的事情”,我说：“我对进健身房做运动并不感兴趣,但我还是去练习,因为我想身体更好,活得更长久。” 生活就像我去健身房,最痛苦的事情是作出去锻炼的决定,一旦你过了这～关,以后的事情就好办了。有很多次,我害怕去健身房,但是只要我去了,我心里就会感到非常愉快。做完了健身练习后我总是非常高兴地对自己说：做运动真好！ 因为我将终生只在该行业里学习到一种有价值的技能,如果我被这一行业所抛弃,我一生所学的技能对于其他行业便毫无用处。一位拥有10万小时驾驶大型运输机记录的高级飞行员,每年能挣15万美元,可一旦下岗,就很难找到一个收入相当的在学校教书的工作了。 世界上到处都是有才华的穷人。在很多情况下,他们之所以贫穷或财务困难,或者只能挣到低于他们本来能够挣到的收入。 富爸爸建议我和迈克“培养”自己。许多企业也做同样的工作,他们在商业学校寻找一位年轻聪明的学生,并开始“培养” 成功所必要的管理素质包括：1.对现金流的管理；2.对系统（包括你本人、时间及家庭）的管理；3.对人员的管理。 正如我提到过的那样,我受过良好教育的爸爸工作越来越努力,也越来越具有竞争力,但他也更深地陷入对自己专业特长的依赖之中。虽然他的工资收入增长了,可他的选择机会却消失了。等到失去了政府中的工作,他才发现自己从职业选择上来讲是多么地无能为力。这就好比职业运动员突然受伤或者年龄太大而无法再参加比赛一样,他们曾经拥有的高收入工作已经失去,而有限的技能又使他们无法另辟蹊径。我想,这就是为什么从那时起,我那受过良好教育的爸爸会变得如此支持工会的原因了,因为他意识到工会能给他带来很大的利益 富爸爸鼓励我和迈克对许多东西都去了解一点儿。他鼓励我们去和比我们更精明的人一起工作,并把他们组织成为一个团队。现在这种做法被称为专家组合。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-富人的投资]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father07%2F</url>
      <content type="text"><![CDATA[在现实世界里,人们往往是依靠勇气而不是聪明去领先于其他人的 为什么必须不厌其烦地提高自己的财商？为什么必须懂得财务知识？ 对此,我回答说,“就是为了获得更多的选择机会。” 那么究竟为什么要努力提高自己的财商呢？因为这样做了,你就会获得更大的成功；而不这样做,对你来说,这个时代就会成为一个令人恐慌的时代。你会发现一些人勇敢地走在了前面,而另一些人却陷入生活的恶性循环并难以自拔。 三百年前,土地是一种财富,因此,谁拥有土地,谁就拥有财富。后来,美国依靠工厂和工业产品上升为世界头号强国,工业家占有了财富。今天,信息便是财富。问题是,信息以光的速度在全世界迅速传播,新的财富形式不再像土地和工厂那样具有明确的范围和界限。变化会越来越快,越来越显著,百万富翁的数目会极大地增加,同样,也会有许多人被远远地抛在后面 今天,我发现许多人在苦苦工作、苦苦挣扎,其原因就是因为他们依然固执于陈旧的观念。他们希望事情都能原封不动,他们抵制任何变化。那些失去了工作或房子的人总在抱怨技术进步,或是埋怨经济状况不佳以及他们的老板,却没有意识到,问题的症结在于他们本身。陈旧的思想是他们最大的包袱,也可以说是最大的债务。为什么呢？原因很简单：他们没有意识到已有的某种思想或方法在昨天还是一种资产,但今天却已经变成了负债。 可大部分人却只知道一种方法：努力工作、储蓄或者借贷。 富爸爸经常教导我和迈克：金钱不是真实的资产。 我们惟一的、最重要的资产是我们的头脑。如果受到良好训练,转瞬间它就能创造大量财富,并使财富从此不再只是三百年前国王和王后们的专属。而一个未经训练的头脑通过教给自己的家庭不正确的生活方式,将会延续给后代极度贫困的生活 它会蒙蔽人们的双眼,使人们看不到事实发展的真相,他们会因此错过更多更好的使资金增加的机会。于是,机会就此与他们失之交臂了。 大部分人都信奉“辛苦工作,努力存钱”的教条。 在人的一生中,几乎每一天你都会遇到许许多多的机会,可是我们常常对此视而不见,但是机会确实存在,世界变化越大,技术进步越快,提供给你和你的家庭以至几代人财务安全的机会也就越多。 就我而言,我主要使用两种工具来实现资金的增值：房地产和小型公司股票。房地产是基础,通过每月买进卖出,我的财产不断地提供现金流人,偶尔也会有价值上的飘升。再有就是等待小型公司股票的快速增值。 下面是我用例证的五个原因：1.激励人们学习更多的知识；2.万丈高楼平地起,希望告诫人们打好基础；3.告诉人们每个人都能取得巨大的财富；4.告诉人们条条大路通罗马。5.告诉人们财务知识并不是深奥的科学。 好机会是用你的脑子而不是用你的眼睛看到的。大部分人没办法致富仅仅是因为他们没有在财务上受到训练,因而不能认识到机会其实就在他们面前。 富裕起来更是同样的道理,不幸的是,大部分人不富有的主要原因就在于他们太担心失去。胜利者是不怕失去的,但失败者都害怕失去。失败是成功之母,如果避开失败,也就避开了成功。 换句话说,大多数人眼睁睁地让缺少资金阻止了他们去做成一笔交易,如果你能越过这些障碍,你就能比那些没有掌握这些技能的人早一步成为百万富翁。 样把精明的人们组织起来。聪明的人往往会雇佣比自己更聪明的人或与他们一道工作。这样,当你需要建议的时候,你有可以信赖的顾问]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-税收的历史和公司的力量]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father06%2F</url>
      <content type="text"><![CDATA[税金通过工作和养老金的形式发放给了政府雇员,通过政府采购的形式付给了富人 政府成了一个巨大的钱库,但问题是还有预算管理,这不是一个自动循环重复的系统。换句话说,政府的政策是,如果你是一个政府官员,就应避免拥有过多的钱；如果你没有用完预算资金,在下次预算中你就有被削减掉这些钱的风险,你不会因为有节余而被认为有效率并得到奖励；为避免被削减预算资金,政府雇员会大量花钱和雇人,虽然这很可能是在浪费。而商人,则因为有节余而被认为有效率 成立公司就会流行起来了,因为企业所得税率低于个人收人所得税率。此外,公司的某些支出可以在税前获得抵减。 任何时候、任何地方只要制定法律,就会发生这种斗争。斗争还会持续下去,吃亏的人一定是无知者,即那些每天起来勤奋工作并不假思索地付税的人。但是如果他们明白了富人玩的游戏,他们也会来玩,这样他们就可以实现经济自立。每次当我听到父母劝说孩子去学校以便找个安定的工作时,我就会感到忧虑,因为一个有着稳定工作的雇员,若没有财务头脑,仍无法躲开财务上的陷阱。 穷爸爸对此从不反抗,富爸爸也不反抗——但他做得更聪明,他利用公司——富人的最大秘密,来达到他的目的。 跟他学习的这么多年,他总在提醒我知识就是力量,而且钱越多,需要的知识也就越多,没有知识,世界会牵着你走。富爸爸经常提醒我和迈克,最大的敌人不是老板或监工,而是税赋,税赋总想从你那里拿走更多,如果你允许的话。 他不知道,仅仅依赖雇主的工资,就永远只能是一头乖乖待挤的奶牛。 我可以通过努力工作跳出作为一名雇员的陷阱。我正在不断地把工资投资于资产项,而用资产项为我生产出来的钱购买我想要的东西。 通过运用富爸爸教我的那些课程,我能够在早期就走出“老鼠赛跑”咒语,而成功的原因就归功于我从那些课程中所学到的财务知识。若没有这些被我称为财商（财务智商,Financial I.Q.）的知识——我的经济自立之路将会困难得多。我现在在研讨班上把这些知识教给其他人,我希望别人能和我一起分享这些知识。无论何时我谈到这些知识,我都提醒人们：财商是由四个方面的专门知识所构成的：第一是会计,也就是我说的财务知识。如果你想建立一个自己的帝国的话,财务知识是非常重要的技能。你管理的钱越多,就越需要精确,否则这大厦就会倒下来。这是左脑要处理的,或者说是细节。财务知识能帮助你读懂财务报表,借助这种能力你还能够辨别业务的优势和弱势。 第二是投资,我称为钱生钱的科学。投资涉及到策略和方案,这是右脑要做的事,或者说是创造。 第三是了解市场,它是供给与需求的科学。这要求了解受感情驱动的市场的“技术面”。 第四是法律。它可以帮助你有效运营一个进入会计、投资和市场领域的企业并实现爆炸性地增长。了解税收优惠政策和公司法律的人能比雇员和小业主更快致富。这就像一个人在走,而另一个人却在飞,若从长远看这种差距就更大了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-关注自己的事业]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father05%2F</url>
      <content type="text"><![CDATA[麦当劳今天已是世界上最大的房地产商了,它拥有的房地产甚至超过了天主教会。今天,麦当劳已经拥有美国以及世界其他地方的一些最值钱的街角和十字路口的黄金地段 “关注自己的事业。”存在财务问题的人经常是一生为别人工作的人,许多人在他们停止工作时就变得一无所有。 请注意,你的工作和你的事业之间存在着巨大的区别。我经常问一些人：“你的事业是什么？”他们会说：“我是个银行职员。”接着我问他是否拥有银行,他们常回答：“不是的,我在那儿工作。” 为了财务安全,人们需要关注自己的事业。你的事业围绕着的是你的资产,而不是你的收入。正如以前说过的,第一号规则是要知道资产负债之间的区别,并且去买人资产。富人关心的焦点是他们的资产而其他人关心的则是他们的收入。 大多数穷人或中产阶级财务保守的基本原因在于：“我不能承担风险”,这意味着他们的财务知识匾乏,他们必须依附于工作,他们必须安全运作 关注你自己的事业并继续你每天的工作。你可以买些房地产,而不是负债或买一些一旦被你带回家使用就没有了价值的个人用品。一旦你把一辆新车开出停车处,你已损失了25％的车钱。汽车不是真正的资产,即使你的银行经理让你把它列在资产项下。当我用过一次价值400美元的新的高尔夫球杆时,它就只值150美元了。 对成年人而言,把支出维持在低水平,减少借款和勤劳地工作会帮你打下一个稳固的资产基础。对还未有自己房子的年轻人来说,父母应教他们明白资产和负债之间的区别,让他们在离家、结婚、买房、有孩子、在高风险的金融交易中下注或依附于工作和贷款买任何东西之前建立起坚实的资产基础,这是非常重要的。我见过许多年轻夫妇,由于他们并不能分清资产和负债,结婚后不久就陷入了以后大部分年月内都无法摆脱债务的生活方式中。 真正的资产可以分为下列几类：1.不需我到场就可以正常运作的业务。我拥有它们,但由别人经营和管理。如果我必须在那儿工作,那它就不是我的事业而是我的职业了；2.股票；3.债券；4.共同基金；5.产生收入的房地产；6.票据（借据）；7.专利权如音乐、手稿、专利；8.任何其他有价值、可产生收入或可能增值并且有很好的流通市场的东西。 对于小公司,我的投资策略是：1年内脱手。另一方面我的房地产投资策略则是从小买卖开始并一点点做大,条件允许的话尽量晚一些出手,这样做的好处是可以推迟缴纳所得税,从而使资产可能戏剧般地增加。我通常持有房地产在7年以上。 我上班,但我也关注自己的事业,我通过买卖小公司的股票和房地产,使我的资产变得非常活跃。富爸爸总是强调财务知识,他说,你对会计和现金管理懂得越多,你就能更好地进行投资分析并开始建立自己的公司。 作个努力工作的雇员,确保你的工作,但要不断构筑你的资产项 真正的奢侈品是对投资和积累真正资产的奖励。例如,当我和妻子通过买卖房屋获得了额外收人时,她去买了辆奔驰,这不是增加她的工作或冒着风险买下的。然而,当房地产投资升值并最终有足够的现金流人足以购买这辆车之前,她等了4年的时间。这奢侈品的确是个奖励,因为它证明了她知道如何增加自己的资产,那辆车对她的意义已不仅是一辆车,而意味着她能用自己的财务知识得到它。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-为什么要教授财务知识]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father04%2F</url>
      <content type="text"><![CDATA[知识才能解决问题并创造财富,不是凭财务知识挣来的钱很快就会消失 从长期来看,重要的不是你挣了多少钱,而是要看你能留下多少钱,以及留住了多久 规则1、你必须明白瓷产和负债的区别,并且尽可能地购买资产。如果你想致富,这一点你必须知道。这就是第一号规则,也是仅有的一条规则,这听起来似乎太简单了,但人们大多不知道这条规则有多么深奥,大多数人就是因为不清楚资产与负债之间的区别而苦苦挣扎在财务问题里。 “如果你想富有,你必须读懂并理解数字。”这话我从富爸爸那听到一千次了,同样频繁出现的话还有“富人得到资产而穷人和中产阶级得到负债”。 资产就是能把钱放进你口袋里的东西，负债是把钱从你口袋里取走的东西。 这就是你所要知道的全部了。如果你想变富,只须在一生中不断地买入资产就行了；如果你想变穷或成为中产阶级,只须不断地买人负债。正是因为不知道资产与负债两者间的区别,人们常常把负债当作资产买进,导致了世界上绝大部分人要在财务问题中挣扎。 看不憧财务方面的文字表述或读不懂数字的含义,是问题发生的根本原因。如果人们陷入财务困难,那就是说有些东西,或是数字或是文字他读不懂,或是有些东西被他误解了。富人之所以富是因为他们比那些挣扎于财务问题的人在某个方面有更多知识,所以如果你想致富并保住你的财富,财务知识是十分重要,包括对文字和数字的理解 一个人可能受过高等教育而且事业成功,但也可能是财务上的文盲。这种人往往比需要的更为努力地工作,因为他们知道应该如何努力工作,但却不知道如何让钱为他们工作。 一对刚结婚、受过高等教育的新婚夫妇住在一套拥挤的租来的公寓里,很快,他们意识到他们在省钱,因为两个人的花销和一个人的差不多。问题是,公寓太挤了,于是他们决定省钱买一栋自己梦想中的房子,这样他们就能有孩子了。现在,他们有两份收入,并开始专心于事业,他们的收入开始增加,随着收入的增加……对大多数人而言,第一项支出是税。许多人以为是所得税,但对大多数美国人而言,最高的税是社会保障税。作为一名雇员,表面上社会保障税和医疗税共约7.5％,实际上却是15％,因为雇主必须为你付15％的社会保障金。关键是,雇主并不会拿自己的钱去为你支付的,实际上他所支付的,都是你所应得到的。此外,你还得为你工资已扣除的社会保障税再交所得税,而这种所得是你从来就未得到过的,因为它们通过预扣直接进入了社会保障体系之中。对这对年轻夫妇的最好描述：随着收入的增加,他们决定去买一套自己的房子。一旦有了房子,他们就得缴税——财产税,然后他们买了新车、新家俱等,去和新房子配套。最后,他们突然发觉已身陷抵押贷款和信用卡贷款的债务之中。 如果你发现你已在洞里,那就别再挖了 很多人认为在公众面前说话比死还可怕。按精神病学的说法,害怕在公众面前说话是因为害怕被排斥、害怕冒尖、害怕被批评、害怕出错、害怕被逐出。简言之,是害怕与别人不同,结果阻碍了人们去想新办法来解决问题。 聪明人总是雇比他更聪明的人 我们开始明白为什么富爸爸说学校是生产好雇员而不是好雇主的地方。 因此我提出这个论点。我并不想让所有人都同意我的观点,因为房子毕竟是人们感情的寄托。此外,对于钱的热衷会降低财务方面的理智,我的个人经历告诉我,钱能使决策变得情绪化。 最大的损失是机会损失。如果你所有的钱都被投在了房子上,你就不得不努力工作,因为你的现金正不断地从支出项流出,而不是流人资产项,这是典型的中产阶级现金流模式。正确的做法应该是怎样的呢？如果一对年轻夫妇早点在他们的资产项中多投些钱,以后几年他们就会过得轻松些,尤其是他们准备把孩子送人大学的话。因为资产项中的投资会使他们的资产不断增加,自动弥补支出。而先投资买下一所大房子的做法只不过是取得抵押贷款以支付不断攀升的开支,其结果不过是拆了东墙补西墙。 总之,决定拥有很昂贵的房子,而不是早早地开始证券投资,将对一个人的财务生活在以下三个方面形成冲击：1.失去了用其他资产增值的时机。 2.本可以用来投资的资本将用于支付房子的各种高额、长期开支。 3.失去受教育机会。人们经常把他们的房子、储蓄和退休金计划列入他们的资产项目。 因为他们无钱投资,所以也就不去投资,这就使他们无法获得投资经验,并永远不会成为投资界认可的“成熟投资者”。而最好的投资机会往往都是先给那些“成熟投资者”,再由他们转手给那些谨小慎微的人的,当然,在转手时他们已经拿走了绝大部分的利益。 中产阶级发现自己总是在财务问题上挣扎,原因何在呢？中产阶级的主要收入是工资,而当工资增加的时候,税收也就增加了,更重要的是他们的支出倾向也随着收入的增加而同等增加。 他们把房子作为主要资产反复进行投资,而不是投资于那些能带来收人的真正的资产上。 这种把房子当资产的想法和那种认为钱越多就能买更大的房子或消费得更多的理财哲学就是形成今天债台高筑的社会的基础。过多的支出把家庭拖入到债务和财务不确定性的旋涡之中,这种情形甚至发生在人们工作成绩优秀和收入固定增长的时候,而这种高风险的生活正是由于缺乏财务知识教育所造成的。 但真正的原因仍在于早年缺乏必要的财务知识教育,这也是普通中产阶级被迫回避风险的原因。他们必须安全操作,因为他们的经济地位虚弱：他们的资产负债表从未平衡过,承担着大量债务而且没有能够产生收入的真实资产。他们的收入来源只是工资,生活完全依赖于他们的雇主。 作为一个自己有房子的雇员,你努力工作的结果如下：1.你为别人工作。如大多数人为工资而工作一样,你的努力使雇主或股东致富,你的工作和成功将使雇主成功并且可以提早退休。 2.你为政府工作。政府在你还未看见工资时就已拿走了一部分,努力工作只是使政府的税收增加。大多数人都在为政府工作。 3.你为银行工作。缴税后,你的下一笔最大支出该是偿还抵押贷款和信用卡贷款了。 一旦你决定把精力集中于创建自己的事业,你该怎样确立目标呢？对大多数人而言,他们的目标是保住他们的职业并依赖工资取得他们想要的资产。 如果我想增加支出,我首先必须增加资产项产生的现金流来维持我的财富水平。注意,这时我不再依赖工资,如果我辞职了,我每月还能用资产项产生的现金流维持支出,也就是说我仍能够生存。 我的下个目标是从资产中得到多余现金再进行投资。流入资产项的钱越多,资产就增加得越快；资产增加得越快,现金流入得就越多。只要我把支出控制在资产所能够产生的现金流之下,我就会变富,就会有越来越多除我自身劳动力收入之外的其他收入来源。 随着这种再投资过程的不断延续,我最终走上了致富之路。 请记住下面这些话：富人买入资产；穷人只有支出；中产阶级买他们以为是资产的负债。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-富人不为钱工作]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father03%2F</url>
      <content type="text"><![CDATA[如果你们放弃了你们才真的只能当穷人了。一件事情的成败并不重要,重要的是你们曾经尝试过。要知道大多数人只是谈论和梦想发财,而你们已经付出了行动。我再说一遍,我为你们骄傲,孩子们,别灰心,别放弃。 那是学校教你们的法子,”他笑着说,“但生活可不是这样的教法。你知道吗,生活才是最好的老师,大多数时候,生活并不对你说些什么,它只是推着你转,每一次推,它都像是在说‘喂,醒一醒,有些东西我想让你学学”’。 所以生活中,人们通常会做两件事。一些人在生活推着他转的同时,抓住生活赐予的每个机会；而另一些人则听任生活的摆布,不去与生活抗争。他们埋怨生活的不公平,因此就去讨厌老板,讨厌工作,讨厌家人,他们不知道生活也赐予了他们机会。” 如果你学会了这一课,你就会成为一个智慧、快乐而富有的人。如果你没有学会,你就只会终生抱怨工作、报怨低报酬和难以相处的老板,你会生活在一劳永逸地把你所有的钱的问题都解决的幻想中。 如果你是那种没有毅力的人,你将放弃生活对你的每一次推动。这样的话,你的一生会过得稳稳当当,不做错事、随时准备着当永远不会发生的事情发生时解救自己,然后,在无聊中老死。你会有许多像你一样的朋友,希望生活稳定、处世无误 你对生活屈服了,不敢承担风险。你的确想赢,但失去的恐惧超过了成功的兴奋,事实是从内心深处,你就始终认为你不可能 我有15O 多个雇员,但没有一个人请教过我这个问题。他们只是要求工作,并获得报酬。他们把一生中最好的年华用来为钱而工作,却不愿去弄明白工作到底是为了什么。 大多数人认为世界上除了自己外,其他人都应该改变。让我告诉你吧,改变自己比改变他人更容易。 大部分人会这么干,他们辞职,然后去找另一份工作,期望能得到更好的机会、更高的报酬,认为一份新的工作或更高的报酬会解决所有问题。 而在大多数情况下,这是不可能的。 最后,我抬起头,又重复了前面的问题：“那么怎样才能解决问题呢？” “用这个,”他说着轻轻地拍着我的脑袋,“你两个耳朵之间的这个家伙。” 直到那一刻富爸爸才显示了他区别于他的职员和我穷爸爸的关键的东西——这一点让他最终成为了夏威夷最富的人之一。而我受过良好教育的爸爸则一生都在与财务问题抗争 穷人和中产阶级为钱而工作,富人让钱为他们工作 说到钱,大多数人希望稳稳妥妥地挣到,他们很少有挣钱的激情,于是,只好有没钱的恐惧 别担心,你只须知道,正是出于恐惧心,人们大多害怕失去工作,害怕付不起账单,害怕遭到火灾,害怕没有足够的钱,害怕挨饿,大多数人期望得到一份稳定的工作。为了寻求稳定,他们会去学习某种专业,或做生意,拼命为钱而工作,大多数人成了钱的奴隶,然后把怒气对准他们的老板 如果选择为钱而工作,这就是许多人所过的生活。” “那么每次三小时工作结束,马丁太太给你三个硬币时,你又有什么感觉？” “我觉得不够。看上去就像什么也没给似的,真让人失望。” “这也正是大多数雇员拿到他们工资单时的感觉,此外还要扣除税和其他一些项目 大多数人上了四年大学后,教育也就到头了,可我知道我会一辈子去研究钱这东西,因为我研究得越深,知道的东西也就越多。大多数人从不研究这个题目,他们去上班,挣工资,然后去开销,总也不明白为何老被钱所困扰,于是以为多点钱就能解决问题,却几乎没有人意识到缺乏财务知识才是他们真正的问题所在 当他们某一天醒来面临严重的财务问题时,他们已不能停止工作。这就是只知道为钱工作而不知如何让钱为你工作的代价。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-富爸爸穷爸爸]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father02%2F</url>
      <content type="text"><![CDATA[我有两个爸爸,一个富,一个穷。一个受过良好的教育,聪明绝顶,拥有博士的光环,他曾经在不到两年的时间里修完了四年制的大学本科学业,随后又在斯坦福大学、芝加哥大学和西北大学进一步深造,并且在所有这些学校都拿到了全奖；与之相反的是,我的另一个爸爸连八年级都没能念完。应该说两位爸爸的事业都相当成功,而且一辈子都很勤奋,因此,两人都有着丰厚的收入。然而其中一个人终其一生都在个人财务问题的泥沼中挣扎,另一个人则成了夏威夷最富有的人之一。一个爸爸身后为教堂、慈善机构和家人留下数千万美元的巨额遗产,而另一个爸爸却只留下一些待付的账单。 在给我建议的时候,富爸爸还不算富有,而穷爸爸当时也并不贫穷,两人都刚刚开始他们的事业,都在为钱和家庭而奋斗。然而,他们对于钱的理解却是如此的迥然不同,这就好像一个爸爸会说：“贪财乃万恶之源”；而另一个爸爸却会说：“贫困才是万恶之本”。 我逐渐意识到富人之所以越来越富,穷人之所以越来越穷,中产阶级之所以总是在债务泥潭中挣扎,其主要原因之一在于他们对金钱的观念不是来自学校,而是来自家庭。我们中的绝大多数人是从父母那里了解钱是怎么回事的。一对贫困的父母在培养孩子的理财观念时,只会说：“在学校里要好好学习喔”。结果,他们的孩子可能会以优异的成绩毕业,但同时也秉承了贫穷父母的理财方式和思维观念——要知道,由于家长的灌输,这些观念在孩子很小的时候就已经开始形成了。 例如,一个爸爸爱说“我可付不起”这样的话,而另一个爸爸则禁止用这类话,他会说：“我怎样才能付得起呢？”这两句话,一个是陈述句,另一个是疑问句,一个让你放弃,而另一个则促使你去想办法。 虽然两个爸爸工作都很努力,但我注意到,当遇到钱的问题时,一个爸爸总会去想办法解决,而另一个爸爸则习惯于顺其自然。长期下来,一个爸爸的理财能力更强了,而另一个的理财能力则越来越弱。我想这种结果类似于一个经常去健身房锻炼的人与一个总是坐在沙发上看电视的人在体质上的变化。经常性的体育锻炼可以强身健体,同样地,经常性的头脑运动可以增加你获得财富的机会。懒惰必定会使你的体质变弱、财富减少。 另一个爸爸则信奉完全的经济自立,他反对这种“理所应当”的心理,并且认为正是这种心理造就了一批虚弱的、经济上依赖于他人的人。他提倡竞争。 一个爸爸努力存钱,而另一个不断地投资。一个爸爸教我怎样去写一份出色的简历以便找到一份好工作；另一个则教我写下雄心勃勃的事业规划和财务计划,进而创造创业的机会。 穷爸爸总是说：“我从不富有”,于是这句话就变成了事实。富有的爸爸则总是把自己说成是一个富人。他拒绝某事时会这样说：“我是一个富人,而富人从不这么做”,甚至当一次严重的挫折使他破产后,他仍然把自己当作是富人。他会这样鼓励自己：“穷人和破产者之间的区别是：破产是暂时的,而贫穷是永久的。我的穷爸爸会说：“我对钱不感兴趣”或“钱对我来说不重要”,富爸爸则说：“金钱就是力量”。 两个爸爸都很重视教育和学习,但两人对于什么才是重要的。应该学习些什么的看法却不一致。一个爸爸希望我努力学习,获得好成绩,找个挣钱多的好工作,他希望我能够成为一名教授。律师或会计师,或者去读MBA.另一个爸爸则鼓励我学习挣钱,去了解钱的运动规律并让这种运动规律为我所用。“我不为钱工作”,这是他说了一遍又一遍的话,“钱要为我工作。” 选择不同,命运也是不同的。 钱是一种力量,但更有力量的是有关理财的教育。钱来了又去,但如果你了解钱是如何运转的,你就有了驾驭它的力量,并开始积累财富。光想不干的原因是绝大部分人接受学校教育后却没有掌握钱真正的运转规律,所以他们终生都在为钱而工作。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《富爸爸穷爸爸》书摘-序言]]></title>
      <url>%2Fpost%2Fread-rich-father-pool-father01%2F</url>
      <content type="text"><![CDATA[得到好的教育和好的成绩不再能确保成功了。而孩子们似乎比我们先意识到了这一点。 我不能再只是简单地对孩子们重复：“去上学,争取拿好成绩,然后找到安全、稳定的工作,它会供养你一辈子”我知道我必须找到一条新路并指引给孩子们。去学校,好好念书,然后找个安全的工作。这是旧的建议而且是坏的建议 这对快乐夫妇,在35岁后陷入了‘老鼠赛跑’的陷饼。他们不停地为公司老板工作,通过缴税为政府工作,通过付住房贷款和信用卡贷款为银行工作,但等待他们的只是越来越多的债务和催款单,于是他们再加倍努力工作,再更多地获取债务,陷于财务紧张的怪圈不能自拔。 “你看到有许多并不富有的会计师以及银行经理、律师、股票经纪人、房地产经纪人了吗？他们懂很多,而且是最聪明的人,但他们大部分都不富有。正是因为我们自己不具备这些知识,我们才想要从这些专业人员那里寻求建议。但是如果有一天,你在高速公路上开车,陷于交通阻塞,挣扎着要去上班,而你向右边看时,发现你的会计师同样陷在交通阻塞中,向左边看又看见了你的银行经理,这时你会怎么想呢？他们自身难保,又怎能帮你？ 如果你想让你的孩子得到一个经济上安全的未来,他们就不能按旧的游戏规则办事,那和过于激进同样危险。 罗伯特不仅描述了雇员和雇主的区别,也说明了掌握自己的命运和把它让给别人掌握的区别。 假定学校体系的教育能使你的孩子准备好了应付真实的生活,这种假定是愚蠢的。我并不是说美国现有的教育体系是完全不好的,但至少它是远远不够的,在今天的世界,每个孩子都需要得到更多的教育,不同的教育,他们需要知道真实生活中的游戏规则,各种不同的规则。富人有他的那套规则,而富人的规则对于绝大多数穷人和中产阶级来说还是个秘密。其他占人口95％的人则有他们的规则,而这些人是从学校学到这些规则的。这就是今天为什么简单地对孩子说：“努力学习,找好工作’是危险的。 罗伯特的两个父亲,富爸爸和穷爸爸,向他解释而他也运用了一生的技能,两个父亲从观念到结果的对立为我们提供了重要的对照。他受过教育的父亲建议他为企业而工作,他的富有的父亲则建议他拥有自己的企业。两种道路都需要教育,但教育的科目却完全不同。他受过教育的父亲鼓励他成为聪明人,而他富有的父亲则鼓励他雇佣聪明人]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-流-用流收集数据和并行流]]></title>
      <url>%2Fpost%2Fjava8-collect%2F</url>
      <content type="text"><![CDATA[收集器简介对流调用collect方法将对流中的元素触发一个规约操作 Collectors实用类提供了很多静态工程方法，可以方便地创建常用收集器的实例，只要拿来用就可以了 最常用的是toList方法，它会把流收集到一个List中：1List&lt;Transcation&gt; transactions = transcationStream.collect(Collectors.toList()); 预定义收集器Collectors类提供的工厂方法（例如groupingBy）创建的收集器，他们主要提供了三大功能： 将流元素规约和汇总为一个值 元素分组 元素分区 规约和汇总counting方法计算菜单里有多少种菜1long howManyDishes = menu.stream().collect(Collectors.counting()); 这还可写的更为直接：1long howMangDishes = menu.stream().count() 假如你已经导入了Collectors类的所有静态工厂方法，你就可以写counting()而不是Collectors.counting() 假如你想要找到菜单中的热量最高的菜，你可以使用两个收集器，Collectors.maxBy和Collextors.minBy，来计算流中的最大和最小值。 123comparator&lt;Dish&gt; disCaloriesComparator = Comparator.comparing(Dish::getCalories);Optional&lt;Dish&gt; mostCalorieDish = menu.stream().collect(maxby(dishCaloriesComparator)); 汇总有时候我们想一次操作就获得，最大值最小值总和与平均值，通过summarizing操作你就可以数出菜单中的元素个数，总和，平均值、最大值和最小值1IntSummaryStatistics menuStatistics = menu.stream().collect(summarizingInt(Dish::getCalories)); IntSummaryStatistics类提供了getter方法来访问每个结果 连接字符串joining工厂方法返回的收集器会把对流中每个对象应用toString方法得到的所有字符串连接成一个字符串1String shortMenu = menu.stream().map(Dish::getName).collect(joining()); 如果Dish类有一个toString方法来返回菜肴的名称，那你无需用提取每一道菜的名称的函数可以得到相同结果：1String shortMenu = menu.stream().collect(joining()); 字符串可读性不是很好，幸好，joining工厂有一个重载版本可以接受元素之间的分界符，这样就可以得到一个逗号分隔的菜肴名称列表1String shortMenu = menu.stream().collect(joining(", ")); 广义的规约汇总前面我们说的所有规约过程，其实都是Collects.reducing工厂方法提供的更广义规约收集器的特殊情况 Collects.reducing工厂方法是所有这些特殊情况的一般化，可以说先去的案例仅仅是为了方便程序员而已，例如，reduceing方法创建的收集器来计算你菜单的总热量，如下1int totalCalories = menu.stream().collect(reducing()0,Dish::getCalories,(i,j)-&gt;i+j); 它需要三个参数： 第一个参数是规约的起始值 第二个参数就是使用的函数 但三个参数是一个BinaryOperator,将两个项目积累成同一类型的值 分组Collectors.groupingBy工厂方法返回的收集器可以完成这项任务1Map&lt;Dish.Type,List&lt;Dish&gt; dishesByType = menu.stream().collect(groupingBy(Dish:getType)) 如果要按照这两个标准分类怎么办呢？可以把一个内层groupingBy传递给外层groupingBy例如：groupingBy(a,b) a是第一级的条件，b是一个groupingBy 当然第二个参数也可以是一个条件，比如要数一数菜单中每类菜有多少个，可以传递counting收集器作为groupingBy收集器的第二个参数123Map&lt;Dish.Type,Long&gt; typesCount = menu.stream().collect( groupingBy(Dish::getType,counting())); 分区分区是分组的特殊情况，由一个谓词作为分类函数，Map的键类型时boolean，一共两组，ture和false例如：1Map&lt;Boolean,List&lt;Dish&gt;&gt; partitionedMenu = menu.stream().collect(partitionBy(Dish::isVegrtarian)) 总结下Collectors类的静态方法 并行流将顺序流转换为并行流只需要调用parallel() 另外，你只需要对并行流调用sequential方法就可以把它变成顺序流 配置并行流线程池并行流的线程从那里来？有多少个？怎么定义的？回答：并行流内部使用了默认的ForkJoinPool，它默认的线程数量就是你的处理器数量，这个值是由Runtime.getRuntime().avaliableProcessors()得到的 改变线程池大小：1System.setProperty("java.util.concurrent.ForkJoinPool.common.parallelism","12") 不建议修改 并行化并不是没有代价，并行化过程本身需要对流做递归划分，把每个子流的归纳分配到不同的线程，然后把这些操作的结果合并成一个值，但在多个内核之间移动数据的代价可可能比你想象的大，所以很重要的一点是要保证在内核中并行执行工作的时间比在内核之间传递数据的时间长 总而言之，很多情况下不可能或不方便并行化，然而，在使用stream之前，你必须确保用的对。 还有一些进阶内容 如分支/合并框架、工作窃取，Spliterator,本文不作介绍 总结 collect是一个终端操作，他接受的参数是将流中的元素积累到汇总的各种方式 预定义收集器包括将流元素规约和汇总到一个值，例如计算最小值，最大值或平均值 预定义收集器可以用groupingBy对流中元素进行分组，或用partitoningBy进行分区 收集器可以高效的复合起来，进行多级分组，分区和规约 你可以实现Collector接口中定义的方法来开发你自己的收集器 内部迭代让你可以并行处理一个流，而无需再代码中显式使用和协调不同的线程 虽然并行处理一个流很容易，却不能保证程序在所有情况下都运行的更快 从性能角度来说，使用正确的数据结构，如尽可能利用原始流而不是一般化的流 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-流-使用流]]></title>
      <url>%2Fpost%2Fjava8-stream-use%2F</url>
      <content type="text"><![CDATA[筛选和切片用谓词筛选filter方法会接收一个谓词（一个返回Boolean）作为参数，并返回一个包括所有符号谓词的元素的流 例子:筛选所有的素菜1List&lt;Dish&gt; vegetarianMenu = menu.stream().filter(Dish::isVegetarian).collect(toList()); 筛选各异的元素distinct方法他会返回一个元素各异的流，实现原理是根据元素的hashCode和equals方法 例子：筛选偶数，且不重复1234List&lt;Integer&gt; numbers = Arrays.asList(1,2,1,3,3,2,4);number.stream().filter(i-&gt;1%2==0) .distinct() .forEach(System.out.println); 截断流limit(n)方法该方法会返回一个不超过给定长度的流，如果流是有序的，则最多会返回前n个元素 例子：选出热量超过300卡路里的头三道菜1234List&lt;Dish&gt; dishes = menu.stream() .filter(d-d.getCalories()&gt;300) .limit(3) .collect(toList()); 跳过元素skip(n)方法，返回一个扔掉前n个元素的流，如果流中元素不足n个，则返回一个空流，请注意limit(n)和skip(n)是互补的 映射一个常见的数据处理套路就是从某些对象中选择信息，比如在sql里面，可以从表中选择一列 对流中每个元素应用函数map方法它会接收一个函数作为参数，这个函数会被应用到每个元素上，并将其映射成一个新的元素（注意是创建一个新的版本，而不是去修改） 例子：提取菜肴的名称1List&lt;String&gt; dishNames = menu.stream().map(Dish::getName).collect(toList()); 流的扁平化例子：对应一张单词表，如果返回一个列表，列出里面各不相同的字符比如单词列表[“Hello”,”Woeld”]你想要返回的列表[“H”,”e”,”l”,”o”,”W”,”r”,”d”] 你可能会觉得很容易，调用distinct方法就可以了1234words.stream() .map(word-&gt;word.split("")) .distinct() .collect(toList()); 这个方法的问题在于，传递给map方法的lambda为每个单词返回了一个String[],因此map返回的流实际上是Stream类型，我们真正想要的是Stream 幸好有flatMap来解决这个问题 尝试1：使用map和Arrays.stream()首先，你需要一个字符流，而不是数组流，有一个Arrays.stream()的方法，可以接受一个数组并产生一个流12345words.stream() .map(word-&gt;word.split("")) .map(Arrays::stream) .distinct() .collect(toList()); 这个方案仍然搞不定！因为现在得到的是一个流的列表,你先是把每个单词转换成一个字母数组，然后把每个数组变成一个独立的流。 尝试2：使用flatMap12345words.stream() .map(word-&gt;word.split("")) .flatMap(Arrays::stream) .distinct() .collect(toList()); flatMap方法的效果是，各个数组并不是分别映射成一个流，而是映射成流的内容，所有使用map(Arrays::stream)时生成的单个流都被合并起来，即扁平化为一个流， 一言以蔽之，flatMap方法让你把一个流中的每个值都换成另一个流，然后把所有的流连接起来成为一个流 查找和匹配检查谓词是否至少匹配一个元素anyMatch方法可以回答“流中是否有一个元素能匹配给定的谓词”例子：菜单里面是否有素食可选择123if(menu.stream().anyMatch(Dish::isVegetarian))&#123; ...&#125; 检查谓词是否匹配所有元素allMatch()用法同上与allMatch()相对的是noneMatch() anyMatch allMatch noneMatch 三个操作都用到了我们所谓的短路，就是大家熟悉的java中的&amp;&amp;和||运算符短路在流中的版本 查找元素findAny方法将返回当前流中的任意元素 findFirst找到第一个元素 规约reduce操作表达更复杂的查询，比如”计算菜单中的总卡路里”或“菜单中卡路里最高的菜是哪一个” 这需要将流中的元素反复结合起来，得到一个值，比如Integer,这样的查询被归类为规约操作，用函数式编程术语来说，这称为折叠(fold) 求和1int producr = numbers.stream().reduce(1,(a,b)-&gt;a*b); reduce操作是如何作用于一个流的：lambda反复结合每个元素，知道流被规约为一个值 可以使用最更简洁的代码：1int producr = numbers.stream().reduce(0,Integer::sum); reduce还有一个重载的变体，它不接受初始值，返回一个Optional对象：Optional sum = numbers.stream().reduce((a,b)-&gt;(a+b)); 最大值和最小值123Optional&lt;Integer&gt; max = numbers.stream().reduce(Integer::max)Optional&lt;Integer&gt; min = numbers.stream().reduce(Integer::min) 总结下目前说到的操作 数值流之前我们看到了可以使用reduce方法计算流中元素的总和，例如：123int calories = menu.stream() .map(Dish::getCalories) .reduce(0,Integer::sum); 这段代码的问题是，它有一个暗含的装箱成本，每个Integer都必须拆箱成一个原始类型，再进行求和，要是可以直接像下面这样调用sum方法，岂不是更好123int calories = menu.stream() .map(Dish::getCalories) .sum(); 这是不可能的，问题在于map方法会生成一个Straem，虽然流中的元素是Integer类型，但Streams接口没有定义sum方法，不要担心，Stream API还提供了原始类型流特化 原始类型流特化Java9引入了三个原始类型特化接口来解决这个问题：IntStream，DoubleStream和LongStream，分别将流中的元素特化为int，long，double,从而避免了暗含的装箱成本，每个接口都带了进行常用数值规约的新方法，比如对数值流求和的sum，找到最大元素的max，此外有必要时再把他们转换回对象流的方法 1.映射到数值流例子：123int calories = menu.stream() .mapToInt(Dish::getCalories) .sum(); 2.转换回对象流把原始流转换成一般流，可以使用boxed方法12IntStream intStream = menu.stream().mapToInt(Dish::getCalories);Stream&lt;Integer&gt; stream = intStream.boxed(); 3.默认值optionalInt求和有默认值0，但是如果计算intStream中最大的元素，就得换个法子了，因为0是错误的结果，我们知道Optional类，这是一个可以表示值存在或不存在的容器，Optional可以用Integer、String等参考类型来参数化，对于三种原始流特化，也分别有一个optional原始类的特化版本：OptionalInt,OptionalDouble,OptionalLong例如：1OptionalInt maxCalories = menu.stream().mapToInt(Dish::getCalories).max(); 数值范围java8引入了两个可以用于IntStream和LongStream的静态方法，帮助生成这种范围：range和rangeClosed。第一个参数接受起始值，第二个参数接受结束值。例子：1InStream evenNumbers = IntStream.rangClosed(1,100).filter(n-&gt;n%2==0) 构建流本节介绍如何从值序、数组、文件来创建流，甚至由生成函数来创建无限流 由值创建流可以使用静态方法Stream.of，它可以接受任意数量的参数例如：以下代码创建一个字符串流，然后你可以将字符串转换为大写，再一个个打印出来12Stream&lt;String&gt; stream = Stream.of("Java 8","Lambda","In","Action");stream.map(String::toUpperCase).forEach(System.out::println); 由数组创建流可以使用静态方法Arrays.stream从数组创建一个流，例子：12int[] numbers = &#123;2,3,5,7,11,13&#125;;int sum = Arrays.stream(number).sum(); 由文件生成流Files.lines方法，它会返回一个由指定文件中的各行构成的字符串流 创建无限流Stream API提供两个静态方法来从函数生成流：Stream.iterate和Stream.generate这两个操作可以创建所谓的无限流：他们产生的流会用给定的函数按需创建值，因此可以无穷无尽地计算下去，一般来说，应该来说应该使用limit(n)来对这种流加以限制，以避免打印无穷多个值 例子123Stream.iterate(0,n-&gt;n+2) .limit(10) .forEach(System.out.println) generate不是依次对每个生成的值应用函数的，它接受一个Supplier类型的lambda提供新的值 例子123Stream.generate(Math::random) .limit(5) .forEach(System.out::println) 小结 流可以简洁地表达复杂的数据处理查询，流可以透明的并行化 你可以使用filter、distinct、skip和limit对流做筛选和切片 你可以使用map和flatMap提取或装换流中的元素 你可以使用findFirst和findAny方法查找流中的元素，你可以使用allMatch、noneMatch和anyMatch方法让流匹配给定的谓词 这些方法都利用了短路：找到结果就立即停止计算，没有必要处理整个流 你可以利用reduce方法将流中的所有元素迭代合并成一个结果，例如求和或查找最大元素 filter和map等操作是无状态的，他们并不储存任何状态，reduce等操作要储存状态才能计算出一个值，sorted和distinct等操作也要储存状态，因为他们需要把流中的所有元素缓存起来才能返回一个新的流，这种操作称为有状态操作 流不仅可以从集合创建，也可以从、数组、文件以及iterate与generate等特定方法创建 无限流是没有固定大小的流 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-流-简介]]></title>
      <url>%2Fpost%2Fjava8-stream-introduction%2F</url>
      <content type="text"><![CDATA[几乎每个java应用程序都会制造和处理集合，要是没有集合，还能做什么呢？尽管集合对于几乎任何一个java应用都是不可或缺的，但是集合操作却远远算不上完美 很多业务逻辑都涉及类似于数据库的操作，比如对几道菜按照类别进行分组，查找出最贵的菜，大部分数据库都允许你声明式地指定这些操作，你不需要实现如何筛选，你只需要表达你想要的，都替你办好了，怎么到了集合就不能这样了呢？ 如果要处理大量元素该怎么办？为了提高性能，你需要并行处理 ，并利用多核架构，但是写并行代码比迭代器还复制，调试困难 为了解决这些问题，java语言的设计者做做些什么.你可能已经猜到了，答案就是流 流是什么流是Java API的新成员，它允许你以声明性的方式处理数据集合（通过查询语句来表达，而不是临时写一个实现），你可以把他们看成遍历数据集的高级迭代器，此外，流还可以透明的并行处理，你无需写任何多线程的代码了 比较java7与java8写一段返回低热量菜肴名称，并按照卡路里排序的代码 java7： java8:为了利用多核架构并行执行这段代码，你只需要把stream()缓存parallelStream() java8优势 代码时声明式的，说明你想要完成什么，而不是说明如何实现 你可以把几个基础操作连接起来，来表达复杂的数据操作流水线 用不着为了任务并行而去操心线程和锁了，streamAPI都帮你做好了 简而言之，java8 Stream 声明性–更简洁，更易读 可复用–更灵活 可并行–性能更好 流简介要讨论流我们先来谈谈集合，这是最容易上手的方式了，java8中的集合支持一歌新的stream方法，会返回一个流，当然还有很多其他的方法可以得到流，比如利用数据范围或从I/O资源生成流元素 流定义简单的定义就是，从支持数据处理操作的源生成的元素序列 元素序列–集合讲的是数据，流讲的是计算 源–流会使用一个提供数据的源，有序集合和列表生成的流，元素顺序会被执 流水线 很多流操作本身返回一个流，这样多个操作就可以链接起来，形成一个大的流水线 内部迭代–流的迭代操作是在背后进行的 流与集合对比粗略的说，集合与流之间的差异在于什么时候进行计算 集合是一个内存中的数据结构，它包含数据解雇中目前所有的值 流则是在概念上固定的数据结构，元素是按需计算的，那些值，在看不见的地方，只会按需生成，这是一种生产者-消费者关系，从另一个角度来说，流就是像一个延迟创建的集合，只有在消费者要求的时候才会计算值 用DVD对比在线流媒体的例子展示流与集合之间的差异 只能遍历一次流只能遍历一次，遍历完了之后，我们说这个流已经被消费掉了你可以从原始数据哪里再获得一个新的流来重新遍历一遍，就想迭代器一样 外部迭代与内部迭代 流操作流操作可以分为两大类，我们看下前面的例子： filter、map、limit可以连成一条流水线 collect触发流水线执行并关闭它 可以连接起来的流操作称为中间操作，关闭流的操作称为终端操作 中间操作触发流水线上触发一个终端操作，否则中间操作不会执行任何处理，他们很懒，这是因为中间操作一般都可以合并起来，在终端操作时一次性全部处理，只使用一次遍历 终端操作终端操作会从流的流水线生成结果，其结果是任何不是流的值，比如List，Integer,甚至是void 使用流流的使用一般包括三件事： 一个数据源来执行一个查询 一个中间操作链，形成一条流的流水线 一个终端操作，执行流水线，并生成结果 操作列表(不完全，以后还会介绍更多)： 小结 流是“从支持数据处理操作的源生成的一系列元素” 流利用内部迭代 流操作有两类：中间操作和终端操作 中间操作返回一个流，并可以链接在一起，可以用他们来设置一条流水线，但并不会生成任何结果 终端操作会返回一个非流的值，并处理流水线的返回结构 流中的元素是按需计算的 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-Lambda表达式]]></title>
      <url>%2Fpost%2Fjava8-lambda%2F</url>
      <content type="text"><![CDATA[Lambda表达式，它可以很简洁地表示一个行为或传递代码，现在你可以把Lambda表达式看作匿名功能，它基本上就是没有声明名称的方法，但和匿名类一样，它也可以作为参数传递给一个方法 Lambda管中窥豹可以把lambda表达式理解为简洁地表示可传递的匿名函数的一种方式：它没有名称，但它有参数列表，函数主体，返回类型，可能还有一个可以抛出的异常列表 lambda表达式有三部分： 参数列表 箭头 lambda主体 有效的lambda表达式举例：1(String s)-&gt;s.length() 这个lambda表达式具有一个string类型的参数，返回一个int，lambda没有return语句，因为已经隐含了return 1(Apple a)-&gt;a.getWeigh()&gt;150 这个lambda表达式有一个Apple类型的参数并返回一个boolean 1234(int x,int y)-&gt;&#123; System.out.println("Result:"); System.out.println(x+y); &#125; 这个lambda表达式具有两个int类型的参数，没有返回值，注意lambda表达式可以包含多行语句 1()-&gt;42 这个lambda表达式没有参数，返回一个int lambda表达式的基本语法是：(parameters)-&gt; expression或：(parameters)-&gt; {expression} 下面哪个不是有效的lambda表达式？1()-&gt;&#123;&#125; 有效，lambda没有参数，返回void 1()-&gt;"Raoul" 有效，lambda没有参数，返回string 1()-&gt;&#123;return "Mario";&#125; 有效，lambda没有参数，返回string，可以显示的写返回语句 1(Integer i)-&gt;return "Alan"+i; 无效，需要使用花括号，如下：(Integer i)-&gt;{return “Alan”+i;} 1(String s)-&gt;("IronMan";) “IronMan”是一个表达式，不是一个语句，要使用此lambda有效，你可以去除花括号或者，可以显示的返回如下：(String s)-&gt;{return “IronMan”} 在哪里以及如何使用Lamdda可以在函数式接口上使用lambda 函数式接口函数式接口就是指定义一个抽象方法的接口比如：123public interface Predicate&lt;T&gt;&#123; boolean test (T t)&#125; 123public interface Compartor&lt;T&gt;&#123; int compare(T o1,T o2);&#125; 123public interface Runnable&#123; void run();&#125; 我们还知道，接口现在还可以拥有默认方法，哪怕有很多默认方法，只要接口只定义一个抽象方法，它就仍然是一个函数式接口 @FunctionalInterface这个标注用于表示该接口会设计成一个函数式接口，如果你用了这个注解，而它却不是一个函数式接口的话，编译器将返回一个提示原因的错误 使用函数式接口为了应用不同的lambda表达式，你需要一套能够描述常见函数描述符的函数式接口比如之间我们见过的Comparable Runnable Callable Predicatejava.util.function.Predicate 接口定义了一个名叫test的抽象方法，它接受泛型T对象，并返回一个boolean,在你需要一个涉及类型T的布尔表达式时，就可以使用这个接口 consumerjava.util.funtion.Consumer 定义了一个名叫accept的抽象方法，它接受泛型T的对象，没有返回，你如果需要访问类型T对象，并对其进行某些操作，可以使用这个接口 Functionjava.util.funtion.Funtion接口定义了一个叫作apply的方法，它接受一个泛型T的对象，并返回一个泛型R的对象，如果你需要定义一个lambda，将输入对象的信息映射到输出，就可以使用这个接口 java8中常用的函数式接口 lambda及函数式接口的例子 类型检查，类型推断以及限制类型检查lambda的类型时从使用拉门等到的上下文推断出来的，比如，接受它传递的方法的参数，或接受它的值的局部变量 lambda表达式类型检查过程 使用局部变量lambda表达式允许使用自由变量（不是参数，而是在外层作用域中定义的变量）例如：12int portNumber = 1337;Runnable r = &#123;&#125;-&gt;System.out.println(portNumber) 尽管如此，有一些限制，lambda可以没有限制地捕获实例变量和静态变量，但是局部变量必须显示声明为final或事实上是final。换句话说，lambda表达式只能捕获指派给他们的局部变量一次 为什么？ 实例变量和局部变量背后的实现有一个关键不同，实例变量储存在堆中，而局部变量保证在栈上，如果lambda可以访问局部变量，而且lambda是在一个线程中使用的，则是哟lambda线程可能会分配该变量的线程将这个变量回收之后，去访问该变量。因此，java在访问自由局部变量时，实际上在是在访问它的副本，而不是访问原始变量， 这一限制不鼓励你使用改变局部变量的典型命令式编程模式，这种模式会阻碍并行处理 方法引用方法引用让你可以重复使用现有的方法定义，并像Lambda一样传递他们，他们似乎更易读，感觉也更自然例如先前:12inventory.sort((Apple a1,Apple a2) -&gt;a1.getWeigh().compareTo(a2.getWeight())); 之后(使用方法引用和java.util.Comparator.comparing)1iventory.sort(comparting(Apple::getWeight)); 方法应用可以被看作仅仅调用特定方法的lambda的一种快捷写法，当你需要使用方法引用时，目标引用放在分隔符::前。方法的名称放在后面，不需要括号，因为没有实际调用这个方法 复合lambda表达式你可以把多个简单的lambda复合成复杂的表达式，比如两个谓词进行一个or操作，还可以让一个函数的结果成为另一个函数的输入 比如 逆序1inventory.sort(comparing(Apple::getWeight).reversed()) 比较器链123inventory.sort(comparing(Apple::getWeight) .reversed() .thenComparing(Apple:getCountry())); 谓词复合三个方法：negate() and() or() 函数复合andThen() 意味着g(f(x))compose() 意味着f(g(x)) 小结 lambda表达式可以理解为一种匿名函数 函数式接口就是仅仅声明了一个抽象方法的接口 只有在接受函数式接口的地方才可以使用Lambda表达式 java8自带了一些常用的函数式接口，放在java.util.function包里 方法引用让你重复使用现有的方法并直接传递他们 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-行为参数化]]></title>
      <url>%2Fpost%2Fjava8-action-as-para%2F</url>
      <content type="text"><![CDATA[在软件工程中，一个众所周知的问题就是，不管你做什么，用户的需求肯定会变行为参数化就是可以帮助你处理频繁变更需求的一种软件开发模式。 它意味着拿出一个代码块，把它准备好却不执行它，这个代码块以后可以被你的程序其他部分调用 我们看一个例子，我们通过不断改进这个例子，展示行为参数化 版本1.1 筛选绿苹果如果要筛选红苹果怎么办呢? 版本1.2 把颜色作为参数如果要筛选重量大于150克的苹果怎么办？你可能会有下面的代码：这有点令人失望，这打破了DRY(don’t repeat yourself 不要重复自己)的软件工程原则 版本1.3 对你能想到的每个属性做筛选这样糟透了，代码可读性降低，而且如果要同时筛选多个属性怎么办？ 让我们后退一步来看看更高层次的抽象，一种可能的解决方案是对你的选择标准建模：需要根据apple的某个属性来返回一个boolean，我们把它称为谓词 让我们来定义一个接口来对选择标准建模 123public interface ApplePredicate&#123; boolean test(Apple apple);&#125; 现在你可以用ApplePredicate的多个实现代表不同的选择标准了 这里使用到了策略模式 接下来，你需要filterApple方法接受ApplePredicate对象，对Apple做条件测试，这就是行为参数化，让方法接受多种行为作为参数，并在内部使用，来完成不同的行为 版本1.4 根据抽象条件筛选 filterAppkes方法的行为取决于你通过ApplePredicate对象传递的代码 目前为止，当要把新的行为传递给filterApples方法的时候，你不得不声明好几个实现ApplePredicate接口的类，然后实例化好几个只会提到一次的ApplePredicate对象，下面是的程序总结了目前看到的一切 很啰嗦，浪费时间！ java有一个机制称为匿名类，可以让你同时声明和实例化一个类，换句话说就是随用随建，它可以帮助你改进代码 版本1.5 使用匿名类 匿名类也有缺点，往往很笨重，占用很多空间，用起来让人费解，还是不能令人满意。 版本1.6 使用lambda表达式看起来干净很多！ 小结一下： 版本1.7 将list抽奖化让逻辑不仅适用于apple 再举几个真实的例子用comprator来排序对苹果的按照重量排序1inventory.sort(Apple a1,Apple a2)-&gt; a1.getWeight().compareTo(a2.getWeight()); 用Runable执行代码块1Thread t = new Thread(()-&gt;System.out.println("Hello world")); GUI事件处理鼠标点击发送后显示一个窗口1button.setAction((ActionEvent event)-&gt;label.setText("sent!")); 小结 行为参数化，就是一个方法接受多个不同的行为作为参数，并在内部使用它们，完成不同行为的能力 行为参数化可以让代码更好的适应不断变化的要求，减轻未来工作量 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java8-概述]]></title>
      <url>%2Fpost%2Fjava8-general%2F</url>
      <content type="text"><![CDATA[1.概述为什么你应该关系java8？ 因为java8所做的改变，在许多方面比java历史上任何一次改变都深远 再也不哟写下面这种啰嗦的程序了(按苹果重量排序)12345Collections.sort(inventory,new Comparator&lt;Apple&gt;)()&#123; public int compare(Apple a1,Apple a2)&#123; return a1.getWeight().compareTo(a2.getWeight()) &#125; &#125;&#125;; java8中：1inventory.sort(Comparing(Apple::getWeight())) java8对cpu核的利用率提高. StreamAPI支持许多处理数据的并行操作，避免用synchronized编写代码 把代码传递给方法的简洁方式(方法引用，Lambda)，行为参数化 接口中的默认方法 2.java怎么还在变java在编程语言生态生态系统中的位置编程语言生态系统的气候正在变化，程序员越来越多的要处理所谓的大数据，并希望利用多核计算机或计算机集群有效的处理，这意味着需要使用并行处理 新的语言不断出现，并因为迅速适应了气候变化，越来越受欢迎，也促使java中开发出并行和编写更简洁通用代码的功能 流处理流处理，流是一系列数据项，一次只生成一项，程序可以从输入流中一个一个读取数据项，然后以同样的方式将数据项写入输出流，一个程序的输出流很可能是另一个程序的输入流 基于这一思想，java8在java.util.stream中添加了Stream API,Stream就是一系列T类型的项目，你现在可以把它看成一种比较花哨的迭代器，Stream API的很多方法可以链接起来形成一个复杂的流水线 Java8可以透明地把输入的不相关的部分拿到几个cpu内核上去分别执行你的stream操作流水线–这是几乎免费的并行，不用去费劲的搞Thread了 用行为参数化把代码传递给方法java8增加了把方法作为参数传递给另一个方法的能力，我们把这一概念称为为行为参数化 并行和共享的可变数据使用流处理，你需要放弃什么，你的行为必须能够同时对不同的输入安全地执行，一般情况下这就意味着，你写代码时不能访问共享的可变数据，这些函数有时被称为“纯函数”或“无副作用函数”或“无状态函数” java需要演变语言需要不断改进以跟进硬件的更新或满足程序员的期待 3.java中的函数编程语言中的函数一词通常指方法，尤其是静态方法，这是在数学函数，也就是没有副作用的函数之外的新含义，幸运的是，你将会看到，在java8谈到函数时，这两种用法几乎是一致的 java8中新增了函数–值的一种新形式 编程语言的整个目的就在于操作值，要是按照历史上编程语言的传统，这些值因此被称为一等值，编程语言中的其他结构比如方法等，在程序执行期间不能传递，而是二等公民，人名发现在运行时传递方法能将方法变成一等公民，这在编程中非常有用，因此java8设计者把这个功能加入到java中 方法不再是二等值了，与用对象引用传递对象类似，在java8里写在诸如1File::isHidden 的时候，就创建了一个方法引用，你同样可以传递它 lambda–匿名函数你可以写 (int x)-&gt;x+1表示，调用是给定参数x。就返回x+1值的函数 我们说使用这些概念的程序为函数式编程风格，这句话的意思是“编写把函数作为一等值来传递的程序” 4.流几乎每个java应用都会制造和处理集合，但集合用起来并不总是这么理想，比如你㤇从一个列表中筛选金额较高的交易，然后按照货币分组，你需要写一大堆套路化的代码来实现这个数据处理命令，如下 一眼很难看出来代码时做什么的，然后使用stream API 你现在可以这样解决这个问题 看上去有点神奇，和Collection API相比 Stream API处理数据的方式很不同 ，用集合的话，你到自己去做迭代的过程，你得用for-each循环去一个个的去迭代元素，然后处理元素，我们把这种数据迭代方法称为外部迭代，有了Stream API ，你根本不用操心循环的事情，数据处理完全是在库内部进行的，我们把这种思想叫做内部迭代 使用集合另一个头疼的地方是，数据量特别大，要怎么处理这个巨大的列表呢，理想情况下，你可能想让多核cpu共同分担处理工作，以缩短处理时间，理论上来说，钥匙你有8个核，那并行起来，处理数据的速度应该是单核的八倍 5.默认方法接口如今可以包含实现类没有提供实现的方法签名了 使用新的关键字default来表示这一点 一个类可以实现多个接口，如果好几个接口里都有多个默认实现，是否意味着java中有了某种形式的多重继承？ 是的，在某种程度上是这样的 6.来自函数式编程的其他好思想java从函数式编程中引入了两个核心思想：将方法和lambda作为一等值和在没有可变共享状态时，函数或方法可以有效、安全地并行执行 常用的函数式语言，如SML、OCaml、Haskell,还提供了进一步的结构来帮助程序员，其中之一就是通过更多的描述性数据类型来避免null 在java8里面有一个Optional类，如果你能一致地使用它的话，就可以帮助你避免出现NullPointer异常 第二个想法是模式匹配，你可以把模式匹配看做switch的扩展，可以同时将一个数据类型分解成元素 7.小结 语言面临“要么改变，要么衰亡”的压力。 java8新增了令人激动的新概念和功能，方便我们编写有效又简介的程序 函数式一等值 stream改变使得collection的许多方面得以推广，代码更易读，并允许并行流处理 你可以在接口中使用默认方法 其他来自函数式编程的有趣思想，包括处理null和使用模式匹配 （注：内容整理自《Java8实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybatis-动态SQL]]></title>
      <url>%2Fpost%2Fmybaits-sql%2F</url>
      <content type="text"><![CDATA[如果使用JDBC或者其他框架，很多时候你得根据需要去拼装SQL,这是一个麻烦的事情，mybatis提供对sql语句动态的组装能力。大部分判断可以在mybatis的映射XML文件里面配置 mybatis也可以在注解中配置SQL，但是由于注解配置功能受限，对于复杂SQL而言可读性差，较少使用 概述动态SQL包括以下几种元素 ifif元素是最常用的判断语句，它常常与test属性联合使用 使用场景举例：不填写的时候不要用它作为查询条件 choose when otherwise类似java中的 switch case default语句 使用场景举例： 当角色编号不为空，则只用角色编号作为查询条件 当角色编号为空，而角色名称不为空，则用角色名称作为条件进行模糊查询 当角色编号和角色名称都为空，则要求角色备注不为空 trim where set一般写sql的时候都要加一个条件 1=1 ，如果没有加，sql语句可能会报错 例如：1select role_no, role_name, note from t_role where role_name like concat(&apos;%&apos;,#&#123;roleName&#125;，&apos;%&apos;) 当然我们也可以去掉1=1，我们可以用where元素 当where元素里面的条件成立的时候，才会加入where这个关键字组装到sql里面 有时候我们需要一些特殊的sql语法，比如常见的and or，使用trim可以达到我们预期的效果 trim元素意味着我们需要去掉一些特殊的字符串，prefix代表是语句的前缀，而prefixOverrides代表的是你需要去掉的那种字符串 我们常常需要更新某一个对象，发送所有字段给持久对象，现实中我们往往需要更新一部分字段，set元素可以完成这个功能 同样我们可以把它转变为trim元素1&lt;trim prefix=&quot;SET&quot; suffixOverrides=&quot;,&quot;&gt;...&lt;/trim&gt; foreachforeach元素是一个循环语句，遍历集合 说明： colletion配置的sexList是传递进来的参数名称，它是一个数组或者List，Set等集合 item配置的是循环中的当前元素 index配置的是当前元素在集合的位置小标 open和close配置的是以什么符号将这些元素包装起来 separator是各个元素的间隔符 test元素用于条件判断语句123&lt;if test=&quot;type=&apos;Y&apos;&quot;&gt;where 1=1&lt;/if&gt; （注：内容整理自《深入浅出MyBatis技术原理与实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybatis映射器]]></title>
      <url>%2Fpost%2Fmybatis-mapper%2F</url>
      <content type="text"><![CDATA[映射器主要元素 select 查询（可以自定义参数,返回结果集） insert 插入（执行后返回一个整数，代表插入的条数） update 更新 （执行后返回一个整数，代表更新的条数） delete 删除 （执行后返回一个整数，代表删除的条数） parameterMap （定义参数映射关系，即将被删除的元素，不建议使用） sql 表的列名，一次定义多个sql使用 resultMap 从结果集中加载对象，提供映射规则 cache 给定命名空间的缓存配置 cache-ref 其他命名空间缓存配置 select配置 简单例子统计一个姓氏的用户数量，姓氏作为参数 xml： 123&lt;select id=&quot;countFirstName&quot; parameterType=&quot;string&quot; resultType=&quot;int&quot;&gt; select count(*) as total form t_user where name like concat(#&#123;firstname&#125;,&apos;%&apos;)&lt;/select&gt; 接口中方法 1public int countFirstName(String firstName); 自动映射参数autoMappingBehavior当它不设置为NONE的时候，Mybatis会提供自动映射功能，只要返回的sql列名和javaBean的属性一直，mybatis会帮助我们回填这些字段 自动策略可以在setting元素中的配置autoMappingBehavior属性值，包括三个值 NONE 取消自动映射 PARTIAL 只会自动映射，没有定义嵌套结果集映射的结果集 FULL 会自动映射任意复杂的结果集（无论是否嵌套） 默认值为PARTIAL 如果你的数据库是规范命名的，即每一个单词都用下划线分隔，POJO采用驼峰式命名方法，那那么你也可以设置mapUnderscoreToCamelCase为true，这样就可以试实现从数据库到POJO的自动映射了 传递多个参数使用Map传递接口中的参数定义为：Map paramsxml中使用 resultMap=’roleMap’sql中可以直接应用map的key值比如#{roleName} 弊端：map的业务关联性不强，可读性下降 使用注解传递可以使用mybatis的参数注解 @Param（org.apache.ibatis.annotations.Param）来实现 接口参数之前加上注解：@Param(“roleName”) String rolenamesql中可以直接引用参数值比如 #{roleName} 弊端：如果参数多的话，这种方式会比较复杂 使用JavaBean传递mybatis允许通过javabean，简单的setter和getter方法直射参数 xml中设置parameterType=”com.xxx.Role” 这样就可以直接使用对象的参数了，例如#{roleName} 总结map导致业务可读性丧失，不要使用@Param适合参数小于等于5个时javaBean方式适合参数大于5时 resultMap使用resultMap映射结果集 说明： id是唯一标识，type去定义它对应是哪个javabean（也可以使用别名） id属性标注哪个属性作为其主键 这样的语句不需要使用自动映射规则，直接使用resultMap属性指定roleResultMap即可，这样mybatis就会使用我们的自定义规则映射 insertmybaits会在执行插入之后返回一个整数，以表示你进行操作后插入的记录数 配置 主键回填和自定义现实中有许多我们需要处理的问题，例如主键自增字段，mysql里面的主键需要根据一些特殊的规则去生成，在插入后我们往往需要获得这个主键，以便于未来的操作，mybatis提供了实现的方法 首先我们可以说使用keyProperty属性指定哪个是主键字段，同时使用useGeneratedKeys属性告诉Mybatis这个主键 是否使用数据库内置策略生成 xml配置： 这样我们传入的role对象无需设置id的值，mybatis会用数据库的设置进行处理，这样做的好处是在mybatis插入的时候，他会回填javabean的id值 如果我们需要一些特殊的关系设置id值，加入我们取消role表id自增规则，如果表t_role没用记录，则我们需要设置id=1，否则我们就取最大id加2，来设置主键 应对方法： update与deleteupdate与delete元素比较简单我们看一下配置方法 参数注意:定义参数属性的时候，mybatis不允许换行 参数配置有时候我们需要处理一些特殊情况，我们可以指定类型，以确定使用哪个typehandler处理他们123456#&#123;age,javaType=int.jdbcType=NUMERIC&#125;#&#123;age,javaType=int.jdbcType=NUMERIC,typeHandler=MyTypeHandler&#125;设置保存的精度：#&#123;price,javaType=double,jdbcType=NUMERIC,numericScale=2&#125; 存储过程支持对于存储过程而言，存在三种参数，输入参数IN,输出参数OUT,输入输出参数（INOUT）,我们通过指定mode属性来确定其实哪一种参数，mybtis会将存储过程的返回结果这只到你制定的参数中，当你返回一个游标的时候，你还需要去设置resultMap以便mybatis将存储过程的参数映射到对应的类型 1#&#123;role,mode,jdbcType=CURSOR,javaType=ResultSet,resultMap=roleResultMap&#125; mybatis还支持结构体，但是当你注册参数的时候，你就需要去制定语句类型的名称 1#&#123;role,mode=OUT,jdbcType=STRUCT,jdbcTypeName=MY_TYPE,resultMap=roleResultMap&#125; 特殊字符串替换和处理（#和$）有时候我们需要传递的是sql语句本身。而不是sql所需要的参数，例如一些动态表格，需要根据不同条件产生不同动态列 例如传递变量colums=”col1,clo2,col3…”给SQL,让其组装成为SQL语句，可以写成如下语句1select $&#123;columns&#125; from t_tablename sql元素可以定于可以复用的sql语句片段例如：123&lt;sql id=&quot;role_columns&quot; id,role_name,not&lt;/sql&gt; 可以在sql是使用它 resultMap元素构成 constructor元素加入一个pojo不存在没有参数的构造方法它的构造方法声明为1public RoleBean(Integer id,String roleName) 那么我们就需要配置这个结果集 使用map存储结果集可以设置 resultType=”map” 可读性下降，不推荐使用 使用POJO储存结果集最常用的的方式 级联对应sql中的join操作 级联有三种： association 代表一对一关系 比如国民和省份证 collection 代表一对多关系，比如班级和学生 discrminator 是鉴别器，它可以根据实际选择采用哪个类所为实例 篇幅原因不详细说，建议少用级联，用处不大，而且会造成复杂度增加 缓存系统缓存mybaits对缓存提供支持，在默认情况下，它只开启一级缓存（一级缓存相对于同一个sqlSession而言） 所以在参数和sql完全一样的情况下，我们使用同一个sqlSession对象调用一个Mapper方法，往往只执行一次SQL 使用sqlSession第一次查询后，mybatis会将其放入缓存中，以后再查询的时候，如果没有声明需要刷新，而且缓存没有超时的情况下，sqlSession都只会取出当前缓存的数据，不会再次发送sql到数据库 一级缓存在sqlSession之间是互相隔离的，为了克服这个问题，我们需要配置二级缓存，在sqlSessionFactory层面提供给各个sqlSession对象共享 耳机缓存默认不开启，需要进行配置，mybatis要求返回的POJO必须是可序列化的，也就是要求实现Serializable接口 配置方法：在xml文件中配置下面内容即可 1&lt;cache/&gt; 很多设置是默认设置，我们只需要这样配置这个配置意味着： 我们也可以添加一些精细的配置：1&lt;cache eviction=&apos;LRU&apos; flushInterval=&quot;100000&quot; size=&quot;1024&quot; readOnly=true&quot;/&gt; evicition：缓存回收策略 flushInterval 刷新间隔时间，单位是毫秒，如果你不配置它，那么当sql被执行的时候才会去刷新缓存 size 引用数目，一个正整数，代码缓存最多可以储存多少个对象，不宜设置过大，会导致内存溢出 readOnly 只读，意味着缓存数据只能读取不能修改，默认是false，不允许修改 自定义缓存使用第三方缓存服务器，比如redis，我们需要mybaits为我们提供接口org.apache.ibatis.cache.Cache 缓存接口简介 上面的接口需要我们去实现，假设我们已经有一个实现类com.learn.chapter4.MyCache那么我们需要配置如下代码 1&lt;cache type=&quot;com.learn.chapter4.MyCache&quot;/&gt; 我们也可以配置sql层面上的缓存规则，来决定他们是否需要使用或者刷新缓存 我们根据两个属性：useCache和flushCache useCache表示是否需要使用缓存flushCache表示插入后是否需要刷新缓存 （注：内容整理自《深入浅出MyBatis技术原理与实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybaits-配置]]></title>
      <url>%2Fpost%2Fmybatis-config%2F</url>
      <content type="text"><![CDATA[Mybatis配置XML文件的层次结构 这些层次是不能够颠倒顺序的，如果颠倒顺序，mybatis解析xml文件时会出现异常 properties元素properties元素提供三种配置方式 property子元素 property配置文件 程序参数传递 property子元素如图： 这样我们就可以在上下文中使用以及配置好的属性了 如图：123456&lt;dataSource type=&quot;POOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;$&#123;dirver&#125;&quot; &lt;property name=&quot;url&quot; value=&quot;$&#123;url&#125;&quot; &lt;property name=&quot;username&quot; value=&quot;$&#123;username&#125;&quot; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;&lt;/dataSource&gt; properties使用配置文件，方便我们在多个配置文件中重复使用它们，也方便日后维护和随时修改 如图： 我们properties文件放在源包下，只要这样引入这个配置文件即可1&lt;properties resource=&quot;jdbc.properties&quot;/&gt; 程序参数传递适用场景举例：生产数据库用户名和密码对于开发者保密，运维人员要求文件中的数据库用户和密码进行加密，需要在代码中进行解码传入配置，这个时候需要通过编码的范式满足我们遇到的场景 如图：InputStream cfgStream = null;decode为解密方法 优先级mybtis支持这三种方式同时出现，可以重复配置同一属性 mybtis按照下面的顺序加载： 在properties元素体内的指定属性首先被读取 根据properties预算内宿的resource属性读取类路径下属性文件，或者根据url属性指定的路径读取属性文件，并覆盖已读取的同名属性 读取作为方法参数传递的属性，并覆盖已读取的同名属性 总结：优先级 代码&gt;配置文件&gt;properties属性 setting setting在mybatis中是最复杂的配置，也是最重要的配置 即使不配置setting，mybatis也可以正常工作 了解setting配置内容，十分必要 如图： typeAliases 别名是一个指代的名称，因为类的全名过长，我们希望用简短的方式去指代它 这个名称可以在mybatis上下文中使用 别名不区分大小写 一个别名实例是在解析配置文件时生成的，然后长期保持在configuration对象中，供随时使用 类型： 系统定义的别名 自定义别名 系统定义的别名 注：支持数组类型的只要加“[]”即可，比如Date数组可以用date[]代替 自定义别名方式： 配置文件 代码 配置文件： 这样mybtis的上下文中使用role来替代其全路径 注解+自动扫描 使用注解@Alias，Mybatis会自动扫描包，将扫描到的类装载到上下文中以便将来使用 注意：配置了包扫描路径，没有注解的类，mybatis也会装载，只是说它将把别名设置为类名第一个字母小写 typeHandler Mybatis在预处理语句(PrepareStatement)中设置一个参数时，或者从结果集(ResultSet)中取出一个值时，都会用注册了的typeHandler进行处理 由于数据库厂商不同，参数不同，typeHandler允许根据项目的需要自定义设置java传递到数据库的参数中，或者从数据库读出的数据，进行特殊处理 在使用枚举的时候我们常常使用typeHandler进行转换 类型： 系统定义 用户自定义 typeHandler常用的配置为java类型（javaType）、JDBC类型（jdbcType）,typeHandler的作用就是将javaType和jdbcType互相转化 由于篇幅考虑，typehandler不详细说明 ObjectFactory当mybatis在构建一个结果返回的时候，都会使用ObjectFactory去构建POJP，在mybatis中可以定制自己的ObjectFactory，一般来说我们使用默认的ObjectFactory即可，默认的ObjectFactory是由org.apache.ibatis.reflection.DefaultObjectFactory来提供服务的 如果要定制特定的工厂则需要进行配置 environment配置环境可以注册多个数据源 每个数据源分为两大部分： 数据库源的配置 数据库事务的配置 如图： 说明： environments中的属性default，标明在缺省的情况下我们将启用哪个数据源配置 属性id这只这个数据源的标志，以便在mybatis上下文中使用它 transactionManager配置的是数据库事务，type属性有三种配置方式： JDBC 采用JDBC方式管理事务，在独立编码中常常使用 MANAGED,采用容器方式管理事务，在JNDI数据源中常用 自定义，由使用者自定义，适用于特殊应用 property元素则是可以配置数据源的各类属性，我们配置了autoCommit=false则是要求数据源不自动提交 dataSource标签，是配置数据源连接的信息，type属性是提供我们队数据库连接方式的配置 type： UNPOOLED,非连接池数据库 POOLED,连接池数据库 JNDI,JNDI数据源 自定义数据源 引入映射器方法映射器是Mybatis最复杂、最核心的组件 引入引射器的方法 用文件路径引入 用包名引入 用类注册引入 用xml引入 如图： （注：内容整理自《深入浅出MyBatis技术原理与实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybaits-基本构成和生命周期]]></title>
      <url>%2Fpost%2Fmybatis-lifecycle%2F</url>
      <content type="text"><![CDATA[基本构成 SqlSessionFactoryBuilder(构造器):它会根据配置信息或者代码来生成SqlSessionFactory(工厂接口) SqlSessionFactory:依靠工厂来生成SqlSession(会话) SqlSession:是一个既可以发送SQL去执行并返回结果，也可以获取Mapper的接口 SQL Mapper:它是Mybatis新设计的组件，它是由一个java接口和xml文件（或注解）构成的，需要给出对应的sql和映射规则，它负责发送SQL去执行，并返回结果 一张图表达他们的关联： SqlSessionFactory SqlSessionFactory的实例可以通过SqlSessionFactoryBuilder获得 SqlSessionFactory是一个工厂接口而不是实现类，它的任务是创建SqlSession,Sqlsession类似于一个JDBC的Connection对象 Mybatis提供两种模式去创建SqlSessionFactory** 通过XML配置的方式，推荐这种方式，这样可以避免硬编码，还有方便日后配置人员修改 通过代码的方式 Configuration类org.apache.ibatis.session.Configuration类，他是Mybatis中将一个Configuration类对象的形式存在，而这个个对象将存在于整个Mybatis应用的生命期中，以便重复读取和运用。在内存中的数据是读取速度最快的，我们可以解析一次配置的XML文件保存到Configuration类对象中，方便我们从这个对象中服务配置信息，性能高，单例占用空间小，可以反复使用 Mybaits提供了两个SqlSessionFactory实现类，DefaultSqlSessionFactory和SqlSessionManager，不过这个SqlSessionManager目前还没有使用，目前使用的是DefaultSqlSessionFactory 他们的关系图： 使用XML方式构建包含获取数据库连接实例的数据源（DataSource)、决定事务范围和控制方式的事务管理器（TransationManager）和映射器(SQL Mapper) 这里给出简单的示例 现在让我们用代码实现创建SqlSessionFactory 使用代码方式构建不推荐这种方式，因为系应该环境的时候，我们不得不重新编译代码，不利于维护 SqlSessionSqlSession是一个接口类，它类似于你们公司前台的美女客服，它扮演着门面的作用，而真正干活的Executor，你可以认为它是公司的工程师，假设我是客户找你们公司干活，我只需要告诉前台的美女客服，我要什么信息，要做什么，过段时间，她会将结果给我，我不关心工程师是怎么工作的，这个步骤对我来说是黑箱操作 SqlSession使用方法 SqlSession用途 获取映射器，让映射器通过命名空间和方法名找到对应的SQL，发送给数据库执行后返回结果 直接通过命名信息去执行SQL返回结果 映射器映射器是由java和xml文件（或注解）共同组成的它有如下作用： 定义参数类型 描述缓存 描述SQL语句 定义查询结果和POJO对应关系 推荐XML文件配置方式的原因 java注解是受限的，功能少，使用xml更加强大灵活 如果sql很负责，条件很多，存在动态sql的时候，写在java文件里面可读性差 XML实现 java接口 xml文件 用sqlSession获取Mapper 注解方式实现 疑问一个没有实现类的接口怎么能够运行呢？其实它需要运用到java语言的动态代理来实现mybatis会为这个接口生成代理类对象，代理对象会根据“接口全路径+方法名”去匹配，找到对象的xml文件去完成它所需要的任务，返回我们需要的结果 生命周期SqlSessionFactoryBuilderSqlSessionFactoryBuilder是用于获得资源来构建SqlSessionFactory的，一旦我们创建了SqlSessionFactory，它的作用就已经完结，失去了存在的意义，这是我们应该废弃它，将它回收 所以它的生命周期只存在于方法的局部，它的作用就是生成SqlSessionFactory对象 SqlSessionFactorySqlSessionFactory的作用是创建SqlSession 所以SqlSessionFactory应该在mybatis应用的整个生命周期中 我们采取单例模式，每一个数据库只对应一个SqlSessionFactory SqlSessionSqlSession是一个会话 它的生命周期应该是在请求数据库处理事务的过程中，它是一个线程不安全的对象 在涉及多线程操作的时候我们需要当心，操作数据库需要注意其隔离级别，数据库锁等高级特性 我们往往通过finally语句块保证我们正确的关闭SqlSession MapperMapper是一个接口，没有任何实现类 它应该在一个SqlSession事务方法之内，是一个方法级别的东西 总结 （注：内容整理自《深入浅出MyBatis技术原理与实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybaits-Java ORM来源于历史、Hibernate和MyBatis比较]]></title>
      <url>%2Fpost%2Fmybtis-introduction%2F</url>
      <content type="text"><![CDATA[主要介绍了Java ORM来源于历史，JDBC、Hibernate和MyBatis的优缺点，比较他们之间区别和应用场景 JDBCJava程序是通过JDBC(Java Data Base Connectivity)连接数据库的，JDBC是由SUN公司提出的一系列规范，但是它只定义了接口的规范，而具体的实现是交给了各个数据库厂商去实现的，JDBC是一种典型的桥接模式 JDBC编程步骤 使用JDBC编程需要连接数据库，注册驱动和数据库信息 操作Connection,打开Statement对象 通过Statement执行SQL,返回结果到ResultSet对象 使用ResultSet读取数据，然后通过代码转换为具体的POJO对象 关闭数据库相关资源 JDBC弊端 工作量大 编程复杂 ORM模型ORM取代了JDBC,所有的ORM模型都是基于JDBC进行封装的，不同ORM模型对JDBC封装的强度是不一样的 ORM对象关系映射(Object Relational Mapping),简单的说ORM框架就是数据库的表和简单Java对象的映射关系模型，我们通过这层映射关系就可以简单迅速地把数据库表的数据转化为POJO 如图 HibernateHibername一问世就成为了Java世界首选的ORM框架，它是建立在POJO和数据库表模型的直接映射关系上的 Hibernate优势 消除了代码的映射规则，它全部被分离到了XML或者注解里面去配置 无需再管理数据库连接，它也可以配置在XML里面 一个会话中，不要操作多个对象，只要操作Session对象即可 关闭资源只需要关闭一个Session便可 Hibernate缺陷 全表映射带来了不便，比如更新时需要发送所有的字段 无法根据不同的条件组装不同的SQL 对多表关联和复杂SQL查询支持较差，需要自己写SQL，返回后，需要自己将数据组装为POJO 不能有效支持存储过程 虽然有HQL，但是性能较差， 大型互联网系统往往需要优化SQL，而hibernate做不到 Mybatis为了解决Hibernate的不足，一个半自动映射的框架MyBatis应运而生 mybatis是一个java持久层框架，它封装少、高性能·可优化、维护简单等优点成为了目前java移动互联网网站服务的首选持久层框架，它特别适合分布式和大数据网络数据库编程 之所以称它为半自动，是因为它需要手工匹配提供POJO、SQL和映射关系，而全表映射的Hibernate只需要提供POJO和映射关系便可 历史Mybaits的前身是Apache的一个开源项目iBatis，2010年这个项目由apache software foundation 迁移到了 google code 并且改名为Mybatis，2013年11月迁移到Github,目前mybaits是由Github维护的 名称iBatis一词来源于“internet”和“abatis”的组合 mybaits所需要提供的映射文件包含以下三个部分 SQL 映射规则 POJO 什么时候用mybaits hibernate只适用于场景不太复杂，要求性能不太苛刻的时候 mybatis拥有动态列，动态表名，存储过程支持，同时提供了简易的缓存、日志、级联，但是它的缺陷是需要你提供映射规则和sql，所以它的开发工作量比Hibernate略大一些 （注：内容整理自《深入浅出MyBatis技术原理与实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《像程序员一样思考》书摘]]></title>
      <url>%2Fpost%2Fread-think-like-programmer%2F</url>
      <content type="text"><![CDATA[这本书的第一章阐述了一些解决问题的策略，举了几个例子，还是挺有趣的，之后还用c++演示了一些具体的解决问题的方法，有兴趣可以读一下 认识到一个已解决的问题和一个未解决的问题之间可供利用的相似之处。如果我们发现问题A的一个特性与已经解决的问题B的一个特性有相似之处，就为解决A奠定了良好的基础。 用更形象化的方式重新陈述问题是一种非常出色的技巧，可以让我们拥有对问题更好的洞察力。许多程序员设法与其他程序员一起讨论问题，并不仅仅是因为对方可能已经有了答案，而是因为清晰的陈述问题常常会激发有用的新思路。重新陈述问题就相当于与其他程序员讨论问题，只不过现在一个人分饰两角。 在解决编程问题时，有时会出现无法看到通向解决方案的清晰道路的情况，但这决不能成为跳过计划和采用系统性方法的借口。更好的办法是采用一种策略，而不是通过简单地反复尝试和失败来解决问题。 在面临一个复杂的问题时，我常常会对这个问题的削减版本进行试验。这些试验常常能够产生有价值的思路。花时间研究怎样对问题进行细分通常是非常合算的，即使无法找到一种清晰的细分，仍然有助于增强对问题的理解，可以促进这个问题的解决。在解决问题时，头脑里已经有一个特点的目标总比随机的尝试要好得多，无论最终是否能够实现这个目标。 应该从最显而易见的那部分任务开始着手，如果可以解决这个部分的问题，就可以在此基础上继续执行其他可以完成的任务，通过审视自己的代码，可能会激发自己的想象力，从而解决剩余部分的问题。 基本的问题解决技巧： 总是要指定计划这也许是最重要的规则，我们事先必须要制定计划，而不是直接进行漫无方向的尝试。 重新陈述问题即使重新陈述问题并没有直接让我们获得新思路，它仍然可能在其他方面提供帮助。例如，如果我们碰到一个问题（又上级或者指导老师指派），我们可以把问题重新陈述给指派这个任务的人，以确定自己的理解无误，另外，重新重述问题对于使用其他常用的技巧也可能是一个必要的先决步骤，例如削减和划分问题。 划分问题 从自己所知的开始 削减问题 寻找类比 试验 避免陷入挫折感 如果你觉得继续干下去会陷入挫折感时，可以休息一会，一个诀窍是让手头上处理的问题不止一个，这样，如果你对一个问题感到无可奈何，可以把精力转向另一个问题，注意，如果成功的划分了问题，就可以对单个问题应用这种技巧，只要把陷入僵局的那部分问题仍到一边，转而解决其他部分问题就可以了，如果没哟可以处理其他问题，也可以离开椅子做一些其他的事情，可以热热身，放松一下脑子，例如散步。洗衣服。做伸展运动，在休息结束之前。不在考虑那个问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-运行和管理RabbitMQ]]></title>
      <url>%2Fpost%2FrabbitMQ-management%2F</url>
      <content type="text"><![CDATA[服务器管理概述RabbitMQ是使用Erlang编写的 Erlang天生就能够让应用程序无须知道对方是否在同一个机器上即可互相通信，这让RabbitMQ集群和可靠的消息路由变得简单 为了达到“简单分布式”，需要两个你可能不太熟悉的概念：Erlang节点和Erlang应用程序如果你熟悉java虚拟机（jvm）的话，两者是很相似的 启动节点节点节点用来指代RabbitMQ服务器实例，事实上，节点描述的是一个Erlang节点运行着一个Erlang应用程序 当你运行java程序时，JVM的一个实例就启动了，并且开始执行指定的java程序，与之相似，Erlang也有虚拟机，而虚拟机的每个实例我们称之为节点(node) 不同与JVM,多个Erlang应用程序可以运行在用一个节点之上，更重要的是，节点之间可以进行本地通信 当我们谈到RabbitMQ节点时，我们指的是RabbitMQ应用程序和其所有的Erlang节点 启动方法在RabbitMQ安装目录下找到./sbin目录，运行 1./rabbitmq-server 当查看控制台，你会发现不同的RabbitMQ子系统运行起来了，并准备好处理消息 日志/var/log/rabbitmq目录下找到名为rabbit@[hostname].log日志文件 -detached 1./rabbitmq-server -detached 怎么这个启动参数，以守护程序的方式在后台运行 停止节点两种方式：干净的方式和肮脏的方式 肮脏的方式当运行RabbitMQ控制台时，按下CTRL-C可以看到以下内容： 12BREAK: (a)bort (c)ountinue (p)roc info (i)nfo (l)oaded (v)ersion (k)ill (D)b-tables (d)istribution 想要关闭整个节点，选择abort就是你要的答案，但是有更好的方法来停止RabbitMQ 干净的方式这种方法会通知RabbitMQ干净地关闭，并保护好那些持久化队列，运行： 1./sbin/rabbitmqctl stop rabbitmqctl会和本地节点通信并指示其干净的关闭，你可以指定关闭不同的节点，包括远程节点，只需要传入 -n rabbit@[hostname] 即可 在RabbitMQ日志中会看到以下内容：当看到rabbit,mnesia和os_mon停止了，Rabbit节点也就完全关闭了 关闭和重启的程序的差异有时候你只想重启RabbitMQ应用程序，而同时保持Erlang节点运行，对集群来说，这种做法是必须的，因为你可能会在同一节点运行除了RabbitMQ之外的其他Erlang应用程序，这使得停止整个节点时不可取的 停止RabbitMQ，只需要运行： 1./rabbitmqctl stop_app Rabbit配置文件RabbitMQ允许你设置系统范围的可调参数并通过配置文件进行设置 该配置文件位于：/etc/rabbitmq/rabbitmq.config 文件位置可以通过rabbitmq-server脚本对CONFIG_FILE环境变量进行设置 你会发现，rabbitmq.config的文件格式有点吓人： 12[ &#123;mnesia,[&#123;dump_log_threshold, 1000&#125;]&#125;,&#123;rabbit,[&#123;vm_memory_hifgh_watermark,0.4&#125;]&#125;]. RabbitMQ配置文件事实上是一个包含嵌套哈希表的数组 mnesia指的是Mnesia数据库配置选项（Mnesia是RabbitMQ用来储存交换器和队列元数据的）rabbit值得是RabbitMQ特定的配置选项，每个选项都表达为这种形式：{[option_name],[option_value]} RabbitMQ中的每个队列，交换器和绑定的元数据（除了消息内容）都保存到Mnesia，Mnesia是内建在Erlang的非SQL型数据库 Mnesia配置选项 dump_log_write_threshold 默认值100 ，将全新追加的日志内容刷新/转储至真实数据库文件的频度 Rabbit配置项 tcp_listeners定义了RabbitMQ应该监听的非SSL加密通信的ip和端口默认值[{“0.0.0.0”,5672},] ssl_listeners{“ip地址”，端口号}数组定义了RabbitMQ应该监听的SSL加密通信的ip和端口默认空 ssl_options{“键”，值}数组指定ssl相关的选择，有效的选项有 cacertfile（CA证书文件）、certfile(服务器证书文件)、keyfile（服务器秘钥文件）和fail_if_no_prrt_cert(需要客户端安装有效证书:True/false) vm_memory_high_watermark控制Rabbit允许消耗的内存 0.4=40%默认值0.4 msg_store_file_size_limitRabbitMQ垃圾收集储存内容之前，消息存储数据库的最大大小单位是字节默认值16777216 -queue_index_max_journal_entries在转储到消息储存数据库并提交之前，消息储存日志的最大条目数默认 262144 权限设置RabbitMQ权限工作原理：用户可以为链接到RabbitMQ主机的应用程序设置不同级别的权限（读、写、和/或配置） RabbitMQ权限系统的一个好的地方在于单个用户可以跨越多个vhost进行授权 管理用户用户是访问控制的基本单元，针对一到多个vhost，其可以被赋予不同级别的访问权限，并使用标准的用户名/密码来认证用户 添加用户 1./rabbitmqctl add_user cashing-tier cashMel 输出： 12Creating user &quot;cashing-tier&quot; ......done 删除用户 1./rabbitmqctl delete_user cashing_tier 输出： 12Deleting user &quot;cashing-tier&quot;...done 注意：当你删除用户的时候，任何引用该用户的访问控制条目都会从Rabbit权限数据库中自动删除，同时，rabbbitmqctl也不会警告你与用户相关的访问控制条目也会一并被删除 查看当前存在哪些用户 1./rabbitmqctl list_users 输出 1234Listing users ..cashing-tierguest...done 修改用户的密码 1./rabbitmqctl change_password cashing-tier coml3xPassword 输出 12Changeing password for user &quot;cashing-tier&quot; ......done RabbitMQ权限系统从1.6.0版本开始，RabbitMQ实现了一套访问控制列表（ACL）风格的权限系统，新的权限系统允许大量细粒度控制，同时可以授予用户读、写、和配置选项，这三者有何差异？ 读–有关消费信息的任何操作，包括“清除”整个队列 写–发布消息（同样需要绑定操作的成功） 配置–队列和交换器的创建和删除 下图展示了不同AMQP命令的列表和对应的权限 每一条访问控制条目由以下四部分组成： 被授予访问权限的用户 权限控制应用的vhost 原药授予的读/写/配置权限的组合 权限范围 注意：访问控制条目是无法跨越vhost的，加入你想给用户cashing-tier在vhost oak和vhost sycamore上赋予相同权限，那么你必须穿件两份访问控制条目 例子：假设你有名为sycamore的vhost,你想要授予cashing-tier完全的访问权限（配置、写和读权限），你需要rabbitmqctl的set_permissions命令完成 12./rabbitmqctl set_permissions -p sycamore \cashing-tier &quot;.*&quot;&quot;.*&quot;&quot;.*&quot; 输出： 12Setting permisssion for user &quot;cashing-tier&quot; in vhost &quot;sycamore ......done 解释： -p sycamore 这告诉set_permissions条目应该应用在那个vhost上 cashing-tier 被授予权限的用户 “.“”.“”.*” 这是授予的权限，分别映射到配置、写、读 权限值中三个值都是正则表达式，”.*”指代所有权限，以为这匹配任何队列或交换器的名字 删除权限 1.rabbitmqctl clear_permissions -p oak cashing-tier 输出 12Clearing permissions for user &quot;cashing-tier&quot; in vhost &quot;oak&quot;......done 查看用户在RabbitMQ服务器所有vhost上的权限 1./rabbitmq list_user_permissons cashing-tier 输出： 使用统计列出队列和消息数目 1./rabbitmqctl list_queues 输出： 12345Listing queues ...msg-inbox-logs 0msg-inbox-errors 0all-logs 3...done 这些都是默认vhost的信息。如果想要获取另一个vhost的话，可以使用： 1./rabbitmqctl list_queues -p sycamore 如果运行rabbitmqctl命令二不携带任何选项的话，就会展示帮助信息 查看交换器和绑定 1./rabbitmqctl list_exchanges 输出： 你会发现若干个交换器早已经声明好了，例如amq.topic,amq.ditect和amq.fanout,AMQP规范里面对这些交换器有规定 只有一个direct类型而没有交换器名称，这就是匿名交换器，每个队列默认绑定到该交换器 （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-理解消息通信-持久化策略]]></title>
      <url>%2Fpost%2FrabbitMQ-persistence%2F</url>
      <content type="text"><![CDATA[概述重启RabbitMQ后，队列和交换器都会消失（随同里面的消息），原因在于每个队列和交换器的durable属性，该属性默认为false durable属性，它决定了RabbitMQ是否需要在崩溃或者重启之后重新创建队列（或者交换器）将他设置为true，这样就不需要在服务器断电后重新创建队列和交换器了 把这个属性设置为true不足以让消息幸免于重启，光这样还不够 如果想要从Rabbit崩溃中恢复，那么消息必须做到这三点： 把它的投递模式选项设置为2（持久） 发送到持久化的交换器 到达持久化队列 RabbitMQ确保持久性消息能从服务器重启中恢复的方式是,将他们写入磁盘上的一个持久化日志文件。当发布一条持久化消息到持久交换器上时，Rabbit会在消息提交到日志文件后才发送响应 如果RabbitMQ重启，服务器会自动重建交换器和队列，重播持久性日志文件中的消息到合适的队列或者交换器上 你可以为所有消息都启动持久化，但是你也要为此付出代价：性能，写入磁盘要比写入内存慢了不止一点点，而且会极大的减少RabbitMQ服务器每秒可处理的消息总数，导致消息吞度量降低至少10倍的情况并不少见 持久化消息在RabbitMQ内建集群环境中工作的并不好，实际上集群上的队列均匀分布在各个节点上而且没有冗余，如果运行a队列的节点崩溃了，那么直到节点恢复前，这个队列就从整个集群消失了，而且这个节点上的所有队列不可用，而且持久化队列也无法重建 什么情况下应该使用持久化消息通信呢？首先你需要分析性能需求，如果持久化通信可以满足性能需求，那么用这种机制是极佳的方式 AMQP事务(transaction)发布操作不返回任何信息给生产者，那你怎么知道服务器是否已经持久化消息到硬盘？可能写入硬盘之前服务器就宕机了，消息丢失，你却不知道 所以你需要把这些行为包装在一个事务中 不要把AMQP事务与大多数数据库事务搞混了，在AMQP中，在把信道设置为事务模式后，你通过信道发送那些想要确认的消息，之后还有多个其他AMQP命令，这些命令时执行还是忽略，取决于第一条消息发送是否成功，一旦你发送完所有命令，就可以提交事务了 虽然事务是正式的AMQP 0-9-1规范的一部分，但是它也有致命弱点：几乎吸干了Rabbit的性能，使用事务不但会降低大约2-10倍的消息吞度量，而且会使生产者应用程序之间产生同步，而你使用消息通信就是想避免同步 发送方确认模式RabbitMQ团队决定拿出更好的方案来保证消息投递：发送方确认模式 和事务相仿，你需要告诉Rabbit将信道设置成confirm模式 一旦信道进入confirm模式，所有在信道上发布的消息都会被指派一个唯一的id号（从1开始) 一旦消息被投递给所有匹配的队列后，信道会发送一个发送方确认模式给生产者应用程序（包含唯一的id），这使得生产者知晓消息已经安全到达目的队列了(如果消息和队列是可持久化的，那么确认消息只会在队列将消息写入磁盘后才会发生) 发送方确认模式最大的好处是他们是异步的 如果Rabbit发生了内部错误从而导致了消息的丢失，Rabbit会发送一条nack消息，就像发送方确认消息那样，只不过这次说明的是消息已经丢失了 由于没有消息回滚的概念，因此发送方确认模式更加轻量级，同时对Rabbit代理服务器的性能几乎没有影响 （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-理解消息通信-虚拟主机和隔离]]></title>
      <url>%2Fpost%2FrabbitMQ-vhost%2F</url>
      <content type="text"><![CDATA[每个RabbitMQ服务器都能创建虚拟的消息服务器，我们称之为虚拟主机(vhost)每一个vhost本质上是一个mini版的RabbitMQ服务器，拥有自己的队列、交换器和绑定等等 多租户模式：虚拟主机和隔离概述 每个RabbitMQ服务器都能创建虚拟的消息服务器，我们称之为虚拟主机(vhost)每一个vhost本质上是一个mini版的RabbitMQ服务器，拥有自己的队列、交换器和绑定等等 更重要的是，他拥有自己的权限机制，这使得你能够安全地使用一个RabbitMQ服务器来服务众多的应用程序 vhost就像是虚拟机之与物理服务器一样：他们在各个实例间提供逻辑上的分离，允许你为不同程序安全保密地运行数据，它既能将同一个Rabbit的众多客户区分开来，又可以避免队列和交换器命名冲突 vhost是AMQP概念的基础，你必须在连接时进行指定 RabbitMQ包含了一个开箱即用的默认vhost:”/“，如果你不需要多个vhost，那么就使用默认的吧，使用缺省的guest用户名和密码guest就可以访问默认的vhost 当你在RabbitMQ集群上创建vhost，整个集群上都会创建该vhost，vhost不仅消除了为基础架构中的每一层运行一个RabbitMQ服务器的需要，同样也避免了为每一层创建不同集群 如何创建vhostvhost和权限控制非常独特，他们是AMQP中唯一无法通过AMQP协议的基元（不同与队列，交换器和绑定） 创建vhost 你需要通过RabbitMQ的安装路径下的./sbin/目录中的rabbitmqctl工具来创建 运行： 1rabbitmqctl add_vhost[vhost_name] 可以创建一个vhost，其中[vhost_name]就是你想要创建的vhost 删除vhost 1rabbitmqctl delete_vhost[vhsost_name] 查看Rabbit服务器上运行着那些vhost 1rabbitmqctl list_vhost 你就会看到如下所示的内容 123456$ ./sbin/rabbitmqctl list_vhostsListing vhosts .../oaksycamore...done. 管理远程RabbitMQ节点 1-n rabbit@[server_name] rabbit表示Erlang应用程序名称[server_name]表示ip （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-理解消息通信-交换器和绑定]]></title>
      <url>%2Fpost%2FrabbitMQ-exchange%2F</url>
      <content type="text"><![CDATA[交换器和绑定当你想要将消息投递到队列时，你通过把消息发送给交换器来完成。然后，根据确定的规则，RabbitMQ将会决定消息该投递到哪个队列，这些规则被称为路由键(routing key)。队列通过路由键绑定到交换器，当你把消息发送到代理服务器时，消息将拥有一个路由键（即使是空的）Rabbit也会将其和绑定使用的路由键进行匹配，如果匹配的话，那么消息将会投递到队列，如果路由的消息不匹配任何绑定模式的话，消息将进入“黑洞” 为什么大费周章使用这种交换器和绑定？ 这样可以满足更多复制的使用场景，利于发布\订阅或者多播 对于发送消息给服务器的发布者来说，它不需要关心服务器的另一端（即队列和消费者） 交换器类型 direct fanout topic headers 每一种类型实现了不同的路由算法 direct非常简单，如果路由键匹配的话，消息就会被投递到对应的队列 服务器必须实现direct类型交换器，包含一个空白字符串名称的默认交换器，当声明一个队列时，他会自动绑定到默认交换器，并以队列名称作为路由键。你可以使用如下代码发送消息到之前声明的队列去，前提是你已经获得了信道实例： $channel-&gt;basic_publish($msg,&apos;&apos;,&apos;queue-name&apos;); 第一个参数是你想要发送的消息内容，第二个参数是一个空字符串，指定了默认交换器，第三个参数是路由键 当默认的direct交换器无法满足你的需求是，你可以声明你自己的交换器，只需要发送exchange.declare命令并设置合适的菜蔬就行了 fanout这种类型的交换器会将收到的消息广播到绑定的队列上 当你发送一条消息到fanout交换器时，他会把消息投递给所有附加在此交换器的队列，这允许你对单条消息做不同的反应 举个例子：一个web应用可能需要在用户上传新的图片时做几件事情： 用户相册清除缓存 用户得到积分奖励 你可以将两个队列绑定到图中的交换器上，一个用于清楚缓存，一个用于积分奖励 使用fanout交换器，你唯一需要做的就是为新的消费者写一段代码，然后声明新的队列并将其绑定到fanout交换器上，而不用修改发送方的代码 topic这类交换器允许你实现有趣的消息通信场景，它使得来自不同源头的消息能够到达同一个队列 让我们以web应用程序日志系统作为示例你拥有不同的日志级别 例如：error info 和warning你的程序拥有以下几个模块：user-profile,image-gallery,msg-inbox等 如果在发送消息的动作失败时，你想要报告一个error的话，则可以编写以下代码 $channel-&gt; basic_publish($msg,&apos;logs-exchange&apos;,&apos;error.msg-inbox&apos;); 然后你声明了一个msg-inbox-errors队列，你可以将其绑定到交换器来接收消息 $channel-&gt;queue_bind(&apos;msg-inbox-errors&apos;,&apos;logs-exchange&apos;,&apos;error.msg-inbox&apos;); 目前为止，看起来和使用direct交换器很像但是如果你想要一个队列监听msg-inbox模块的所有级别的话，你该怎么做？你可以通过将新的队列绑定到已有的同一个交换器来实现就像下面这样： $channel-&gt;queue_bind(&apos;msg-inbox-logs&apos;,&apos;logs-exchange&apos;,&apos;*.msg-inbox&apos;); msg-index-logs队列将会接收从msg-inbox模块发来的所有error，warning和info日志 通配符 单个“.”把路由键分为几部分 “*”匹配特定位置的任意文本 “*”操作符将“.”视为分隔符，“#”操作符没有分块的概念，它将任意”.”字符均视为关键词的匹配部分 为了实现匹配所有规则，你可以使用“#” $channel-&gt;queue_bind(&apos;all-log&apos;,&apos;log-exchage&apos;,‘#’) headers允许你匹配AMQP消息的header而非路由键，除此之外，和direct交换器完全一致，但是性能差很多，因此它并不实用，而且几乎用不到 我们已经理解了这几种交换器类型，并能体会AMQP的强大之处了，你可以对服务器的行为编程以满足自己的需求，它既能够以发布\订阅模式的设置方式作为队列服务器使用，也可以作为RPC服务器使用 总结 AMQP架构中最关键的几个组件分别是 交换器 队列和绑定 根据绑定规则将队列绑定到交换器上 消息是发布到交换器上的 有四种类型的交换器 - direct fanout topic headers 基于消息的路由键和交换器类型，服务器会决定将消息投递到哪个队列去 （注：内容整理自《Rabbit实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-理解消息通信-队列]]></title>
      <url>%2Fpost%2FrabbitMQ-queue%2F</url>
      <content type="text"><![CDATA[队列AMQP消息路由必须有三部分：交换器、队列和绑定 生产者把消息发布到交换器上，消息最终到达队列，并被消费者接收，绑定决定了消息如何从路由器路由到特定的队列 消费者通过以下两种方式从特定的队列中接收消息： 通过AMQP的basic.comsume命令订阅，这样就会将信道置为接收模式，知道取消对队列的订阅为止 如果我们只想从队列获取单条消息而不是持续订阅，向队列发送单条消息是通过AMQP的basic.get，如果要获取更多的消息的话，需要再次发送basic.get命令。（不要将basic.get放在循环里面来替代basic.consume）因为这样做会影响Rabbit的性能 如果消息到达了无人订阅的队列呢？这种情况下，消息会在队列中等待，一旦有消费者订阅到该队列，那么队列上的消息会发给消费者 当Rabbit队列拥有多个消费者时，队列收到的消息将以循环（round-robin）的方式发给消费者，每条消息只会发送给一个订阅的消费者 消费者接收到每一条消息都必须进行确认，消费者必须通过AMQP的basic.ack命令显式的向RabbitMQ发送一个确认，或者在订阅到队列的时候就将auto_ack参数设置为true，当设置了auto_ack时，一旦消费者接收消息，RabbitMQ会自动视其确认了消息 注意：消费者对消息的确认和告诉生产者消息已经被接收了这两件事情毫无关系 消费者通过确认命令告诉rabbitMQ它已经正确的接收了消息，通过RabbitMQ才能安全地把消息从队列中删除 如果消费者收到一条消息，然后确认之前从Rabbit断开连接，RabbitMQ会认为这套消息没有分发，然后重新分发给下一个订阅的消费者 如果应用程序有bug而忘记确认消息的话，Rabbit将不会给该消费者发送更多的消息了，这是因为在上一条消息被确认之前，Rabbit会认为这个消费者并没有准备好接受下一条消息 如果想要明确拒绝而不是确认收到该消息的话，该如何呢？两种选择： 把消费者从RabbitMQ断开连接，这会导致RabbitMQ自动重新把消息入队给另一个消费者 如果你使用的RabbitMQ 2.0.0或者更新的版本，那就使用AMQP的basic.reject命令，顾名思义：basic.reject允许消费者拒绝RabbitMQ发送的消息，如果把reject命令的requeue参数设置为true的话，RabbitMQ会将消息重新发送给下一个订阅的消费者 为什么丢弃一条消息时，要使用basic.reject命令，并将requeue参数设置成false来替代确认消息呢？在将来的RabbitMQ版本中会支持一个特殊的“死信”队列，用来存放那些被拒绝而不重入队列的消息，如果应用程序想自动从死信队列功能总获益的话，需要使用reject命令。并将requeue参数设置为false 如何创建队列消费者和生产者都能使用AMQP的queue.declare命令来创建队列 如果消费者在同一条信道上订阅了另一个队列的话，就无法再声明队列了。必须首先取消订阅，将信道设置为”传输”模式 创建队列时，你常常想要指定队列名称，如果不指定，Rabbit会分配一个随机名称并在queue,declare命令的响应中返回 以下是队列设置中另一些有用的参数： exclusive如果设置为true的话，队列将变成私有的，只有你的应用才能够消费队列消息 auto-delete当最后一个消费者取消订阅的时候，队列就会自动移除 如果尝试声明一个已经存在的队列只要声明参数完全匹配现存的队列的话，Rabbit什么都不做，并成功返回 如果你只想检验队列是否存在，则可以设置queue,declare的passive选择为true，在该设置下，如果队列存在，那么queue.declare命令会成功返回，如果不存在，命令不会创建队列而会返回一个错误 总结队列时AMQP消息通信的基础模块 为消息提供了住所，消息在此等待消费 对负载均衡来说，队列时绝佳方案，值只需附加一堆消费者，并让RabbitMQ以循环的方式均匀的分配发来的信息 队列时Rabbit中的消息的最后的终点 （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-理解消息通信-消费者和生产者]]></title>
      <url>%2Fpost%2FrabbitMQ-producer-consumer%2F</url>
      <content type="text"><![CDATA[消费者和生产者生产者生产者创建消息，然后发布到代理服务器 消息什么是消息呢？消息包含两部分：有效载荷（payload）和标签（label） 有效负荷就是你想要传输的数据，它可以试任何内容，一个json数组或者是你最喜欢的mp4，Rabbitmq不会在意这些 标签描述了有效载荷，并且RabbitMQ用它来决定谁将获得消息的拷贝举例来说：不同于TCP协议，当你明确指定发送方和接收方时，AMQP只会用标签表述这条信息，然后把消息交由Rabbit，Rabbit会根据标签把消息发送给感兴趣的接收方，这种通信方式是一种“发后即忘(fire-and-forget)”的单项方式 消费者消费者很容易理解，他们连接到代理服务器上，并订阅到队列上，把消息想象成一个具名邮箱，每当消息到达特定邮箱时，RabbitMQ会将其发送给其中一个订阅的/监听消费者 当消费者接受到消息是，它只得到消息的一部分：有效载荷 信道你必须首先连接到Rabbit，才能消费或者发布消息，你在应用程序和Rabbit代理服务器之间创建一条TCP连接，一旦TCP连接打开，应用程序就可以创建一条AMQP信道 信道是建立在“真实的”tcp连接内的虚拟连接 为什么不直接通过TCP连接发送AMQP命令？主要原因是对于操作系统来说建立和销毁TCP会话是非常昂贵的开销 如果我们为所有线程只使用一条TCP连接，但又确保每个线程的私密性，就像拥有独立连接一样的话，那不就非常完美了 线程启动后会在现成的连接上创建一条信道，也就获得了连接Rabbit上的私密通信路径，而不会给操作系统的TCP栈造成额外负担，在一条TCP连接上创建多少条信道是没有限制的，把它想象成一束光纤电缆就可以了 有了AMQP,你可以灵活的使用多个信道来满足应用程序的需求，而不会有众多tcp连接的开销 总结消息通信，特别是AMQP,可以被当做加强版的传输层 当你理解了这些概念，你就吧RabbitMQ看做软件的路由器了 （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RabbitMQ-概述]]></title>
      <url>%2Fpost%2FrabbitMQ-general%2F</url>
      <content type="text"><![CDATA[RabbitMQ是一个开源的消息代理和队列服务器，用来通过普通协议在完全不同的应用之间共享数据，或者简单地将作业队列以便让分布式服务器进行处理 RabbitMQ概述RabbitMQ是一个开源的消息代理和队列服务器，用来通过普通协议在完全不同的应用之间共享数据，或者简单地将作业队列以便让分布式服务器进行处理 它现实了AMQP协议，并且遵循Mozilla Public License开源协议，它支持多种语言，可以方便的和spring集成 消息队列使用消息将应用程序连接起来，这些消息通过像RabbitMQ这样的消息代理服务器在应用程序之间路由 消息通信起源 Teknekron1983年一位来自孟买的26岁工程师脑海中浮现了一个激进的想法：为什么没有一种通用的软件“总线”–一种通信系统，可以解决程序间繁重的信息通信工作呢？来自MIT的硬件设计教育工作者Vivek Ranadive设想了一种通用的软件总线，就想主板上的总线那样，供其他应用程序接入，在1983年，Teknekron诞生了 TIB随后，诞生了世界上第一个现代消息队列软件：The information bus IBM MQ/MSMQ20世纪80年代后期，IBM开始研究开发自己的消息队列软件，IBM MQ产品系列问世同时期，微软也在消息通信市场崭露头角，MSMQ JMS中小技术公司对高价格MQ供应商表示不满，金融服务公司也对此激动不起来，为了解决这个问题，Java Message Servive在2001年诞生了，JMS只需要针对JMS API编程，选择合适的MQ驱动即可，JMS会打理好其他部分，问题是你在尝试使用单独接口标准化来胶合众多不同的接口，就想把不同类型的衣服黏在一起，缝合处终究会裂开，使用JMS的应用程序会变得更加脆弱，我们需要新的消息通信标准化方案 救世主AMQPAMQP:advanced Message Queuing Protocol(高级消息队列协议)2004年开发从一开始就设计成为开发标准，以解决众多的消息队列需求和拓扑结构问题凭借开放，任何人都可以执行这一标准，针对标准编码的任何人都可以和任意AMQP供应商提供的MQ服务器进行交互 消息队列简史 关于为什么叫Rabbit兔子是行动非常迅速的动物而且繁殖起来也非常疯狂 为什么选择RabbitMQ 除了Qpid外，Rabbit是唯一实现了AMQP标准的代理服务器 正是由于Erlang，RabbitMQ集群不可思议的简单 比竞争对手更可靠，更能防止奔溃 （注：内容整理自《RabbitMQ实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《重新定义公司：谷歌是如何运营的》书摘]]></title>
      <url>%2Fpost%2Fread-how-google-work%2F</url>
      <content type="text"><![CDATA[大卫帕卡德严肃对待企业文化，1960年，在一次面对惠普管理者的演讲中，他提出，企业之所以存在，就是为了“做些有意义的事情，为社会做一份贡献……放眼四周，我们仍然能看到那些只盯着钱的人，但是多数人之所以有动力前进，是因为他们想要做一番事业：制作一款产品，提供一种服务，一言以蔽之，就是想要做些有意义的事” 谷歌鼓励员工享受假期，但这么做，并不是宣扬“工作与生活平衡”的理念，如果某个人对企业的成败至关重要，以至于他一旦抽身一两周，企业就运营不下去了，那么这个问题是决定不同忽视的，没有人对企业来说是真正不可或缺的，有时候，有的员工会有意的把自己打造成企业不可或缺的人物，这或许是他们的自负在作祟 如果你决心开创一番新事业（或是对现有的事业做出彻底改造），就要面对冗长的工作日，无眠的夜晚，甚至连参加生日派对的时间都要被砍去，另外，你还必要聘请那些跟你志同道合、甘愿与你做同样牺牲的人，要做到这一切，你必须拥有坚信自己定会成功的激情，也要拥有为梦想而实干的理想，这不仅需要潜心投入、坚韧不拔、更重要的心无旁骛，在迎战敌军的时候，以色列坦克指挥官并不会大喊“冲啊”而是会用“aH’ ‘cha’ rye” 来动员自己的部队，这句话是希伯来语，翻译过来就是“跟我来”，任何有志于做创意精英领导者的人，都需要拥有这样的态度 如果你觉得你已经定型，那么无论环境如何，你都会一次又一次的重蹈覆辙，但如果你拥有成长型思维模式，你就会相信，经过努力，你的个性是能够培养和改变的，你能够改变自己，也能够适应环境，实际上，在不得已的情况下，你反而能更好、更自如地做出改变和适应，如果你的思维是成长型的，你就会为自己拟定“学习目标” 这些目标会鼓励你勇于挑战，而不会因为提出愚蠢的问题或者给出错误的答案而惴惴不安，之所以不因此而担忧，是因为你是好学的动物，而从长远看来，有助于你积累更多的知识，攀登更高的山峰 人生就像小水道，想从中得到什么，要看你往里面扔了些什么 如果你不知道前进的方向，那么就要注意了，因为你也许实现不了自己的目标 把问题解释清楚，就如同问题解决了一半 不要在会议一开始就声明自己的立场，你的任务，是抛开大家的职位差异，鼓励每个人发表自己的观点，如果领导者在这时表面态度，那么大家就难以各抒己见了 积极寻找最佳途径，而不要一味坚持自己的意愿 不要成为紧迫感的奴隶，在最后一刻来临之前，都要保持灵活变通 如果你想改变他人，不仅要晓之以理，更要学会动之以情 一般情况下一个正常人的观点不可能错得一无是处 他应该召开会议，保证会议质量，设立会议目标，确立与会人员，以及至少提前24小时传达议事日程，在会议结束后48小时内，决策者应该用email的形式向每一位与会者以及任何需要了解会议情况的人传达会议达成的决策以及待办事项 信息才是真正的生命之源，要想在21世纪建立一家企业，吸引创意精英并引导他们大展宏图才是成功的关键，但创意精英若不能接触大量的信息，这一切只是空谈 身而为人，在于提出问题，而非回答问题 赛车手马里奥安得雷蒂说话：“如果万事都看似尽在掌握中，那只能说明你的速度不够快” 人们对咱们的使用不足，也低估了赞美的价值，该赞美的时候，不要吝惜 70%的资源配置给核心业务，20%分配给新兴产业，剩下的10%投在全新产品上 人类建筑最为辉煌的时期，就是那些限制最多的时期 如果你不提出问题，就永远找不到解决方案]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch段合并]]></title>
      <url>%2Fpost%2Felasticsearch-segment-merging%2F</url>
      <content type="text"><![CDATA[elasticsearch 中每个索引都会创建一个到多个分片和零个到多个副本，这些分片或副本实质上都是lucene索引 lucene索引是基于多个索引段创建，索引文件中绝大部分数据都是只写一次，读多次，而只有用于保存文档删除信息的文件才会被多次更改 在某些时刻，当某种条件满足时，多个索引段会被拷贝合并到一个更大的索引段，而那些旧的索引段会被抛弃并从磁盘中删除，这操作叫做段合并（segment merging） 为什么要进行段合并？ 索引段的个数越多，搜索性能越低且消耗内存更多 索引段是不可变的，物理上你并不能从中删除信息（如果你碰巧从索引中删除了大量文档，但这些文档只是做了删除标记，物理上并没有被删除）而当段合并发送时，这些标记为删除的文档并没有被复制到新的索引段中 段合并好处 当多个索引段合并为一个的时候，会减少索引段的数量并提高搜索速度 同时也会减少索引的容量（文档数） 段合并代价 IO操作代价，在速度较慢的系统中，段合并会显著影响性能 elasticsearch允许用户选择段合并政策（merge policy）及储存级节流（store level throttling） 选择正确的段合并策略尽管段合并是lucene的责任，elasticsearch也允许用户配置想用的段合并策略到目前为止有三种可用的合并策略： tiered（默认）它能合并大小相似的索引段，并考虑每层允许的索引段的最大个数 log_byte_size该策略不断地以字节数的对数为计算单位，选择多个索引来合并创建新索引 log_doc与log_byte_size类似，不同的是前者基于索引的字节数计算，后者基于索引段文档数计算 为了告知elasticsearch我们想使用的段合并策略，可以将配置文件的index.merge.policy字段泪痣成我们期望的段合并策略类型例如：index.merge.policy.type: tiered 调度es允许我们定制合并策略的执行方式，调度器分两种默认的是并发合并调度器 ConcurrentMerge-Scheduler 并发合并调度器该调度器使用多线程执行索引合并操作 顺序合并调度器它使用同一个线程执行所有的索引合并操作，在执行合并时，该线程的其他文档处理都会被挂起，从而索引操作会延迟进行 （注：内容整理自《深入理解elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch底层索引控制]]></title>
      <url>%2Fpost%2Felasticsearch-index-control%2F</url>
      <content type="text"><![CDATA[如何使用不同评分公式及其特性 如何使用不同的倒排表格式极其特性 如何处理准实时搜索、实时读取、以及搜索器重新打开之后的动作 配置搜索事务日志以满足应用需求，并查看它对部署的影响 如何使用不同评分公式及其特性2012年Apache lucene 4.0发布之后，用户便可以改变默认的基于TF/IDF的评分算法了，这是因为lucene的API做了一些改变，使得用户能轻松地修改和扩展该评分公式 新增的相似度模型： Okipi BM25模型 一种基于概率模型的相似度模型，可用于估算文档与给定查询匹配的概率 在短文本文档上的效果最好，因为这种场景中重复词项对文档的总体得分损害较大 随机偏离模型(Divergence from randomness) 是一种基于同名概率模型相似度的模型 随机偏离模型在类似自然语言的文本上效果较好 基于信息的模型(Infomation based) 与随机偏离模型类型，也在类似自然语言的文本上拥有较好的效果 为每个字段配置相似度模型我们希望name字段使用BM25模型，我们需要添加similarity字段 如何使用不同的倒排表格式极其特性lucene 4.0的另外一个显著改变就是允许用户改变索引文件编码方式，在此之前，只能通过修改lucene内核代码来实现，而lucene 4.0出现后，它提供了灵活的索引方式，允许用户改变倒排索引的写入方式 为什么用户需要修改lucene索引写入格式？理由之一是性能 为每个字段配置编解码器需要在字段配置文件中添加一个postings_format属性，并将具体的编解码器所对应的属性值赋给它 有哪些倒排表格式 default 默认的格式，该格式提供了储存字段和词向量压缩功能 puling 该编码器将包含大量不同值的字段的倒排表编码为词项数组 这会减少lucene在搜索文档时的查找操作，可以提高这种字段的搜索速度 direct 在读索引阶段将词项载入词典，且词项在内存中为未压缩状态， 能提升常用字段的查询性能，但也需要谨慎使用，非常消耗内存 memory 改解编码器将所有数据写入磁盘 bloom_default 是default编码器的一种扩展 用户快速判断某个值是否存在 如何处理准实时搜索、实时读取、以及搜索器重新打开之后的动作在索引期新文档会写入索引段，索引段是独立的lucene索引，这意味着查询时可以与索引并行的，只是不会有新增的索引段被添加到可被搜索的索引段集合之中 Apache lucene通过创建后续的segment_N文件来实现此功能，且该文件列举了索引中的索引段，这个过程称之为提交(committing) Lucene以一种安全的方式来执行该操作，能确保索引更改以原子操作方式写入索引 尽管我们向索引中添加了文档，但它并没有执行提交commit操作， lucene使用了一个叫做searcher的抽象类类执行新索引段的加入 searcher重新打开的过程叫做刷新（refresh） 在例子中可以使用下面的命令： curl -xGET localhost:9200/test/_refresh 更改默认的刷新时间可以更改elasticsearch中的index.refresh_interval参数或者使用配置更新相关的api 刷新操作是耗资源的，因此刷新间隔时间越长，索引速度越快 如果需要长时间高速建索引，并且在建索引结束之前暂时不执行查询，那么可以考虑将index.refresh_interval参数设置为-1.然后在建索引结束以后再将改参数恢复为默认值 事务日志lucene 能保证索引的一致性，但是这并不能保证当往索引中写数据失败时不会损失数据，；另外，频繁提交操作会导致严重的性能问题 elasticsearch通过使用 事务日志（transaction log）来解决这些问题，它能保存所有未提交的事务，而es会不时创建一个新的日志文件用于记录每个事务的后续操作，当有错误发生时就会检查事务日志，必要是会再次执行某些操作，以确保没有丢失任何更改的信息 事务日志中的信息与存储介质之间的同步，称为事务日志刷新（flushing） 除了自动的事务日志刷新以外，也可以使用对应的api curl -XGET localhost:9200/flush 事务日志相关配置 index.translog.flush_threshold_period 该参数的默认值为30分钟，它控制了强制自动事务日志刷新的时间间隔 index.translog.flush_threshold_ops该参数确定了一个最大操作数，在上一次事务日志刷新以后，当索引更改操作次数超过了该参数值时，强制进行事务日志刷新操作，默认为5000 index.trans.flush_threshold_size该参数确定了事务日志的最大容量，当容量大于等于该参数值，就强行进行事务日志刷新操作，默认为200MB index.translog.disable_flush禁用事务日志刷新 （注：内容整理自《深入理解elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch使用过滤器优化查询]]></title>
      <url>%2Fpost%2Felasticsearc-filtered%2F</url>
      <content type="text"><![CDATA[elasticsearch提供了一种特殊的缓存，即过滤器缓存（filter cache），用来储存过滤器的结果 被缓存的过滤器不需要消耗过多的内存，因为他们只储存了哪些文档能与过滤器相匹配的相关信息，而且可供后续所有与之相关的查询重复使用，从而极大的提高了查询性能 执行下面这个查询：1234567891011121314&#123; "query":&#123; "bool":&#123; "must":[ &#123; "term":&#123;"name":"joe"&#125; &#125;, &#123; "term":&#123;"year":1981&#125; &#125; ] &#125; &#125;&#125; 该查询能查询出满足指定姓名和出生年代条件的足球运动员，只有同时满足两个条件的查询才可以被缓存起来。 优化这个查询：人名有太多可能性，它不是完美的缓存候选对象，而年代是，我们使用另一种查询方法，该查询组合了查询类型与过滤器： 123456789101112&#123; "query":&#123; "filtered":&#123; "query":&#123; "term"：&#123;"name":"joe"&#125; &#125;, "filter":&#123; "term":&#123;"year":1981&#125; &#125; &#125; &#125;&#125; 第一次执行该查询以后，过滤器会被es缓存起来，如果后续的其他查询也要使用该过滤器，则她会被重复使用，避免es重复加载相关数据 （注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch数据更新API]]></title>
      <url>%2Fpost%2Felasticsearch-update%2F</url>
      <content type="text"><![CDATA[简单字段更新 使用脚本按条件更新 使用更新api创建或删除文档 为什么我们无法真的更新索引中的文档在索引一个新文档的时候，lucene会对每个字段进行分析并产生词条流，词条流中的词条可以会经过过滤器的额外处理，而没有过滤掉的词条会写入倒排索引中，索引过程中，一些不需要的信息可能被抛弃，这些信息包括： 某些特殊的词条位置（当词向量没有储存时） 特定词汇（停用词或同义词） 词条的变形（如词干还原） 因此我们无法更新索引中的文档，而且每次修改时不得不向索引发送文档所有字段数据 _sourceelasticsearch可以通过使用_source伪字段储存和检索文档的原始数据来解决这个问题，当用户需要更改文档时，elasticsearch会获取_soucre字段中的值，做相应的修改，然后向索引提交一个新文档，替换老文档 为了使得这个特征起效,_source字段必须是可用的 局限更新命令的一个很大的局限就是它只能更新单个文档，目前还不支持通过查询实现批量更新 更新api文档更新可以通过执行发送至端点的更新请求来实现，也可以通过在更新请求的url中添加_update参数来更新某个特定的文档 比如 /library/book/_update 本文其余部分都将使用下面命令索引的文档： 简单字段更新使用下面命令 curl -XPOST localhost:9200/library/book/1/_update -d&apos;{ &quot;doc&quot;:{ &quot;title&quot;:&quot;The Complete Sherlock Homes Book&quot; &quot;year&quot;:1935 } } 作为响应，elasticsearch将返回一个与建索引操作类似的回复： {&quot;ok&quot;:true,&quot;_index&quot;:&quot;library&quot;,&quot;_type&quot;:&quot;book&quot;,&quot;_id&quot;:&quot;1&quot;,&quot;_version&quot;2} 现在，如果我们想从索引获取刚才修改的文档是否被修改了，可以执行下面的命令： curl -xGET localhost:9200/library/book/1?pretty 改命令响应如下： 可以看到_source字段中的值已经被修改了 使用脚本按条件更新有时候在修改文档的时候添加一些额外的逻辑是很有好处的例如我们发送下面的请求： ctx变量来引用源文档 使用更新api创建或删除文档更新api不仅仅可以修改文档，也可以用来操作整个文档 upsert属性允许用户在当url中地址不存在时创建一个新文档查看下面命令：该命令修改了某个已有的文档year字段，如果文档不存在，将会创建一个文档，并且该文档会创建一个新字段title 前面的命令还可以使用脚本重写为以下形式 有条件的移除整个文档： （注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch排序]]></title>
      <url>%2Fpost%2Felasticsearch-sort%2F</url>
      <content type="text"><![CDATA[当你发送请求至elasticsearch,返回文档默认按文档得分降序排序，这是通常我们想要了，然而，有时候我们希望能改变这种排序方式 下面的例子就容易做到：该查询会返回所有在title字段上至少命中一个词项的文档，并且基于section数据排序 也可以通过添加查询sort部分的missing属性为那些section字段缺失的文档定制排序行为 基于多值字段的排序某些文档的release-dates字段里面储存了多个电影上映日期（同一部电影在不同国家的上映日期不同）我们可以构造查询请求：例子中es将基于每个文档的release_dates字段的最小值进行排序mode参数可以设置为以上值： min 按照该字段最小值排序 max 按照该字段最大值进行排序 avg 按照多个字段平均值排序 sum 按照字段总和进行排序 后面两个选项只对数值类型有效 基于多值geo字段的排序es提供了基于多维坐标系数据的排序，我们通过一个实例了解这种类型的排序例如，要查找特点国家里离自己最近的一个机构我们使用下面的这个映射：查询如图：查询结果：我们可以看到，返回结果包含这个值：“sort”：[0.0]这是因为返回文档的地理坐标和查询中的坐标精确匹配还可以设置mode属性为max，min，avg例如avg代表：此时基于字段中的地理位置坐标与查询坐标的距离的均值排序 基于嵌套对象的排序继续嵌套对象的排序，对以下两种情形都适用： 适用了显式嵌套映射(在映射中配置type=“nested”)的文档 使用了对象类型的文档两者之间的一些细微区别需要注意假设我们索引如下数据：查询：查询返回结果按照嵌套对象的usert字段最小值降序如果将子文档视为一种数据类型，则可以将查询简化为如下形式：当我们使用对象类型时，可以简化查询，这是因为整个对象结构被当成一个lucene文档进行储存有些时候，使用nested_path属性会更加便捷按照下面的方式构造查询：我们也可以使用nested_filter参数，改参数只对嵌套文档有效，利用这个参数，我们可以在排序前就已经通过一个过滤器在检索期排除了某些文档，而不是检索结果文档集中过滤它们（注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch批量操作]]></title>
      <url>%2Fpost%2Felasticsearch-bulk%2F</url>
      <content type="text"><![CDATA[elasticsearch提供了批量操作来读取数据和检索 批量取可以通过_mget端点操作，一个请求获取多个文档，elasticsearch会返回哪些被索引的文档，而不论这些文档可用于搜索还是暂时对查询不可见查看下面的操作：elasticsearch返回如下形式的文档：前面的范例还可以写成如下的更紧凑的形式：这种形式便于获取不同文档有不同目标索引及类型我们来看下面这个查询：该查询返回了id为1和3的两个文档，但是第一个文档从索引library_backup中获取，第二个文档则从索引library中获取（因为url中定义索引名为library，因此将它作为默认值），而且第一个文档我们限制只返回otitle字段 批量查询允许用户将多个查询请求打包到一组，elasticsearch将输入解析成一行一行的文本，而文本行包含了目标索引、其他参数以及查询串等信息范例：查询请求被发送到_msearch端点，请求的偶数行复制携带真正的查询结果：批量查询允许我们将多个独立的查询打包到一个请求中（注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch二次评分]]></title>
      <url>%2Fpost%2Felasticsearch-rescore%2F</url>
      <content type="text"><![CDATA[二次评分给了用户很多机会来定制业务逻辑 理解二次评分二次评分是指重新计算查询返回文档中指定个数文档的得分，es会截取查询返回的前N个，并使用预定义的二次评分方法来重新计算他们的得分 二次评分查询结构从最简单的查询入手：match_all查询类型，返回索引中所有文档，每个返回的文档的得分都是1.0，这样可以充分体现二次评分对查询返回文档集的影响 查询范例如下： 改查询将每一个文档的得分改写为该文档的year字段中的值 返回结果： 二次评分参数配置在resource对象中，必须配置下面的参数： window_size 窗口大小，默认值是from和size参数值之和，它指定了每个分片上参与二次评分的文档个数 query_weight 查询权重，默认值是1，原始查询得分与二次评分的得分相加之前将乘以改值 rescore_query_weight 二次评分查询的权重值，默认值是1，二次评分查询得分在与原始查询得分相加之前，乘以该值 rescore_mode 二次评分模式，默认为total，可用的选项有total、max、min、avg和mutiply total 得分是两种查询之he max 两种查询中的最大值 min 两种查询中的最小值 avg 两种查询的平均值 multiply 两种查询的乘积 小结 有时候，我们需要显示查询结果，并且使得页面上靠前文档的顺序能受到一些额外的规则控制，但遗憾的是，我们并不能通过二次评分来实现，也许有些读者会想到window-size参数，然而实际上这个参数与返回列表中靠前文档并无关系，他只是制定了每个分片应该返回的文档数，而且window_size不能小于页面大小 二次评分功能并不能与排序一起使用，这是因为排序发生在二次评分之前，所以排序没有考虑后续新计算出来的文档得分 （注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MyBatis二级缓存]]></title>
      <url>%2Fpost%2Fmybatis-cache%2F</url>
      <content type="text"><![CDATA[MyBatis提供一级缓存和二级缓存，其中一级缓存是sqlSession级别的缓存，不同的sqlSession之间的缓存互不影响。二级缓存是Mapper级别的缓存，多个sqlSession操作同一个Mapper，其二级缓存是可以共享的。 MyBatis有多种二级缓存方案可供选择。其中对Memcached的支持较为成熟，现以Memcached为例介绍与spring项目的集成。 使用配置配置pom.xml，添加依赖。123456789&lt;dependencies&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.mybatis.caches&lt;/groupId&gt; &lt;artifactId&gt;mybatis-memcached&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; ...&lt;/dependencies&gt; 全局开关123&lt;settings&gt; &lt;setting name="cacheEnabled" value="true"/&gt;&lt;/settings&gt; 配置需要缓存的mapper.xml二级缓存是Mapper级别的，可以针对具体的mapper.xml开启二级缓存。 123&lt;mapper namespace="org.acme.FooMapper"&gt; &lt;cache type="org.mybatis.caches.memcached.MemcachedCache" /&gt;&lt;/mapper&gt; 开启后，则mapper中所有的select语句默认都将被缓存，此命名空间下所有insert、update、delete语句将会导致空间下的缓存被清空。 可以针对具体某条语句禁用缓存。 123&lt;select id="retObj" resultMap="results" useCache="false" &gt; select id, name, sex from employee &lt;/select&gt; 也可针对某条语句触发刷新动作（默认对应所有插入、更新、删除语句）。 123&lt;select id="retObj" resultMap="results" flushCache="true" &gt; select id, name, sex from employee &lt;/select&gt; 配置参数在memcached.properties中设置，如果缺失则使用缺省设置。以下是配置参数： 如果需要记录cache操作日志，可由如下配置实现1234&lt;mapper namespace="org.acme.FooMapper"&gt; &lt;cache type="org.mybatis.caches.memcached.LoggingMemcachedCache" /&gt; ...&lt;/mapper&gt; 注意事项缓存在Memcached中的类需要实现Serializable接口，否则会报错java.io.NotSerializableException。 参考文档MyBatis 缓存机制深度解剖 / 自定义二级缓存 MyBatis的二级缓存的设计原理 Caching over MyBatis : Memcached Performance Tuning Over MyBatis]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis进阶内容]]></title>
      <url>%2Fpost%2Fredis-progression%2F</url>
      <content type="text"><![CDATA[减低内存占用 扩展Redis （注：内容整理自《redis实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis数据安全与性能保障]]></title>
      <url>%2Fpost%2Fredis-data-safety%2F</url>
      <content type="text"><![CDATA[持久化选项 复制 处理系统故障 redis事务 非事务型流水线可以接受多个参数的添加命令和更新命令，比如：MGET,MSET,HMGET,HMSET,RPUSH,LPUSH,SADD,ZADD,这些命令简化了那些需要重复执行相同命令的操作，而极大的提升了性能 性能方面注意事项可以用性能测试程序redis-benchmark来测试 （注：内容整理自《redis实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis命令]]></title>
      <url>%2Fpost%2Fredis-command%2F</url>
      <content type="text"><![CDATA[字符串 列表 集合 散列 有序集合 字符串字符串可以存储：字节串（btye string），整数，浮点数自增自减命令INCR key-name 将键储存的值加上1 DECR key-name 将储存的值减去1 INCRBY key-name amount 将键储存的值加上整数amount DECRBY key-name amount 将键储存的值减去整数amount INCRBYFLOAT key-name amount 将键储存的值加上浮点数amount，但这个命令在redis2.6或者以上的版本可用 redis还拥有对字符串其中一部分内容进行读取或者写入的操作 列表RPUSH key-name value[value...]–将一个或者多个值推入列表的右端，（LPUSH 是推入左端） RPOP key-name 移除并返回列表最右端的元素（LPOP 是左边） LINDEX key-name offset 返回列表中的偏移量为offset的元素 LRANGE key-name start end 返回列表从start偏移量到end偏移量范围内的所有元素 LTRIM key-name start end 对列表进行修剪，只保存从start到end范围的元素。start和end也会被保留 集合SADD SADD key-name item[item...] 将一个或多个元素添加到集合里面，并返回被添加元素当中原本并不存在与集合里面的元素数量 SREM key-name item[item...] 从集合里面移除一或多个元素，并返回被移除元素的数量 SISMEMBER SISMEMBER key-name item[item…]检查元素item是否存在于集合key-name里 SCARD SCARD key-name 返回集合包含的元素的数量 SMEMBERS SMEMBERkey-name 返回集合包含的所有元素 SRANDMEMBER SRANDMEMBER key-name[count]-从集合里面随机地返回一个或多个元素，当count为正数时，命令返回的元素不会重复，当为负数是，命令返回的元素可能会重复 SPOPSPOP key-name 随机地移除集合中的一个元素，并返回被移除的元素 SMOVE SMOVE source-key dest-key item 如果集合soucre-key包含元素item，那么从集合source-key里面移除元素item。并将元素item添加到集合dest-key中，如果item被成功移除，返回1.否则返回0 散列HMGET key-name key[key...] 从散列里面获取一个或对个键的值 HMSET key-name key value [key value..] 为散列里面的一个或多个键设置值 HDEL key-name key[key …]删除散列里面的一个或多个键值对，返回成功找到并删除键值对数量 HLEN key-name返回散列包含的键值对的数量 进阶：HEXISTS key-name key 检查给定键是否存在于散列中 HKEYS key-name获取散列包含的所有键 HVALS key-name获取散列包含的所有值 HGETALL key-name获取散列包含的所有键值对 HINCRBY key-name key increment 将键存储的值加上整数increment HINCRBYFLOAT key-name key increment 将键key存储的值加上浮点数increment 有序集合 发布和订阅SUBSCRIBEUNSUBSCRIBEPUBLISHPSUBSCRIBEPUNSUBSCRIBE 其他命令SORTMULTIEXECTTLEXPIRE （注：内容整理自《redis实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch学习笔记]]></title>
      <url>%2Fpost%2Felasticsearch-study-note%2F</url>
      <content type="text"><![CDATA[Elasticsearch是一个可伸缩的开源全文搜索和分析引擎，它使你可以快速且接近实时的去保存，查询和分析海量的数据，他的潜在应用场景是作为一些有复杂搜索功能和需求的应用的搜索引擎 简介Elasticsearch是一个基于Apache Lucene(TM)的开源搜索引擎。无论在开源还是专有领域，Lucene可以被认为是迄今为止最先进、性能最好的、功能最全的搜索引擎库。但是，Lucene只是一个库。想要使用它，你必须使用Java来作为开发语言并将其直接集成到你的应用中，更糟糕的是，Lucene非常复杂，你需要深入了解检索的相关知识来理解它是如何工作的。Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。 与关系型数据库对比Relational DB -&gt; Databases -&gt; Tables -&gt; Rows -&gt; ColumnsElasticsearch -&gt; Indices -&gt; Types -&gt; Documents -&gt; Fields Elasticsearch集群可以包含多个索引(indices)（数据库），每一个索引可以包含多个类型(types)（表），每一个类型包含多个文档(documents)（行），然后每个文档包含多个字段(Fields)（列） 基础概念near realtime（NRT）es是一个接近实时的搜索平台，这意味着你查询一个文档的时候有一个延时。大约一秒 cluster集群是一个或多个节点（服务器）的集合在一起，保存所有的数据，联合所有节点一起提供查询能力。一个集群有一个唯一的名字，默认是“elasticsearch”,集群名很重要，因为集群节点加入集群的唯一方式是根据这个名字。 node节点的默认名字是漫威的一个角色，默认加入集群elasticsearch index索引是一系列具有相似特点文档的集合实际上，索引只是一个用来指向一个或多个分片(shards)的“逻辑命名空间(logical namespace) 「索引」含义的区分你可能已经注意到索引(index)这个词在Elasticsearch中有着不同的含义，所以有必要在此做一下区分:索引（名词） ：如上文所述，一个索引(index)就像是传统关系数据库中的数据库，它是相关文档存储的地方，index的复数是indices 或indexes。索引（动词） ：「索引一个文档」表示把一个文档存储到索引（名词）里，以便它可以被检索或者查询。这很像SQL中的INSERT关键字，差别是，如果文档已经存在，新的文档将覆盖旧的文档。 倒排索引 传统数据库为特定列增加一个索引，例如B-Tree索引来加速检索。Elasticsearch和Lucene使用一种叫做倒排索引(inverted index)的数据结构来达到相同目的。 Type索引中，类型是一种逻辑的分类，它的意义由使用者来赋予 mapping 每个类型(type)都有自己的映射(mapping)或者结构定义，就像传统数据库表中的列一样。所有类型下的文档被存储在同一个索引下，但是类型的映射(mapping)会告诉Elasticsearch不同的文档如何被索引Elasticsearch支持以下简单字段类型：类型 表示的数据类型String stringWhole number byte, short, integer, longFloating point float, doubleBoolean booleanDate date document文档是搜索信息的基本单元，用json表达，文档必须被包含于一个type中 文档 ID文档唯一标识由四个元数据字段组成： _id：文档的字符串 ID _type：文档的类型名 _index：文档所在的索引 _uid：_type 和 _id 连接成的 type#id 默认情况下，_uid 是被保存（可取回）和索引（可搜索）的。_type 字段被索引但是没有保存，_id 和_index 字段则既没有索引也没有储存，它们并不是真实存在的。 shards&amp;replicases提供能力，让你把index分成好几个部分，叫做分片，当你创建索引的时候，你可以简单的定义分片的个数，每个分片本身是一个独立的功能齐全的“索引”，可以被放到任何的集群节点中 分片的意义：1.可以水平分割和扩展数据2.可以把操作分配给多个分区，提高性能 es允许你制作一个或多个分片的副本。叫做复制分片复制分片的意义：1、他提供了高可用性，副本和原始分区不处于一个节点中。2.他提高了性能，因为搜索可以在任何分区上允许。每一个分片是一个lucene索引，每个Lucene实例有一个最大的存放文档的数量。这个数量是2417483519 analysis分析也称分词Elasticsearch中的数据可以大致分为两种类型：确切值 及 全文文本。确切值是确定的，正如它的名字一样。比如一个date或用户ID，也可以包含更多的字符串比如username或email地址。全文文本常常被称为非结构化数据，而对于全文数据的查询来说，却有些微妙。我们不会去询问这篇文档是否匹配查询要求？。 但是，我们会询问这篇文档和查询的匹配程度如何？。换句话说，对于查询条件，这篇文档的相关性有多高？ 为了方便在全文文本字段中进行这些类型的查询，Elasticsearch首先对文本分析(analyzes)，然后使用结果建立一个倒排索引。 分析(analysis)机制用于进行全文文本(Full Text)的分词，以建立供搜索用的反向索引。 分析(analysis)是这样一个过程：首先，标记化一个文本块为适用于倒排索引单独的词(term)然后标准化这些词为标准形式，提高它们的“可搜索性”或“查全率” 这个工作是分析器(analyzer)完成的。一个分析器(analyzer)只是一个包装用于将三个功能放到一个包里：字符过滤器1.首先字符串经过字符过滤器(character filter)，它们的工作是在标记化前处理字符串。字符过滤器能够去除HTML标记，或者转换”&amp;”为”and”。分词器2.下一步，分词器(tokenizer)被标记化成独立的词。一个简单的分词器(tokenizer)可以根据空格或逗号将单词分开（译者注：这个在中文中不适用）标记过滤3.最后，每个词都通过所有标记过滤(token filters)，它可以修改词（例如将”Quick”转为小写），去掉词（例如停用词像”a”、”and”、”the”等等），或者增加词（例如同义词像”jump”和”leap”） index参数控制字符串以何种方式被索引。它包含以下三个值当中的一个：analyzed首先分析这个字符串，然后索引。换言之，以全文形式索引此字段。not_analyzed索引这个字段，使之可以被搜索，但是索引内容和指定值一样。不分析此字段。no不索引这个字段。这个字段不能为搜索到。string类型字段默认值是analyzed。如果我们想映射字段为确切值，我们需要设置它为not_analyzed：{ “tag”: { “type”: “string”, “index”: “not_analyzed” }}其他简单类型（long、double、date等等）也接受index参数，但相应的值只能是no和not_analyzed，它们的值不能被分析。 Elasticsearch提供很多开箱即用的字符过滤器，分词器和标记过滤器。这些可以组合来创建自定义的分析器以应对不同的需求。 string类型的字段，默认的，考虑到包含全文本，它们的值在索引前要经过分析器分析，并且在全文搜索此字段前要把查询语句做分析处理。对于string字段，两个最重要的映射参数是index和analyer。 highlight很多应用喜欢从每个搜索结果中高亮(highlight)匹配到的关键字，这样用户可以知道为什么这些文档和查询相匹配。 score每个节点都有一个_score字段，这是相关性得分(relevance score)，它衡量了文档与查询的匹配程度。默认的，返回的结果中关联性最大的文档排在首位；这意味着，它是按照_score降序排列的。这种情况下，我们没有指定任何查询，所以所有文档的相关性是一样的，因此所有结果的_score都是取得一个中间值1max_score指的是所有文档匹配查询中_score的最大值。 refresh默认情况下，每个分片每秒自动刷新一次。这就是为什么说Elasticsearch是近实时的搜索了：文档的改动不会立即被搜索，但是会在一秒内可见。 sort排序默认情况下，结果集会按照相关性进行排序 – 相关性越高，排名越靠前字段值排序:按时间排序 GET /_search { &quot;query&quot; : { &quot;filtered&quot; : { &quot;filter&quot; : { &quot;term&quot; : { &quot;user_id&quot; : 1 }} } }, &quot;sort&quot;: { &quot;date&quot;: { &quot;order&quot;: &quot;desc&quot; }} } 返回： &quot;hits&quot; : { &quot;total&quot; : 6, &quot;max_score&quot; : null, &lt;1&gt; &quot;hits&quot; : [ { &quot;_index&quot; : &quot;us&quot;, &quot;_type&quot; : &quot;tweet&quot;, &quot;_id&quot; : &quot;14&quot;, &quot;_score&quot; : null, &lt;1&gt; &quot;_source&quot; : { &quot;date&quot;: &quot;2014-09-24&quot;, ... }, &quot;sort&quot; : [ 1411516800000 ] &lt;2&gt; }, ... } _score 是比较消耗性能的, 而且通常主要用作排序 – 我们不是用相关性进行排序的时候，就不需要统计其相关性字段值默认以顺序排列（从小到大 ），而 _score 默认以倒序排列。 缓存过滤器是怎么计算的。它们的核心是一个字节集来表示哪些文档符合这个过滤器。Elasticsearch 主动缓存了这些字节集留作以后使用。一旦缓存后，当遇到相同的过滤时，这些字节集就可以被重用，而不需要重新运算整个过滤。集很“聪明”：他们会增量更新。你索引中添加了新的文档，只有这些新文档需要被添加到已存的字节集中，而不是一遍遍重新计算整个缓存的过滤器。过滤器和整个系统的其他部分一样是实时的，你不需要关心缓存的过期时间。 安装 首先需要依赖java7以上版本 curl -L -O https://download.elastic.co/elasticsearch/release/org/elasticsearch/distribution/tar/elasticsearch/2.3.3/elasticsearch-2.3.3.tar.gztar -xvf elasticsearch-2.3.3.tar.gzcd elasticsearch-2.3.3/bin./elasticsearch 可以在启动的时候重写集群和节点的名字./elasticsearch –cluster.name my_cluster_name –node.name my_node_name 默认，es使用9200端口提供 restapi访问 安装配置：config/elasticsearch.yml network : host : 10.0.0.4 path: logs: /var/log/elasticsearch data: /var/data/elasticsearch cluster: name: &lt;NAME OF YOUR CLUSTER&gt; node: name: &lt;NAME OF YOUR NODE&gt; 浏览你的集群es提供了丰富的restapi用于和集群直接通信包括：1.检查集群，节点，索引，状态和统计2.管理集群，节点，索引数据和与元数据3.对索引curd4.使用高级搜索功能，比如分页，排序，过滤，脚本，聚合和其他很多 集群健康：curl &apos;localhost:9200/_cat/health?v&apos; 返回epoch timestamp cluster status node.total node.data shards pri relo init unassign1394735289 14:28:09 elasticsearch green 1 1 0 0 0 0 0 颜色 意义green 所有主要分片和复制分片都可用yellow 所有主要分片可用，但不是所有复制分片都可用red 不是所有的主要分片都可用 列举出所有索引：curl&apos;localhost:9200/_cat/indices?v&apos; 创建一个索引：curl -XPUT &apos;localhost:9200/customer?pretty&apos; 创建一个文档：curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos;-d &apos;{ &quot;name&quot;: &quot;John Doe&quot;}&apos; 删除一个索引：curl -XDELETE &apos;localhost:9200/customer?pretty&apos; 修改你的数据替代你的文档： curl -XPUT ‘localhost:9200/customer/external/1?pretty’-d ‘ { “name”: “John Doe” }’ curl -XPUT &apos;localhost:9200/customer/external/1?pretty&apos;-d &apos; { &quot;name&quot;: &quot;Jane Doe&quot; }&apos; 如果指定了id，去创建，之前的那个会被覆盖如果没有指定id，es会随机的生成一个id 更新文档：es并不会真正的更新文档，当我们进行更新操作的时候，es删除原来的文档，然后添加一个新的文档。 更新文档也可以使用脚本，动态脚本在1.4.3版本默认被禁用curl -XPOST ‘localhost:9200/customer/external/1/_update?pretty’-d ‘{ “script” : “ctx._source.age += 5” }’ ctx._source指向要被修改的文档 es后续将会提供能力，类似sql中的 UPDATE-WHERE statement 删除文档curl -XDELETE &apos;localhost:9200/customer/external/2?pretty&apos; delete-by-query 插件可以删除满足要求的一类文档 批量操作：curl -XPOST &apos;localhost:9200/customer/external/_bulk?pretty&apos; -d &apos; {&quot;index&quot;:{&quot;_id&quot;:&quot;1&quot;}} {&quot;name&quot;: &quot;John Doe&quot; } {&quot;index&quot;:{&quot;_id&quot;:&quot;2&quot;}} {&quot;name&quot;: &quot;Jane Doe&quot; } &apos; curl -XPOST &apos;localhost:9200/customer/external/_bulk?pretty&apos; -d &apos; {&quot;update&quot;:{&quot;_id&quot;:&quot;1&quot;}} {&quot;doc&quot;: { &quot;name&quot;: &quot;John Doe becomes Jane Doe&quot; } } {&quot;delete&quot;:{&quot;_id&quot;:&quot;2&quot;}} &apos; 浏览你的数据查询APItook: es搜索使用了多少毫秒timed_out 是否超时_shards 告诉我们多少个分片被搜索，和被成功和失败搜索的分片的数量hits 搜索结果hits.total 结果数量hits.hits 搜索结果的列表（默认给出前10个）_score 评分 查询语句：Query DSL {&quot;query&quot;:{&quot;match_all&quot;:{}}} 规定数目curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;size&quot;: 1 }&apos; 分页curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;from&quot;: 10, &quot;size&quot;: 10 }&apos; 规定返回指定的fieldcurl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;match_all&quot;: {} }, &quot;_source&quot;: [&quot;account_number&quot;, &quot;balance&quot;] }&apos; match:curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;match&quot;: { &quot;account_number&quot;: 20 } } }&apos; bool: 加入布尔逻辑 curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;lane&quot; } } ] } } }&apos; curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;should&quot;: [ { &quot;match&quot;: { &quot;address&quot;: &quot;mill&quot; } }, { &quot;match&quot;: { &quot;address&quot;: &quot;lane&quot; } } ] } } }&apos; curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: [ { &quot;match&quot;: { &quot;age&quot;: &quot;40&quot; } } ], &quot;must_not&quot;: [ { &quot;match&quot;: { &quot;state&quot;: &quot;ID&quot; } } ] } } }&apos; filter使用filter，es不再计算相关性得分，只是严格的按照条件过滤比如：range curl -XPOST &apos;localhost:9200/bank/_search?pretty&apos;-d &apos; { &quot;query&quot;: { &quot;bool&quot;: { &quot;must&quot;: { &quot;match_all&quot;: {} }, &quot;filter&quot;: { &quot;range&quot;: { &quot;balance&quot;: { &quot;gte&quot;: 20000, &quot;lte&quot;: 30000 } } } } } }&apos; Aggregations提供能力是分组和提炼你的数据 就像sql里面的GROUP BYes里你可以查询返回hit和hit的聚合，在一次查询中而且可以进行多重聚合 安装state分类，然后返回10个状态，按照数量排序设置size=0，不展示hits，因为我们只关心聚合结果 SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC 平局值 聚合每一种balance的平均值 suggestlasticsearch 0.9.0.3终于基于AnalyzingSuggester加上了prefix suggestions ，可直接做搜索提示功能，在0.9.0.1之前版本都是使用外部插件实现的参考文章http://www.nosqldb.cn/1376024289369.htmlhttp://www.cnblogs.com/jiuyuehe/p/3840821.html 总结es是一个简单又复杂的产品，还有很多高级的功能。 拓展官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html elasticSearch中文社区：http://elasticsearch.cn/ elasticsearch 索引优化:http://itindex.net/detail/52316-elasticsearch-%E7%B4%A2%E5%BC%95-%E4%BC%98%E5%8C%96 与其他相似功能产品对比：http://www.cnblogs.com/chowmin/articles/4629220.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch自动补齐建议-completion suggester]]></title>
      <url>%2Fpost%2Felasticsearch-completion-suggester%2F</url>
      <content type="text"><![CDATA[1.mappingcurl -XPUT 192.168.0.1:9200/person -d&apos; //新建一个persion的索引 { &quot;mappings&quot;: { &quot;person&quot;: { //这个是_type &quot;properties&quot;: { &quot;name&quot;: { &quot;type&quot;: &quot;string&quot; } &quot;tag_suggest&quot;: { &quot;type&quot;: &quot;completion&quot;, //设置为completion才能被suggest捕获 &quot;index_analyzer&quot;: &quot;ik&quot;, &quot;search_analyzer&quot;: &quot;ik&quot;, &quot;payloads&quot;: false } } } } }&apos; 2 .添加测试数据curl -XPUT 192.168.2.20:9200/person/person/1 -d&apos; { &quot;name&quot;: [ &quot;david&quot;, &quot;jacky&quot; ], &quot;tag_suggest&quot;: { &quot;input&quot;: [ &quot;david&quot;, &quot;jacky&quot; ] } }&apos; curl -XPUT 192.168.0.1:9200/person/person/1 -d&apos; { &quot;name&quot;: [ &quot;andy&quot;, &quot;jackson&quot; ], &quot;tag_suggest&quot;: { &quot;input&quot;: [ &quot;andy&quot;, &quot;jackson&quot; ] } }&apos; 3.DSLcurl -XPOST 192.168.0.1:9200/person/_suggest -d&apos; { &quot;person_suggest&quot;:{ &quot;text&quot;:&quot;jack&quot;, &quot;completion&quot;: { &quot;field&quot; : &quot;tag_suggest&quot; } } }&apos; 4.结果{ &quot;_shards&quot;: { &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;failed&quot;: 0 }, &quot;person_suggest&quot;: [ { &quot;text&quot;: &quot;word&quot;, &quot;offset&quot;: 0, &quot;length&quot;: 4, &quot;options&quot;: [ { &quot;text&quot;: &quot;jacky&quot;, &quot;score&quot;: 1 }, { &quot;text&quot;: &quot;jackson&quot;, &quot;score&quot;: 1 } ] } ] } 5.代码123456789101112131415161718192021 CompletionSuggestionBuilder completionSuggestionBuilder = new CompletionSuggestionBuilder("complete"); completionSuggestionBuilder.text(paramMap.get("text")); completionSuggestionBuilder.field(paramMap.get("field")); completionSuggestionBuilder.size(10); IElasticsearchClient client = index.getIndexClient(); CompletionSuggestionBuilder completionSuggestion = completionSuggestionBuilder SuggestResponse resp = client.prepareSuggest(realIndexName) .addSuggestion(completionSuggestion).execute().actionGet(); List&lt;? extends Suggest.Suggestion.Entry&lt;? extends Suggest.Suggestion.Entry.Option&gt;&gt; list = response.getSuggest().getSuggestion("complete").getEntries(); List&lt;String&gt; suggestList = new ArrayList&lt;String&gt;(); if (list == null) &#123; return null; &#125; else &#123; for (Suggest.Suggestion.Entry&lt;? extends Suggest.Suggestion.Entry.Option&gt; e : list)&#123; for (Suggest.Suggestion.Entry.Option option : e) &#123; suggestList.add(option.getText().toString()); &#125; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch地理位置查询-Geo Distance Range Query]]></title>
      <url>%2Fpost%2Felasticsearch-geo-distance-query%2F</url>
      <content type="text"><![CDATA[官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/2.4/query-dsl-geo-distance-query.html#_geohash_3 索引mapping定义:索引中定义一个字段pin，添加一个属性location，type为geo_point &quot;pin&quot; : { &quot;properties&quot; : { &quot;location&quot; : { &quot;type&quot; : &quot;geo_point&quot; } } } DSL:报文中的包含一个match all的query , filter中的distance指定了距离范围，pin.location是经纬度 { &quot;bool&quot; : { &quot;must&quot; : { &quot;match_all&quot; : {} }, &quot;filter&quot; : { &quot;geo_distance&quot; : { &quot;distance&quot; : &quot;200km&quot;, &quot;pin.location&quot; : { &quot;lat&quot; : 40, &quot;lon&quot; : -70 } } } } } 代码：拼装 query 和 sort1234567891011121314QueryBuilder builder = new GeoDistanceRangeQueryBuilder （"pin.location"） .point(lat,lon) .from("0km") .to("10000km") .includeLower(true) .includeUpper(false) .optimizeBbox("memory") .geoDistance(GeoDistance.ARC); GeoDistanceSortBuilder sort = SortBuilders.geoDistanceSort("location"); GeoDistanceSortBuilder sort = new GeoDistanceSortBuilder("location"); sort.unit(DistanceUnit.KILOMETERS); sort.order(SortOrder.ASC); sort.point(lat,lon); 构造dsl生产器12345 SearchSourceBuilder sourceBuilder = SearchSourceBuilder.searchSource(); sourceBuilder.query(QueryBuilders.boolQuery()) .must(builder) sourceBuilder.sort(sort); 调用elasticsearch123456789101112131415final SearchResponse response = executeGet(new ClientCallback&lt;SearchResponse&gt;() &#123; @Override public ActionFuture&lt;SearchResponse&gt; execute(final Client client) &#123; final SearchSourceBuilder sourceBuilder = SearchParamUtils .genSearchSourceBuilderFromSearchParam(searchParam); String[] indexNames = new String[aliasIndexNameList.size()]; SearchRequest request = Requests.searchRequest(aliasIndexNameList.toArray(indexNames)) .types(type); request.source(sourceBuilder.toString()); if (searchParam.getSearchType() != null) &#123; request.searchType(searchParam.getSearchType()); &#125; return client.search(request); &#125; &#125;); 获取每条记录的距离12345678910SearchResult searchResult = new SearchResult(); SearchHits hits = response.getHits(); for (SearchHit hit : hits.getHits()) &#123; Object[] sortArray = hit.getSortValues(); if(sortArray!=null&amp;&amp;sortArray.length&gt;0)&#123; BigDecimal geoDis = new BigDecimal((Double) sortArray[sortArray.length-1]); map.put("geoDistance", geoDis.setScale(0, BigDecimal.ROUND_HALF_DOWN)); System.out.println("距离" + hit.getSource().get("geoDistance")); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch如何评分?-Apache Lucene默认评分公式解释]]></title>
      <url>%2Fpost%2Felasticsearch-lucene-score-rule%2F</url>
      <content type="text"><![CDATA[lucene默认评分机制：TF/IDF(词频/逆文档频率)算法默认评分公式解释何时文档被匹配上当一个文档经过lucene返回，则意味着该文档与用户提交的查询时匹配的，在这种情况下，每个返回的文档都会有一个得分，得分越高，文档相关度更高，但是，同一个文档针对不同查询的得分是不同的，比较某个文档在不同查询中的分数是没有意义的，这是因为文档得分依赖多个因子，除了权重和查询本身的结构，还包括匹配的词项的数目，词项所在字段，以及用于查询规范化的匹配类型等。 计算文档得分需要考虑以下因子 文档权重（document boost）：索引期赋予某个文档的权重值 字段权重（field boost）:查询期赋予某个字段的权重值 协调因子（coord）：基于文档中词项命中个数的协调因子，一个文档中命中了查询中的词项越多，得分越高(比如：查询关键词被分词为A和B,如果文档1命中了A和B,文档2命中了A,那么在这个项目上，文档1的分数更高) 逆文档频率(inverse document frequency):一个基于词项的因子,用来告诉评分公式该词项有多么罕见，逆文档频率越低，词项越罕见，评分公式利用该因子为包含罕见词项的文档加权(比如：查询关键词是A和B,如果文档1命中了A,文档2命中了B,但是在整个文档范围内，A出现的次数比B少，那么在这个项目中，文档1分数更高) 长度范数(length norm):每个字段的基于词项个数的归一化因子，一个字段包含的词项越多，改因子的权重越低，*这意味着lucene评分公式更”喜欢”包含更少词项的字段(比如：查询关键词是A,文档1和2都匹配上了A,但是文档1内容长度比文档1短，那么在这个项目中，文档1分数更高) 词频：一个基于词项的因子，用来表示一个词项在某个文档中出现多少次，词频越高，文档得分越高(比如：查询关键词是A，文档1和文档1都匹配上了，但是文档1中出现了2次A,文档2中出现了1次A,那么在这个项目中，文档1分数更高) 查询范数（query norm）：一个基于查询的归一化因子，它等于查询中词项的权重平方和，查询范数使得不同查询的得分能相互比较，尽管这种比较通常是困难且不可行的 TF/IDF评分公式Lucene理论评分公式注意，你并不需要深入理解这个公式的来龙去脉，了解它的工作原理非常重要上面的公式理论形式糅合了布尔检索模型和向量空间检索模型，我们可以不讨论这个理论评分公式，直接跳到lucene实际评分公式 Lucene实际评分公式现在让我来看看Lucene实际评分公式： 解释：这是一个关于查询q和文档d的函数，有两个因子coord和queryNorm并不直接依赖查询词项，而是与查询词项的一个求和公式相乘，求和公式中的每个加数由以下因子连乘所得：词频 逆文档频率 词项权重 长度范数 由这个公式我们可以导出一些规则： 越多罕见的词项被匹配上，文档分数越高 文档字段越短，文档分数越高 权重越高（无论是索引期还查询期赋予的权重值），文档得分越高 elasticsearch如何看评分elasticsearch使用了lucene的评分功能，但是好在我们可以替换默认的评分算法，elasticsearch使用了Lucene的评分功能但是不仅限于lucene的评分功能，用户可以使用各种不同的查询类型以精确控制文档的评分计算，如custom_boost_factor查询、constant_score查询，custom_score查询）还可以通过使用脚本（scrpting）来改变文档得分，还可以使用二次评分功能，通过在返回文档集合之上执行另外一个查询，重新计算前N个文档得分 （注：内容整理自《深入理解elasticsearch》，斜体为本人添加的理解）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java IO 简介]]></title>
      <url>%2Fpost%2Fjava-io-introduction%2F</url>
      <content type="text"><![CDATA[学习Java中的IO，首先要理解Java中IO的流模型。所谓流，可以假想成河流，流的数据源，就是河流的发源地，流是单向的，流的单向性，就像河流的水流是单向的一样。 Java中的流可以从两方面的分类 输入流和输出流 节点流和处理流 输入流和输出流，就是程序和外部的数据源进行IO操作。这些数据源可以是可以是内存，文件，还可以是网络上的一个URL。 输入流和输出流的定义都是相对程序来说的，也就是输入流是从外部读取数据进入程序，然后由程序处理。输出流是从程序中输出的数据。 节点流和处理流。节点流是直接跟数据源连接的流，而处理流是用来装饰节点流的，是为使节点流有更多的功能。 io流 输入流 输出流 字节流 InputStream OutputStream 字符流 Reader Writer 对于上面的字节流和字符流，他们的区别就在于字节流会以字节的形式来处理数据，而字符流会以字符的形式来处理数据。 对于字节流的输入流，就是 InputStream的抽象类向下延伸。因为数据源可以在在文件中，在内存中。所以，一般的字节输入流有 FileInputStream、ByteArrayInputStream 等方法。因此，对应的，就有字节输出流的 FileOutputStream、ByteArrayOutputStream。 而对于字符流的输入流，就是 Reader的抽象类的向下延伸。同样的，数据源可以在文件中，在内存中。因此，一般的字符输入流有 FileReader、CharArrayReader等方法。输出流也相对应。 上面了解了节点流。下面是处理流。 所谓处理流，就是不直接连接到数据源，而是对连接数据流的节点流进行装饰，使得流能提供更多更好的功能。 常见的处理流有 缓冲流。BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream。这种流是就是为增加了缓冲功能，提高的IO效率。 字节流转为字符流。 InputStreamReader，InputStreamWriter，当然，可以在创建此类的时候设置字符编码。 对象序列化流。ObjectInputStream、ObjectOutputStream。 各种类型数据的输入输出。DataInputStream、DataOutputStream。 行流。LineNumberReader；LineNumberInputStream 打印流。PrintWriter；PrintOutputStream。 再次理解一下流的概念流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 字符流和字节流字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流对输入流只能进行读操作，对输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 Java IO流对象 输入字节流InputStreamIO 中输入字节流的继承图可见下图，可以看出： InputStream 是所有的输入字节流的父类，它是一个抽象类。 ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。 ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 输入字节流InputStreamIO 中输入字节流的继承图可见下图，可以看出： OutputStream 是所有的输出字节流的父类，它是一个抽象类。 ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。PipedOutputStream 是向与其它线程共用的管道中写入数据， ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 字节流的输入与输出的对应 图中蓝色的为主要的对应部分，红色的部分就是不对应部分。紫色的虚线部分代表这些流一般要搭配使用。从上面的图中可以看出Java IO 中的字节流是极其对称的。“存在及合理”我们看看这些字节流中不太对称的几个类吧！ LineNumberInputStream 主要完成从流中读取数据时，会得到相应的行号，至于什么时候分行、在哪里分行是由改类主动确定的，并不是在原始中有这样一个行号。在输出部分没有对应的部分，我们完全可以自己建立一个LineNumberOutputStream，在最初写入时会有一个基准的行号，以后每次遇到换行时会在下一行添加一个行号，看起来也是可以的。好像更不入流了。 PushbackInputStream 的功能是查看最后一个字节，不满意就放入缓冲区。主要用在编译器的语法、词法分析部分。输出部分的BufferedOutputStream 几乎实现相近的功能。 StringBufferInputStream 已经被Deprecated，本身就不应该出现在InputStream 部分，主要因为String 应该属于字符流的范围。已经被废弃了，当然输出部分也没有必要需要它了！还允许它存在只是为了保持版本的向下兼容而已。 SequenceInputStream 可以认为是一个工具类，将两个或者多个输入流当成一个输入流依次读取。完全可以从IO 包中去除，还完全不影响IO 包的结构，却让其更“纯洁”――纯洁的Decorator 模式。 PrintStream 也可以认为是一个辅助工具。主要可以向其他输出流，或者FileInputStream 写入数据，本身内部实现还是带缓冲的。本质上是对其它流的综合运用的一个工具而已。一样可以踢出IO 包！System.out 和System.out 就是PrintStream 的实例！ 字符输入流Reader在上面的继承关系图中可以看出： Reader 是所有的输入字符流的父类，它是一个抽象类。 CharReader、StringReader 是两种基本的介质流，它们分别将Char 数组、String中读取数据。PipedReader 是从与其它线程共用的管道中读取数据。 BufferedReader 很明显就是一个装饰器，它和其子类负责装饰其它Reader 对象。 FilterReader 是所有自定义具体装饰流的父类，其子类PushbackReader 对Reader 对象进行装饰，会增加一个行号。 InputStreamReader 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。FileReader 可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。 字符输出流Writer在上面的关系图中可以看出： Writer 是所有的输出字符流的父类，它是一个抽象类。 CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。PipedWriter 是向与其它线程共用的管道中写入数据， BufferedWriter 是一个装饰器为Writer 提供缓冲功能。 PrintWriter 和PrintStream 极其类似，功能和使用也非常相似。 OutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 字符流的输入与输出的对应 字符流与字节流转换转换流的特点： 其是字符流和字节流之间的桥梁 可对读取到的字节数据经过指定编码转换成字符 可对读取到的字符数据经过指定编码转换成字节 何时使用转换流？ 当字节和字符之间有转换动作时； 流操作的数据需要编码或解码时。 具体的对象体现： InputStreamReader:字节到字符的桥梁 OutputStreamWriter:字符到字节的桥梁 这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来。 File类File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 RandomAccessFile类该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点： 该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。 该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw) 注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mybatis-trim标签]]></title>
      <url>%2Fpost%2Fmybatis-where-trim%2F</url>
      <content type="text"><![CDATA[今天遇到一个场景需要写一个这样的查询语句：用户对象userInfo包含下面几个字段：userName phone email qqId weiboId wxId 现在新注册用户，传过来一个注册userInfo对象，现在要到数据库中验证状态status=1 （表示激活的用户）的用户中，是否存在一个用户，只要它这些字段中至少有一个与新注册的对象对应的字段内容相同，那就说明重复注册。 翻译成sql语句表示一下的意思大概就是：select * from tablename where(userName=”xxx”or phone =”xxx”or …)and status=1 一开始我是这样写的，在mybatis中的代码就是这样：1234567891011121314151617181920212223242526&lt;select id="selectBySelective" resultType="xxx.UserInfo"&gt; select &lt;include refid="Base_Column_List" /&gt; from uc_user &lt;where&gt; (&lt;if test="userName != null" &gt; user_name = #&#123;userName&#125; &lt;/if&gt; &lt;if test="email != null" &gt; or email = #&#123;email&#125; &lt;/if&gt; &lt;if test="phone != null" &gt; or phone = #&#123;phone&#125; &lt;/if&gt; &lt;if test="weiboId != null" &gt; or weibo_id = #&#123;weiboId&#125; &lt;/if&gt; &lt;if test="wxId != null" &gt; or wx_id = #&#123;wxId&#125; &lt;/if&gt; &lt;if test="qqId != null" &gt; or qq_id = #&#123;qqId&#125; &lt;/if&gt;) &lt;/where&gt; and status = 1&lt;/select&gt; 这样代码看似没有什么问题但是其实是有问题的。为什么呢？如果userName 为空，后面某字段不为空，最后的sql语言会成为这样：1select * from uc_user where(or email = "xxx") and status = 1 使用mybatis &lt; where &gt; 标签就是为了防止这种情况，mybatis会在第一个userName 为空的情况下，帮我们去掉后面的语句的第一个”or” 但是我加了where标签中加入（）后，语句会报错。因为自动去掉”or”会失效。 查看了mybatis官方文档发现了另一个标签 &lt; trim &gt;可以通过自定义 trim 元素来定制我们想要的功能 trim标签包围的内容可以设置几个属性：prefix ：内容之前加的前缀suffix ：内容之后加的后缀prefixOverrides： 属性会忽略通过管道分隔的文本序列（注意此例中的空格也是必要的，多个忽略序列用“|”隔开）。它带来的结果就是所有在 prefixOverrides 属性中指定的内容将被移除。 所以我修改后的代码是：1234567891011121314151617181920212223242526&lt;select id="selectBySelective" resultType="xxx.UserInfo"&gt; select &lt;include refid="Base_Column_List" /&gt; from uc_user &lt;trim prefix="WHERE (" suffix=")" prefixOverrides="AND |OR "&gt; &lt;if test="userName != null" &gt; user_name = #&#123;userName&#125; &lt;/if&gt; &lt;if test="email != null" &gt; or email = #&#123;email&#125; &lt;/if&gt; &lt;if test="phone != null" &gt; or phone = #&#123;phone&#125; &lt;/if&gt; &lt;if test="weiboId != null" &gt; or weibo_id = #&#123;weiboId&#125; &lt;/if&gt; &lt;if test="wxId != null" &gt; or wx_id = #&#123;wxId&#125; &lt;/if&gt; &lt;if test="qqId != null" &gt; or qq_id = #&#123;qqId&#125; &lt;/if&gt; &lt;/trim&gt; and status = 1&lt;/select&gt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java多线程简介之基本概念、Thread类、Executor]]></title>
      <url>%2Fpost%2Fjava-concurrent01%2F</url>
      <content type="text"><![CDATA[在没有接触并发编程概念之前，你学到的都是有关顺序编程的知识，即程序中的所以事物在任意时刻都只能执行一个步骤并行编程可以使程序执行速度得到极大的提高，或者为设计某些类型的程序提供更易用的模型，或者两者皆有。当并行执行的任务彼此开始产生互相干涉时，实际的并发问题就会接踵而至。这些并发问题，如果视而不见，就会遭到其反噬。因此，使用并发时你得自食其力，并且只有变得多疑而且自信，才能用Java编写出可靠的多线程代码。 并发的多面性用并发解决的问题大体上可以分为“速度”和“设计可管理性”两种 如果程序中的某个任务因为改程序控制范围外的某些条件（通常是I/O）而导致不能继续执行，那么我们就说这个任务或线程阻塞了，如果没有并发，则整个程序都将停止下来，直至外部条件发生变化。但是，如果使用并发来编写程序，那么当一个任务发生阻塞时，程序中的其他任务还可以继续执行。因此这个任务可以保持继续向前执行。 实现并发最直接的方式就是操作系统级别的使用进程。进程是运行在它自己的地址空间内的自包容程序。 因此编写多线程程序最基本的困难在于协调不同线程驱动任务之间对这些资源的使用，以使得这些资源不会被多个任务访问。 某些编程语言被设计为可以将并发任务彼此隔离，这些语言通常为称为函数式语言，其中每个函数调用都不会产生任何副作用（并因此而不能干涉其他函数），并因此可以当作独立的任务驱动。 java的线程机制是抢占式的，这表示调度机会周期性地中断线程，将上下文切换到另一个线程，从而为每个线程都提供时间片，使得每个线程都会分配到数量合适的时间去驱动它的任务。在协作式系统中，每个任务都会自动的放弃控制，这要求程序员要有意识地在每个任务中插入某种类型的让步语句 基本的线程机制一个线程就是在进程中一个单一的顺序控制流，因此，单个进程可以拥有多个并发执行任务 定义任务要想定义一个任务，只需要实现Runnable接口并编写run()方法 例程：12345678910111213141516171819202122//: concurrency/LiftOff.java// Demonstration of the Runnable interface.public class LiftOff implements Runnable &#123; protected int countDown = 10; // Default private static int taskCount = 0; private final int id = taskCount++; public LiftOff() &#123;&#125; public LiftOff(int countDown) &#123; this.countDown = countDown; &#125; public String status() &#123; return "#" + id + "(" + (countDown &gt; 0 ? countDown : "Liftoff!") + "), "; &#125; public void run() &#123; while(countDown-- &gt; 0) &#123; System.out.print(status()); Thread.yield(); &#125; &#125;&#125; ///:~ Thread.yield() 的调用时对线程调度器的一种建议，它在声明：”我已经执行完生命周期中最重要的部分了，此刻正是切换给其他任务执行一段时间的大好时机“ 12345678910//: concurrency/MainThread.javapublic class MainThread &#123; public static void main(String[] args) &#123; LiftOff launch = new LiftOff(); launch.run(); &#125;&#125; /* Output:#0(9), #0(8), #0(7), #0(6), #0(5), #0(4), #0(3), #0(2), #0(1), #0(Liftoff!),*///:~ 当从Runable导出一个类时，它必须具有run()方法，但是这个方法并无特殊之处–它不会产生任何内在的线程能力，要实现线程行为，你必须显示的调用一个任务附着到线程上 Thread类Thread构造器只需要一个Runable对象，调用Thread对象的start()方法为该线程执行必须的初始化操作。 例程：12345678910111213//: concurrency/BasicThreads.java// The most basic use of the Thread class.public class BasicThreads &#123; public static void main(String[] args) &#123; Thread t = new Thread(new LiftOff()); t.start(); System.out.println("Waiting for LiftOff"); &#125;&#125; /* Output: (90% match)Waiting for LiftOff#0(9), #0(8), #0(7), #0(6), #0(5), #0(4), #0(3), #0(2), #0(1), #0(Liftoff!),*///:~ 注意：main()和LiftOff.run()是程序中与其他线程“同时”执行的代码。 使用Thread与使用普通实现Runable接口对象的区别？ 在使用普通对象时，这对于垃圾回收来说是一场公平的游戏，但是使用Thread时，情况就不同了，每个Thread都“注册”了它自己，因此确实有一个对它的引用。而且在它的任务退出其run()并死亡之前，垃圾回收期无法清除它。因此，一个线程会创建一个单独的执行线程，在对start()的调用完成之后。它仍然会继续存在 使用Executor在Java SE5的Java.util.concurrent包中的执行器（Executor）将为你管理Thread对象，Executor在客户端和任务执行之间提供了一个间接层Executor在Java SE 5/6 中是启动任务的优选方法。 例程：12345678910111213//: concurrency/CachedThreadPool.javaimport java.util.concurrent.*;public class CachedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for(int i = 0; i &lt; 5; i++) exec.execute(new LiftOff()); exec.shutdown(); &#125;&#125; /* Output: (Sample)#0(9), #0(8), #1(9), #2(9), #3(9), #4(9), #0(7), #1(8), #2(8), #3(8), #4(8), #0(6), #1(7), #2(7), #3(7), #4(7), #0(5), #1(6), #2(6), #3(6), #4(6), #0(4), #1(5), #2(5), #3(5), #4(5), #0(3), #1(4), #2(4), #3(4), #4(4), #0(2), #1(3), #2(3), #3(3), #4(3), #0(1), #1(2), #2(2), #3(2), #4(2), #0(Liftoff!), #1(1), #2(1), #3(1), #4(1), #1(Liftoff!), #2(Liftoff!), #3(Liftoff!), #4(Liftoff!),*///:~ 注意：ExecutorService对象是使用静态的Executor方法创建的，这个方法可以确定Executor的类型，类型有CachedThreadPool，FixedThreadPool，SingleThreadPool。 shutdown()方法的调用可以防止新任务被提交给这个Executor，当前线程将继续执行在shutdown()被调用之前提交的所以任务，这个程序将在Executor中的所有任务完成之后尽快退出。 FixedThreadPool使用了有限的线程集来执行所提交的任务。例程：1234567891011121314//: concurrency/FixedThreadPool.javaimport java.util.concurrent.*;public class FixedThreadPool &#123; public static void main(String[] args) &#123; // Constructor argument is number of threads: ExecutorService exec = Executors.newFixedThreadPool(5); for(int i = 0; i &lt; 5; i++) exec.execute(new LiftOff()); exec.shutdown(); &#125;&#125; /* Output: (Sample)#0(9), #0(8), #1(9), #2(9), #3(9), #4(9), #0(7), #1(8), #2(8), #3(8), #4(8), #0(6), #1(7), #2(7), #3(7), #4(7), #0(5), #1(6), #2(6), #3(6), #4(6), #0(4), #1(5), #2(5), #3(5), #4(5), #0(3), #1(4), #2(4), #3(4), #4(4), #0(2), #1(3), #2(3), #3(3), #4(3), #0(1), #1(2), #2(2), #3(2), #4(2), #0(Liftoff!), #1(1), #2(1), #3(1), #4(1), #1(Liftoff!), #2(Liftoff!), #3(Liftoff!), #4(Liftoff!),*///:~ 有了FixedThreadPool在事件驱动的系统中，需要线程的事件处理器，通过直接从池中获取线程，也可以如你所愿地尽快的得到服务。你不会滥用可获得的资源。 SingleThreadPool就像是数量为1的FixedThreadPool向SingleThreadPool提交多个任务，那么这些任务将排队，每个任务都会在下一个任务开始之前结束。所有的任务将使用相同的线程。 从任务产生返回值Runnable不返回任何值，如果希望任务在完成时能够返回一个值。那么可以实现Callable接口,在Java SE5中引入的Callable是一种具有参数类型的泛型，它从方法call()中返回值，必须使用ExecutorService.submit()方法调用它。 例程：12345678910111213141516171819202122232425262728293031323334353637383940414243444546//: concurrency/CallableDemo.javaimport java.util.concurrent.*;import java.util.*;class TaskWithResult implements Callable&lt;String&gt; &#123; private int id; public TaskWithResult(int id) &#123; this.id = id; &#125; public String call() &#123; return "result of TaskWithResult " + id; &#125;&#125;public class CallableDemo &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); ArrayList&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;Future&lt;String&gt;&gt;(); for(int i = 0; i &lt; 10; i++) results.add(exec.submit(new TaskWithResult(i))); for(Future&lt;String&gt; fs : results) try &#123; // get() blocks until completion: System.out.println(fs.get()); &#125; catch(InterruptedException e) &#123; System.out.println(e); return; &#125; catch(ExecutionException e) &#123; System.out.println(e); &#125; finally &#123; exec.shutdown(); &#125; &#125;&#125; /* Output:result of TaskWithResult 0result of TaskWithResult 1result of TaskWithResult 2result of TaskWithResult 3result of TaskWithResult 4result of TaskWithResult 5result of TaskWithResult 6result of TaskWithResult 7result of TaskWithResult 8result of TaskWithResult 9*///:~ （注：内容整理自《Thinking in Java》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用Redis构建web应用]]></title>
      <url>%2Fpost%2Fredis-webapp%2F</url>
      <content type="text"><![CDATA[目录 登录和cookie缓存cookie：当我们登录互联网服务的时候，这些服务都会使用cookie来记录我们的身份，cookie由少量数据组成，网站会要求我们的浏览器存储这些数据，并在每次服务发生请求时将这些数据传回给服务 对于用来登录的cookie，有两种方法可以将登录信息储存在cookie里面，一种是签名（signed）cookie一种是令牌（token）cookie 签名cookie通常会储存用户名，还可能有用户ID，用户最后一次成功登录的时间，以及网站觉得有用的其他信息，除此之外，cookie还会包含一个签名，服务器可以用它来验证浏览器发生的信息是否被改动（比如cookie中的登录用户名改成另一个用户） 令牌cookie会在cookie里面储存一串随机字节作为令牌，服务器可以根据令牌在数据库中查找令牌的拥有者，随着时间的推移，旧令牌会被新令牌取代 redis实现令牌登录cookie：首先，我们将使用一个hash来储存登录cookie令牌和已登录用户之间的映射，要检查一个用户是否已经登录，需要根据给定的令牌来查找与之对应的用户，并在用户已经登录的情况下，返回该用户的id 尝试获取并返回令牌对应的用户12def check_token(conn,token): return conn.hget('login:',token) 更新令牌：用户每次浏览页面的时候，程序都会对用户存储在登录hash里面的信息进行更新，并将用户的令牌和当前时间戳添加到记录最近登录用户的有序集合里面，如果用户正在浏览一个商品页面，那么程序还会将这个商品添加到记录这个用户最近浏览过的商品的有序集合里面，并在被记录的商品的数量超过25个时，对这个有序集合进行修剪 1234567def update_token(conn,token,user,item=None): timestamp = time.time() //获取当前时间戳 conn.hset('login:',token,user) //维持令牌与用户之间的映射 conn.zadd('recent',token,timestamp)//记录令牌最后一次出现的时间 if item： conn.zadd('viewed:'+token,item,timestamp) //记录用户浏览过的商品 conn.zremrangebyrank('viewed:'+token,0,-26)//移除旧的记录，只保存用户浏览过的25个商品 储存会话的数据所需的内存会随着时间的推移不断增加，需要清理旧数据，只保存1000万个，清理程序是一个循环，检查集合的大小，超过了限制就移除最多100个旧令牌，并移除记录用户信息的hash信息，并清除浏览信息。如果没有要清理的，休眠1秒，在检查(附：使用redis过期时间，就可以在一段时间之后让redis自动删除他们) 12345678910111213141516171819202122 QUIT = False LIMIT = 10000000 def clean_sessions(conn): while not QUIT://找出目前已有令牌的数量size = conn.zcard('recent:') //令牌数量未超过限制，休眠并在之后重新检查if size &lt;= LIMIT: time.sleep(1) continue//获取需要移除的令牌idend_index = min(size-LIMIT,100)tokens = conn.zrange('recent:',0,end_index-1)//为那些要被删除的令牌构建键名session_keys = [] for token in tokens: session_keys.append(‘viewed:’+token)//移除旧的那些令牌conn,delete(*session_keys)conn.hdel('login:',*tokens)conn.zrem('recent',*tokens) 购物车每个用户的购物车是一个散列，这个散列储存了商品ID与商品订购数量之间的映射，对商品数量验证的工作由web应用程序复杂，我们要做的是在商品的订购数量发生变化的时候，对购物车进行更新12345 def add_to_cart(conn,session,count)if count &lt;=0; conn.hrem('cart:'+session,item)else conn.hset('cart:'+session,item,count) 网页缓存1234567891011121314def cache_request(conn,request,callback): //对于不用背缓存的请求，直接调用回调函数 if not can_cache(conn,request); return callback(request) //将请求装换成一个简单的字符串建。方便之后进 行查找 page_key = 'cache:'+hash_request(request) //查找被缓存的页面 content = conn.get(request) //如果页面还没有被缓存，那么生成页面 if not content: content=callback(request) //将新生成的页面放到缓存里 conn.setex(page_key,content,300) return content 数据行缓存为了应对促销活动带来的大量负载，我们需要对数据进行缓存，具体的做法是：编写一个持续运行的守护进程函数，让这个函数将指定数据行缓存到redis里面，并不定期地对缓存进行更新，数据将被转为json储存在redis的字符串里 程序使用了两个zset，来记录应该在何时何地对缓存进行更新：第一个有序集合为调度有序集合，它的成员为数据行的行id，而分值是一个时间戳，记录了应该在何时将指定的数据行缓存到redis里面，第二个有序集合为延时zset，它的成员也是数据行的行id，而分值则记录了指定数据行的缓存需要每隔多少秒更新一次 调度缓存和终止缓存的函数12345def schedule_row_cache(conn,row_id,delay): //先设置数据行的延迟值 conn.zadd('delay',row_id,delay) //立即对需要缓存的数据进行调度 conn.zadd('schedule‘，row_id,time.time()) 复杂缓存数据的函数1234567891011121314151617181920212223def cache_rows(conn): while not QUIT: //尝试获取下一个需要被缓存的数据行以及该行的调度时间戳，命令会返回一个包含零个或一个元组的列表 next = conn.zrange('schedule:',0,0,withscores=Ture) now = time.time() //暂时没有行需要被缓存，休眠60ms后重试 if not next or next[0][1]&gt;row time.sleep(.05) continue row_id = next[0][0] //提前获取下一次调度的延迟时间 delay=conn.zscore('delay',row_id) if delay &lt;=0: //不必缓存这个行，从他从缓存中移除 conn.zrem('delay:',row_id) conn.zrem('schedule:',row_id) conn.delete('inv:'+row_id) continue //读取数据行，更新调度时间并设置缓存值 row = Inventory.get(row_id) conn.zadd('schedule:',row_id,now+delay) conn.set('inv'+row_id,json.dump(row.to_dict())) 网页分析在原来的update_token中12345678 def update_token(conn,token,user,item=None): timestamp = time.time() //获取当前时间戳 conn.hset('login:',token,user) //维持令牌与用户之间的映射 conn.zadd('recent',token,timestamp)//记录令牌最后一次出现的时间if item： conn.zadd('viewed:'+token,item,timestamp) //记录用户浏览过的商品 conn.zremrangebyrank('viewed:'+token,0,-26)//移除旧的记录，只保存用户浏览过的25个商品 conn.zincrby('viewed:'item,-1) 新添加的代码记录了商品的浏览次数，并根据浏览次数对商品进行了排序，被浏览的最多的商品将被放在有序集合的索引0的位置上，并且具有最少的分值为了让商品浏览次数保持最新，我们需要定期修剪有序集合的长度并调整已有元素的分值，从而使得新流行的商品也可以在排行榜里面占据一席之地 1234567def rescale_viewed(conn); while not QUIT: //删除所有牌面在2万名之后的商品 conn.zremrangebyrank('viewed',0,-20001) //将浏览次数降低为原来的一般 conn,zinterstore('viewed:' .5) time.sleep(300) 修改之前的的can_cache()函数12345678910 def can_cache(conn request); //尝试从页面里面获取商品id item_id = extract_itrm_id(request) //检查这个页面能否被缓存已经这个页面是否为商品页面 if not item_id or is_dynamic(request);return False //获取商品的浏览次数牌面 rank = conn,zrank('viewed',item_id) 根据排名判断是否需要被缓存 return rank is not None and rank&lt;10000 （注：内容整理自《redis实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《年轻可以一无所有》书摘]]></title>
      <url>%2Fpost%2Fread-youth%2F</url>
      <content type="text"><![CDATA[青春五千日 人生三万天 你要盯住有限的几个机会，把手艺越磨越精，最后才能再众多对手的竞争中胜出，速成文化，也许让位与专精文化。年轻人的心态要改。不是什么事情稍微玩儿几年就会成大器的。你也许真要“十年磨一剑”，甚至在此之后还要付出长年的努力，最终才会有理想的结果。 在一个正常的社会，年轻人就是应该一无所有的。难道生活刚起步，就应该指望别人给你许多许多吗？拜托，你有没有想过给了别人什么？凭什么向别人索取。 大家总是盯着自己没有的东西，并为之死去活来的争，恨不得把什么都搭上，却不珍视自己所拥有的，不觉得那有什么价值。《纽约时报》著名专栏作家戴维布鲁克斯又一次吐露了真言：大概所以功成名就的中年人，都宁愿把自己的金钱和地位统统放弃来换回年轻！ 但我知道有两样东西属于我的私有产权，别人想侵占也不可能：一是知识，一是健康。我正式因为精心守护这两样东西，而碰到了后来成为妻子的美丽女友–对方居然和我一样一无所有。 当你买任何一件自己不真正需要的东西时，就开始接受这个东西的奴役了。 要尽可能把青春全部投入自己的发展，而不是廉价拍卖。 成长首先意味着和比自己年纪大的人相处，听他们的教诲。 生活的速度比虚拟世界中的速度慢的多，我从小听着“十年寒窗”“十年磨一剑”“十年冷板凳”之类的教训。于是总看着10年以后的图景。惦记着生活何时开始：28岁学英文，是为了生活做准备；34岁拿到教职，觉得生活刚刚开始；如今年过半百。觉得一生大部分的事情尚未启动。我相信。20年后我大概会效法许多美国人，以退休迎接人生的另一个开端，一个充满了创造和快乐的开端。这并不是觉得自己能干多么惊天动地的事情，而是觉得只要足够长的时间准备，就会有所得。就会有东西贡献出来。 生活中右许多现成的东西早该在那里等着自己。就像在网络上那样。只需要点击一个“喜欢”或“不喜欢”。他们似乎没有想明白：生活中本没有那么简单的点击键。当他们找不到这样的点击键时，似乎就不知道该怎么生活了。 你们老的太快，因为你们对自己成长期待太急切，因为你们沉溺于虚拟世界。忘了真实的生活是什么样子。 如果你们总是以扎克伯格，乔布斯为样板树立自己的生活的目标，觉得一个技艺高超可靠的老技工不足道也，如果你们期待这以网络般的速度和戏剧感达到这样的目标，那么，更可能发生的是：你们很年轻的时候就觉得自己错过了人生，早早就觉得自己成为无足轻重的屌丝。 一生中真正的朋友用五个手指头就可以数过来 我们花同样的甚至更多的时间，去追逐那些我们很少接触的好友，冷落的则是我们所爱的人 登山是为了看世界，而不是让世界看你。 友情是除了智慧之外，人所能得到的最好的礼物。 过度的受同龄人的影响，错过了想长辈学习的机会，不要连向同龄人学习的机会也错过，你余下的一生，不是对16岁时就形成的自我不停地玩味品尝、没有终结得自爱。如果你能从虚拟世界中出来，看看身边，也许你会马上发现“三人行必有我师”也许你会不停地翻上一座又一座的高山，超越自己既有的生活眼界。 如果被体制耽误的同时，还自己耽误自己，那可就真是青春荒芜了 即刻行动才是王道，这里有一个底线，就是自食其力，在此基础上，再设计未来的计划。 青春首先不能荒芜，日后才能开花结果 工作是生活的一个立脚点，有了这个立脚点，就要为自己的未来经营。给自己不停地充电。 每天在办公室加班到半夜十一二点，干的又是下层白领的简单劳动，自身还怎么发展？自身不发展，还怎么升值？ 发展出一套清晰的人生哲学还是非常重要的，这会帮助你在具体的环境中明智地权衡得失。 这需要你在一个高度物质化得攀比时代，能够保持自信和淡定。如果你一看见同学买车，买房就内心沉沦，你怎么可以专注于自己？俗话说，有所得，必有所失。你不懂得舍弃，就很难获得 年轻人应该意识到，房子、车子这类东西，未必能搞界定人的幸福感，而且现在没有，以后也可以有，毕竟这些都是可以靠金钱买来的。但是，你还有两样更珍贵的东西：健康和知识（或者说本事）。这些不是有钱就可以买到的，需要你不停地用生命来投入。 怎么超脱世俗世界对物质的追逐？以我自己的经验，最简单的办法就是眼望未来，当你有了“我是属于未来的人”的定位后，对于眼前的得失就会有一种类似出世的解脱心态。年轻人日后的路比我们长得多。做的这一点也更容易些。这就是青春的力量。 那些教育水平比较高、职业地位比较高、中高收入的阶层，往往经受着更大的心理压力。 中国人之间的攀比也特别强。穿什么牌子，开什么车，住什么房子，似乎都是个事，大家比来比去，总觉得自己还不够有实力，心里愈发焦急。 人如果能“看破红尘”一些，别在乎那些别人都特别在乎的事情，生活开销会少不少，压力会减轻很多，幸福感会提高，当然也就会更健康了。总之，成功如果仅仅是别人的期待的话，那就不值得你为之付出那么多。 对屌丝心态的一个最直接的解释，恐怕就是一个破罐破摔的心态，这种心态的一个后果，就是无所谓，满不在乎，最终导致对社会主流价值的拒绝和反叛，这也是主流社会对之戒备的理由。 社会给一个人的回报，不是看你关系了自己多少，为自己争夺利益和报酬，而是看你关系了别人多少，为别人提供了多少有价值的服务。中国的年轻一代，一旦接受了这种用物质来界定成功的价值观念，就会从一开始就追求最低层级的满足，限制自己的人生视野，一生往往会在低层级的境界中转悠。 那种一次性事件带来的幸福感，仅仅停留在那个时刻，几个月后就会消失，即使你为了那个时刻奋斗了许多年，所以，剩下的12%，虽然看起来分量不大，实际上却非常重要，那就是信仰、家庭、社区、和工作这四方面的价值。 寻求低层次的满足、不管你有多满足，还是很低的满足。 如果你根据自己拥有的东西来衡量成功，并有很大的成功和赚钱的压力，那么在你眼前，这一切就都是破产的买卖。再自我反省一下，我们中国人，似乎每个人都是理财大师，看着美国人这么冒傻气会偷偷地笑。然而，当把一切都化约为理财时，我们就丧失了自己的生活和生活的幸福。 《独立宣言》上标举“生命、自由和追求幸福的权利”为什么是“追求幸福的权利”而不是“幸福的权利”？因为“幸福”不是一个理所应得的东西。你有追求的自由，但不能指望别人担保你就一定能得到，你更不能因为你自己的要求没有获得满足，就觉得自己是天下最不幸的人。 走出自己的舒适域，这样才能进步。 自己待会儿，即所谓独处，本事人格形成的重要环节。丧失了这样的机会。后果又是什么呢？恐怕，现在大部分人都忘记了这回事。 而今中国的年轻一代，有太多太多人从小就紧张地东张西望，生怕自己错过什么，最终，他们所错过的，恰恰是自己。 在学业、事业等技术细节上，年轻人应该尽可能自己寻求解决问题的答案，但是，在人格的塑造、品性的培养等素质层面，“老人言”往往会提供一个明确的指南。 现在的年轻人中，那种被称为出息的人，往往合乎上述的规范。他们属于家长基本上不用操心的孩子，因为他们在学业等具体事务上，都能自己安排得井井有条，比家长更会处理。但其做人的基本品信往往受长辈影响。和家长沟通得也比较好。那些被称为没出息的人。则几乎完全倒过来。在做人的基本品性上，他们非常讨厌大人的教导。比如要用功一点。持之以恒等教诲，在他们看来都是愚不可及的道德说教。他们我行我素，甚至会对父母颐指气使。但是，真要办具体的事情，则几乎完全拜托家长。比如托家长的关系获得种种机会，让家长花钱送自己去留学，等等。也就是说。他们不听老人言。但是要老子钱。 苏格拉底说：“满足于最少的所得的人，才是最富足的” 他们只看到自己得到了什么，并拿这个比来比去，从来不相信自己付出什么，从来不想想看自己能从内心中掏出什么东西来献给这个世界。因为他们内心太贫乏了。哪里有东西与别人分享？这让我想起西方的谚语：“虽然一个没钱的人很穷，但除了钱以为什么都没有的人更穷” 学生在没有走向社会前，总觉得世界以自己为中心。但一求职就知道，世界是以别人为中心，而且这些别人还动不动就把你扔进纸篓，你事业的起步，首先是要证明你不是垃圾。做到这一点还并不是那么容易。要闯荡世界。就要从建立这样的认识开始。 出来见识多了就知道，聪明人实在太多了，自己算不上老几。这些聪明绝顶人之间还要竞争，最终成功的还是少数，往往是那些没有和杰出人才直接打交道的，更容易对所谓天生才能迷信备至。 对压力的心理反应，往往取决于我们事先的期待。是吧压力当成生活中必要的挑战，并为这样的挑战感到兴奋，还是把压力视为洪水猛兽，动不动就觉得压力超过自己的承受极限，这在很大程度上决定了我们在压力之下的心理和生理健康。 人的潜力总是比自己想象得要大。自信能够帮助人最大限度地发挥这种潜力 人生是一场马拉松，青春算是起跑，作为马拉松迷，我看了那么多著名的比赛，还没有一次看到起跑时领先的人最后拿了冠军的。最后拿了冠军的人，往往都是开始特别淡定。跟在别人后面。调整自己的身体。以我的经验看，青春期最重要的就是培养自己的品信和习惯。这比具体学到什么东西还重要。 其中特别重要的猛攻自己的弱项，当一个人发现经过努力，在自己的弱项上也可以不弱时，就更有信心和勇气去迎接更大的挑战。一旦品性和习惯养成，形成指导自己一生的准则，十几年后就会赢。 现在我们学到的所以东西，随时有可能随着知识更新而变得废弃无用，但是，强健的人格则是面对变幻不定的挑战的最好本钱。 发展有效的学习能力，学习习惯，比具体学到的东西更为重要。 为什么是我？因为我要干一些大部分人都不会干的事情。 接受一个理念很容易，但培养一个习惯却很难。 西方有句谚语总结得更为精到：“留意你的思想，因为思想会变成言辞。留意你的言辞，因为言辞会变成行动。留意你的行动，因为行动会变成习惯。留意你的习惯，因为习惯会变为品格，留意你的品格，因为品格会变成你的命运” 你目前的境遇不会决定你走到哪里，而只是决定你的起点。 境遇本身无法造就一个人，而只是揭示一个人。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java ThreadLocal 简介]]></title>
      <url>%2Fpost%2Fjava-ThreadLocal%2F</url>
      <content type="text"><![CDATA[ThreadLocal在Spring中发挥着重要的作用，在管理request作用域的Bean、事务管理、任务调度、AOP等模块都出现了它们的身影，起着举足轻重的作用。要想了解Spring事务管理的底层技术，ThreadLocal是必须攻克的山头堡垒。 我们知道spring通过各种模板类降低了开发者使用各种数据持久技术的难度。这些模板类都是线程安全的，也就是说，多个DAO可以复用同一个模板实例而不会发生冲突。我们使用模板类访问底层数据，根据持久化技术的不同，模板类需要绑定数据连接或会话的资源。但这些资源本身是非线程安全的，也就是说它们不能在同一时刻被多个线程共享。虽然模板类通过资源池获取数据连接或会话，但资源池本身解决的是数据连接或会话的缓存问题，并非数据连接或会话的线程安全问题。 按照传统经验，如果某个对象是非线程安全的，在多线程环境下，对对象的访问必须采用synchronized进行线程同步。但模板类并未采用线程同步机制，因为线程同步会降低并发性，影响系统性能。此外，通过代码同步解决线程安全的挑战性很大，可能会增强好几倍的实现难度。那么模板类究竟仰仗何种魔法神功，可以在无须线程同步的情况下就化解线程安全的难题呢？答案就是ThreadLocal！ ThreadLocal在Spring中发挥着重要的作用，在管理request作用域的Bean、事务管理、任务调度、AOP等模块都出现了它们的身影，起着举足轻重的作用。要想了解Spring事务管理的底层技术，ThreadLocal是必须攻克的山头堡垒。 ThreadLocal是什么早在JDK 1.2的版本中就提供Java.lang.ThreadLocal，ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。使用这个工具类可以很简洁地编写出优美的多线程程序。ThreadLocal，顾名思义，它不是一个线程，而是线程的一个本地化对象。当工作于多线程中的对象使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程分配一个独立的变量副本。所以每一个线程都可以独立地改变自己的副本，而不会影响其他线程所对应的副本。从线程的角度看，这个变量就像是线程的本地变量，这也是类名中“Local”所要表达的意思。 线程局部变量并不是Java的新发明，很多语言（如IBM XL、FORTRAN）在语法层面就提供线程局部变量。在Java中没有提供语言级支持，而以一种变通的方法，通过ThreadLocal的类提供支持。所以，在Java中编写线程局部变量的代码相对来说要笨拙一些，这也是为什么线程局部变量没有在Java开发者中得到很好普及的原因。 ThreadLocal的接口方法ThreadLocal类接口很简单，只有4个方法，我们先来了解一下。void set(Object value)设置当前线程的线程局部变量的值；public Object get()该方法返回当前线程所对应的线程局部变量；public void remove()将当前线程局部变量的值删除，目的是为了减少内存的占用，该方法是JDK 5.0新增的方法。需要指出的是，当线程结束后，对应该线程的局部变量将自动被垃圾回收，所以显式调用该方法清除线程的局部变量并不是必须的操作，但它可以加快内存回收的速度；protected Object initialValue()返回该线程局部变量的初始值，该方法是一个protected的方法，显然是为了让子类覆盖而设计的。这个方法是一个延迟调用方法，在线程第1次调用get()或set(Object)时才执行，并且仅执行1次。ThreadLocal中的默认实现直接返回一个null。 值得一提的是，在JDK5.0中，ThreadLocal已经支持泛型，该类的类名已经变为ThreadLocal。API方法也相应进行了调整，新版本的API方法分别是void set(T value)、T get()以及T initialValue()。 ThreadLocal是如何做到为每一个线程维护变量的副本的呢？其实实现的思路很简单：在ThreadLocal类中有一个Map，用于存储每一个线程的变量副本，Map中元素的键为线程对象，而值对应线程的变量副本。我们自己就可以提供一个简单的实现版本： 代码清单9-3 SimpleThreadLocal1234567891011121314151617181920212223242526public class SimpleThreadLocal &#123; private Map valueMap = Collections.synchronizedMap(new HashMap()); public void set(Object newValue) &#123; //①键为线程对象，值为本线程的变量副本 valueMap.put(Thread.currentThread(), newValue); &#125; public Object get() &#123; Thread currentThread = Thread.currentThread(); //②返回本线程对应的变量 Object o = valueMap.get(currentThread); //③如果在Map中不存在，放到Map中保存起来 if (o == null &amp;&amp; !valueMap.containsKey(currentThread)) &#123; o = initialValue(); valueMap.put(currentThread, o); &#125; return o; &#125; public void remove() &#123; valueMap.remove(Thread.currentThread()); &#125; public Object initialValue() &#123; return null; &#125; &#125; 虽然代码清单9 3中这个ThreadLocal实现版本显得比较幼稚，但它和JDK所提供的ThreadLocal类在实现思路上是非常相近的。 一个TheadLocal实例下面，我们通过一个具体的实例了解一下ThreadLocal的具体使用方法。代码清单9-4 SequenceNumber 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.baobaotao.basic; public class SequenceNumber &#123; //①通过匿名内部类覆盖ThreadLocal的initialValue()方法，指定初始值 private static ThreadLocal&lt;Integer&gt; seqNum = new ThreadLocal&lt;Integer&gt;()&#123; public Integer initialValue()&#123; return 0; &#125; &#125;; //②获取下一个序列值 public int getNextNum()&#123; seqNum.set(seqNum.get()+1); return seqNum.get(); &#125; public static void main(String[ ] args) &#123; SequenceNumber sn = new SequenceNumber(); //③ 3个线程共享sn，各自产生序列号 TestClient t1 = new TestClient(sn); TestClient t2 = new TestClient(sn); TestClient t3 = new TestClient(sn); t1.start(); t2.start(); t3.start(); &#125; private static class TestClient extends Thread &#123; private SequenceNumber sn; public TestClient(SequenceNumber sn) &#123; this.sn = sn; &#125; public void run() &#123; //④每个线程打出3个序列值 for (int i = 0; i &lt; 3; i++) &#123; System.out.println("thread["+Thread.currentThread().getName()+ "] sn["+sn.getNextNum()+"]"); &#125; &#125; &#125; &#125; 通常我们通过匿名内部类的方式定义ThreadLocal的子类，提供初始的变量值，如①处所示。TestClient线程产生一组序列号，在③处，我们生成3个TestClient，它们共享同一个SequenceNumber实例。运行以上代码，在控制台上输出以下的结果： thread[Thread-2] sn[1]thread[Thread-0] sn[1]thread[Thread-1] sn[1]thread[Thread-2] sn[2]thread[Thread-0] sn[2]thread[Thread-1] sn[2]thread[Thread-2] sn[3]thread[Thread-0] sn[3]thread[Thread-1] sn[3] 考查输出的结果信息，我们发现每个线程所产生的序号虽然都共享同一个Sequence Number实例，但它们并没有发生相互干扰的情况，而是各自产生独立的序列号，这是因为我们通过ThreadLocal为每一个线程提供了单独的副本。 与Thread同步机制的比较ThreadLocal和线程同步机制相比有什么优势呢？ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。 在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。这时该变量是多个线程共享的，使用同步机制要求程序缜密地分析什么时候对变量进行读写，什么时候需要锁定某个对象，什么时候释放对象锁等繁杂的问题，程序设计和编写难度相对较大。 而ThreadLocal则从另一个角度来解决多线程的并发访问。ThreadLocal为每一个线程提供一个独立的变量副本，从而隔离了多个线程对访问数据的冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的对象封装，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。 由于ThreadLocal中可以持有任何类型的对象，低版本JDK所提供的get()返回的是Object对象，需要强制类型转换。但JDK 5.0通过泛型很好的解决了这个问题，在一定程度上简化ThreadLocal的使用，代码清单9-2就使用了JDK 5.0新的ThreadLocal版本。 概括起来说，对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式：访问串行化，对象共享化。而ThreadLocal采用了“以空间换时间”的方式：访问并行化，对象独享化。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 Spring使用ThreadLocal解决线程安全问题我们知道在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域。就是因为Spring对一些Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全的“状态性对象”采用ThreadLocal进行封装，让它们也成为线程安全的“状态性对象”，因此有状态的Bean就能够以singleton的方式在多线程中正常工作了。 一般的Web应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程，如图9-2所示。这样用户就可以根据需要，将一些非线程安全的变量以ThreadLocal存放，在同一次请求响应的调用线程中，所有对象所访问的同一ThreadLocal变量都是当前线程所绑定的。下面的实例能够体现Spring对有状态Bean的改造思路： 代码清单9-5 TopicDao：非线程安全123456789public class TopicDao &#123; //①一个非线程安全的变量 private Connection conn; public void addTopic()&#123; //②引用非线程安全变量 Statement stat = conn.createStatement(); … &#125; &#125; 由于①处的conn是成员变量，因为addTopic()方法是非线程安全的，必须在使用时创建一个新TopicDao实例（非singleton）。下面使用ThreadLocal对conn这个非线程安全的“状态”进行改造： 代码清单9-6 TopicDao：线程安全12345678910111213141516171819202122232425import java.sql.Connection; import java.sql.Statement; public class TopicDao &#123; //①使用ThreadLocal保存Connection变量 private static ThreadLocal&lt;Connection&gt; connThreadLocal = new ThreadLocal&lt;Connection&gt;(); public static Connection getConnection()&#123; //②如果connThreadLocal没有本线程对应的Connection创建一个新的Connection， //并将其保存到线程本地变量中。 if (connThreadLocal.get() == null) &#123; Connection conn = ConnectionManager.getConnection(); connThreadLocal.set(conn); return conn; &#125;else&#123; //③直接返回线程本地变量 return connThreadLocal.get(); &#125; &#125; public void addTopic() &#123; //④从ThreadLocal中获取线程对应的 Statement stat = getConnection().createStatement(); &#125; &#125; 不同的线程在使用TopicDao时，先判断connThreadLocal.get()是否为null，如果为null，则说明当前线程还没有对应的Connection对象，这时创建一个Connection对象并添加到本地线程变量中；如果不为null，则说明当前的线程已经拥有了Connection对象，直接使用就可以了。这样，就保证了不同的线程使用线程相关的Connection，而不会使用其他线程的Connection。因此，这个TopicDao就可以做到singleton共享了。 当然，这个例子本身很粗糙，将Connection的ThreadLocal直接放在Dao只能做到本Dao的多个方法共享Connection时不发生线程安全问题，但无法和其他Dao共用同一个Connection，要做到同一事务多Dao共享同一个Connection，必须在一个共同的外部类使用ThreadLocal保存Connection。但这个实例基本上说明了Spring对有状态类线程安全化的解决思路。在本章后面的内容中，我们将详细说明Spring如何通过ThreadLocal解决事务管理的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java注解简介]]></title>
      <url>%2Fpost%2Fjava-annotation-introduction%2F</url>
      <content type="text"><![CDATA[有必要对JDK 5.0新增的注解（Annotation）技术进行简单的学习，因为spring 支持@AspectJ，而@AspectJ本身就是基于JDK 5.0的注解技术。所以学习JDK 5.0的注解知识有助于我们更好地理解和掌握Spring的AOP技术。 了解注解对于Java开发人员来说，在编写代码时，除了源程序以外，我们还会使用Javadoc标签对类、方法或成员变量进行注释，以便使用Javadoc工具生成和源代码配套的Javadoc文档。这些@param、@return等Javadoc标签就是注解标签，它们为第三方工具提供了描述程序代码的注释信息。使用过Xdoclet的朋友，对此将更有感触，像Struts、hibernate都提供了Xdoclet标签，使用它们可以快速地生成对应程序代码的配置文件。 JDK5.0注解可以看成是Javadoc标签和Xdoclet标签的延伸和发展。在JDK5.0中，我们可以自定义这些标签，并通过Java语言的反射机制中获取类中标注的注解，完成特定的功能。注解是代码的附属信息，它遵循一个基本原则：注解不能直接干扰程序代码的运行，无论增加或删除注解，代码都能够正常运行。Java语言解释器会忽略这些注解，而由第三方工具负责对注解进行处理。第三方工具可以利用代码中的注解间接控制程序代码的运行，它们通过Java反射机制读取注解的信息，并根据这些信息更改目标程序的逻辑，而这正是Spring AOP对@AspectJ提供支持所采取的方法。 很多东西的设计都必须遵循最基本的原则，为了防止机器人伤害人类，科幻作家阿西莫夫于1940年提出了“机器人三原则”：第一，机器人不能伤害人类；第二，机器人应遵守人类的命令，与第一条违背的命令除外；第三，机器人应能保护自己，与第一条违背的命令除外。这是给机器人赋予的伦理性纲领，机器人学术界一直将这三条原则作为机器人开发的准则。 一个简单的注解类通常情况下，第三方工具不但负责处理特定的注解，本身还提供了这些注解的定义，所以我们通常仅需关注如何使用注解就可以了。但定义注解类本身并不困难，Java提供了定义注解的语法。下面，我们马上着手编写一个简单的注解类，如代码清单7-1所示： 代码清单7-1 NeedTest注解类 1234567891011package com.baobaotao.aspectj.anno; import java.lang.annotation.ElementType; import java.lang.annotation.Retention; import java.lang.annotation.RetentionPolicy; import java.lang.annotation.Target; @Retention(RetentionPolicy.RUNTIME) //①声明注解的保留期限 @Target(ElementType.METHOD)//②声明可以使用该注解的目标类型 public @interface NeedTest &#123;//③定义注解 boolean value() default true;//④声明注解成员 &#125; Java新语法规定使用@interface修饰符定义注解类，如③所示，一个注解可以拥有多个成员，成员声明和接口方法声明类似，这里，我们仅定义了一个成员，如④所示。成员的声明有以下几点限制： 成员以无入参无抛出异常的方式声明，如boolean value(String str)、boolean value() throws Exception等方式是非法的；可以通过default为成员指定一个默认值，如String level() default “LOW_LEVEL”、int high() default 2是合法的，当然也可以不指定默认值；成员类型是受限的，合法的类型包括原始类型及其封装类、String、Class、enums、注解类型，以及上述类型的数组类型。如ForumService value()、List foo()是非法的。 在①和②处，我们所看到的注解是Java预定义的注解，称为元注解（Meta-Annotation），它们被Java编译器使用，会对注解类的行为产生影响。@Retention(RetentionPolicy. RUNTIME)表示NeedTest这个注解可以在运行期被JVM读取，注解的保留期限类型在java.lang.annotation.Retention类中定义，介绍如下： SOURCE：注解信息仅保留在目标类代码的源码文件中，但对应的字节码文件将不再保留；CLASS：注解信息将进入目标类代码的字节码文件中，但类加载器加载字节码文件时不会将注解加载到JVM中，也即运行期不能获取注解信息；RUNTIME：注解信息在目标类加载到JVM后依然保留，在运行期可以通过反射机制读取类中注解信息。Target(ElementType.METHOD)表示NeedTest这个注解只能应用到目标类的方法上，注解的应用目标在java.lang.annotation.ElementType类中定义：TYPE：类、接口、注解类、Enum声明处，相应的注解称为类型注解；FIELD：类成员变量或常量声明处，相应的注解称为域值注解；METHOD：方法声明处，相应的注解称为方法注解；PARAMETER：参数声明处，相应的注解称为参数注解；CONSTRUCTOR：构造函数声明处，相应的注解称为构造函数注解；LOCAL_VARIABLE：局部变量声明处，相应的注解称为局域变量注解；ANNOTATION_TYPE：注解类声明处，相应的注解称为注解类注解，ElementType. TYPE包括ElementType.ANNOTATION_TYPE；PACKAGE：包声明处，相应的注解称为包注解。 如果注解只有一个成员，则成员名必须取名为value()，在使用时可以忽略成员名和赋值号（=），如@NeedTest(true)。注解类拥有多个成员时，如果仅对value成员进行赋值则也可不使用赋值号，如果同时对多个成员进行赋值，则必须使用赋值号，如DeclareParents (value = “NaiveWaiter”, defaultImpl = SmartSeller.class)。注解类可以没有成员，没有成员的注解称为标识注解，解释程序以标识注解存在与否进行相应的处理；此外，所有的注解类都隐式继承于java.lang.annotation.Annotation，但注解不允许显式继承于其他的接口。 我们希望使用NeedTest注解对业务类的方法进行标注，以便测试工具可以根据注解情况激活或关闭对业务类的测试。在编写好NeedTest注解类后，就可以在其他类中使用它了。 使用注解我们在ForumService中使用NeedTest注解，标注业务方法是否需要测试，如代码清单7-2所示： 代码清单7-2 ForumService：使用注解1234567891011package com.baobaotao.aspectj.anno; public class ForumService &#123; @NeedTest(value=true) ① public void deleteForum(int forumId)&#123; System.out.println("删除论坛模块："+forumId); &#125; @NeedTest(value=false) ② public void deleteTopic(int postId)&#123; System.out.println("删除论坛主题："+postId); &#125; &#125; 如果注解类和目标类不在同一个包中，需要通过import引用的注解类。在①和②处，我们使用NeedTest分别对deleteForum()和deleteTopic()方法进行标注。在标注注解时，可以通过以下格式对注解成员进行赋值： &lt;注解名&gt;(&lt;成员名1&gt;=&lt;成员值1&gt;,&lt;成员名1&gt;=&lt;成员值1&gt;,…) 如果成员是数组类型，可以通过{}进行赋值，如boolean数组的成员可以设置为{true,false,true}。下面是几个注解标注的例子： 示例1，多成员的注解：12@AnnoExample(id= 2868724, synopsis = "Enable time-travel", engineer = "Mr. Peabody",date = "4/1/2007") 示例2，一个成员的注解，成员名为value。可以省略成员名和赋值符号： 1@Copyright("2011 bookegou.com All Right Reserved") 示例3，无成员的注解：1@Override 示例4，成员为字符串数组的注解：1@SuppressWarnings(value=&#123;"unchecked","fallthrough"&#125;) 示例5，成员为注解数组类型的注解：12@Reviews(&#123;@Review(grade=Review.Grade.EXCELLENT,reviewer="df"), @Review(grade=Review.Grade.UNSATISFACTORY,reviewer="eg", comment="This method needs an @Override annotation")&#125;) Reviews注解拥有一个@Review注解数组类型的成员，@Review注解类型有三个成员，其中reviewer、comment都是String类型，但comment有默认值，grade是枚举类型的成员。由于NeedTest注解的保留限期是RetentionPolicy.RUNTIME类型，因此当ForumService被加载到JVM时，仍就可通过反射机制访问到ForumService各方法的注解信息。 访问注解前面提到过，注解不会直接影响程序的运行，但是第三方程序或工具可以利用代码中的注解完成特殊的任务，间接控制程序的运行。对于RetentionPolicy.RUNTIME保留期限的注解，我们可以通过反射机制访问类中的注解。 在JDK5.0里，Package、Class、Constructor、Method以及Field等反射对象都新增了访问注解信息的方法：T getAnnotation(Class annotationClass)，该方法支持通过泛型直接返回注解对象。 下面，我们就通过反射来访问注解，得出ForumService 类中通过@NeedTest注解所承载的测试需求，如代码清单7-3所示： 代码清单7-3 TestTool：访问代码中的注解1234567891011121314151617181920212223242526package com.baobaotao.aspectj.anno; import java.lang.reflect.Method; public class TestTool &#123; public static void main(String[] args) &#123; //①得到ForumService对应的Class对象 Class clazz = ForumService.class; //②得到ForumSerivce对应的Method数组 Method[] methods = clazz.getDeclaredMethods(); System.out.println(methods.length); for (Method method : methods) &#123; //③获取方法上所标注的注解对象 NeedTest nt = method.getAnnotation(NeedTest. class); if (nt != null) &#123; if (nt.value()) &#123; System.out.println(method.getName() + "()需要测试"); &#125; else &#123; System.out.println(method.getName() + "()不需要测试"); &#125; &#125; &#125; &#125; &#125; 在③处，通过方法的反射对象，我们获取了方法上所标注的NeedTest注解对象，接着就可以访问注解对象的成员，从而得到ForumService类方法的测试需求。运行以上代码，输出以下的信息： deleteForum()需要测试 deleteTopic()不需要测试]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java代理简介]]></title>
      <url>%2Fpost%2Fjava-proxy-introduction%2F</url>
      <content type="text"><![CDATA[spring AOP使用动态代理技术在运行期织入增强的代码，为了揭示Spring AOP底层的工作机理，有必要对涉及到的Java知识进行学习。Spring AOP使用了两种代理机制：一种是基于JDK的动态代理；另一种是基于CGLib的动态代理。之所以需要两种代理机制，很大程度上是因为JDK本身只提供接口的代理，而不支持类的代理。 http://www.iteye.com/topic/1123293 带有横切逻辑的实例我们通过具体化代码实现上一节所介绍例子的性能监视横切逻辑，并通过动态代理技术对此进行改造。在调用每一个目标类方法时启动方法的性能监视，在目标类方法调用完成时记录方法的花费时间。 代码清单 ForumService.java和ForumServiceImpl.java：包含性能监视横切代码 123456789package proxyTest;public interface ForumService &#123; void removeForum(int i); void removeTopic(int i);&#125; 12345678910111213141516171819202122232425262728293031323334package proxyTest;public class ForumServiceImpl implements ForumService &#123; public void removeTopic(int topicId) &#123; // ①-1开始对该方法进行性能监视 PerformanceMonitor.begin("ForumServiceImpl. removeTopic"); System.out.println("模拟删除Topic记录:" + topicId); try &#123; Thread.currentThread(); Thread.sleep(20); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; // ①-2结束对该方法进行性能监视 PerformanceMonitor.end(); &#125; public void removeForum(int forumId) &#123; // ②-1开始对该方法进行性能监视 PerformanceMonitor.begin("ForumServiceImpl. removeForum"); System.out.println("模拟删除Forum记录:" + forumId); try &#123; Thread.currentThread(); Thread.sleep(40); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; // ②-2结束对该方法进行性能监视 PerformanceMonitor.end(); &#125;&#125; 代码清单中粗体表示的代码就是具有横切逻辑特征的代码，每个Service类和每个业务方法体的前后都执行相同的代码逻辑：方法调用前启动PerformanceMonitor，方法调用后通知PerformanceMonitor结束性能监视并给记录性能监视结果。 PerformanceMonitor是性能监视的实现类，我们给出一个非常简单的实现版本，其代码如代码清单所示： 代码清单 PerformanceMonitor.java12345678910111213141516171819202122package proxyTest;public class PerformanceMonitor &#123; //①通过一个ThreadLocal保存调用线程相关的性能监视信息 private static ThreadLocal&lt;MethodPerformance&gt; performanceRecord = new ThreadLocal&lt;MethodPerformance&gt;(); //②启动对某一目标方法的性能监视 public static void begin(String method) &#123; System.out.println("begin monitor..."); MethodPerformance mp = new MethodPerformance(method); performanceRecord.set(mp); &#125; public static void end() &#123; System.out.println("end monitor..."); MethodPerformance mp = performanceRecord.get(); //③打印出方法性能监视的结果信息。 mp.printPerformance(); &#125; &#125; ThreadLocal是将非线程安全类改造为线程安全类的法宝，在9.2节中我们将详细介绍这个Java基础知识。PerformanceMonitor提供了两个方法：通过调用begin(String method)方法开始对某个目标类方法的监视，method为目标类方法的全限定名；而end()方法结束对目标类方法的监视，并给出性能监视的信息。这两个方法必须配套使用。 用于记录性能监视信息的MethodPerformance类的代码如所示： 代码清单 MethodPerformance.java123456789101112131415161718192021222324package proxyTest;public class MethodPerformance &#123; private long begin; private long end; private String serviceMethod; public MethodPerformance(String serviceMethod)&#123; this.serviceMethod = serviceMethod; //①记录目标类方法开始执行点的系统时间 this.begin = System.currentTimeMillis(); &#125; public void printPerformance()&#123; //②获取目标类方法执行完成后的系统时间，并进而计算出目标类方法执行时间 end = System.currentTimeMillis(); long elapse = end - begin; //③报告目标类方法的执行时间 System.out.println(serviceMethod+"花费"+elapse+"毫秒。"); &#125; &#125; 通过下面的代码测试拥有性能监视能力的ForumServiceImpl业务方法：12345678910package proxyTest;public class TestForumService &#123; public static void main(String[] args) &#123; ForumService forumService = new ForumServiceImpl(); forumService.removeForum(10); forumService.removeTopic(1012); &#125;&#125; 我们得到以下输出信息： begin monitor… ①removeForum(10)方法的性能监视报告模拟删除Forum记录:10end monitor…com.baobaotao.proxy.ForumServiceImpl.removeForum花费47毫秒。begin monitor… ①removeTopic(1012)方法的性能监视报告模拟删除Topic记录:1012end monitor…com.baobaotao.proxy.ForumServiceImpl.removeTopic花费26毫秒。 正如代码清单实例所示，当某个方法需要进行性能监视，就必须调整方法代码，在方法体前后分别添加上开启性能监视和结束性能监视的代码。这些非业务逻辑的性能监视代码破坏了ForumServiceImpl业务逻辑的纯粹性。我们希望通过代理的方式，将业务类方法中开启和结束性能监视的这些横切代码从业务类中完全移除。并通过JDK动态代理技术或CGLib动态代理技术将横切代码动态织入到目标方法的相应位置。 JDK动态代理JDK 1.3以后，Java提供了动态代理的技术，允许开发者在运行期创建接口的代理实例。在Sun刚推出动态代理时，还很难想象它有多大的实际用途，现在我们终于发现动态代理是实现AOP的绝好底层技术。 JDK的动态代理主要涉及到java.lang.reflect包中的两个类：Proxy和InvocationHandler。其中InvocationHandler是一个接口，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编织在一起。 而Proxy利用InvocationHandler动态创建一个符合某一接口的实例，生成目标类的代理对象。这样讲一定很抽象，我们马上着手使用Proxy和InvocationHandler这两个魔法戒对上一节中的性能监视代码进行革新。 首先，我们从业务类ForumServiceImpl中删除性能监视的横切代码，使ForumServiceImpl只负责具体的业务逻辑，如代码清单6-5所示： 代码清单6-5 ForumServiceImpl：移除性能监视横切代码 1234567891011121314151617181920212223242526package proxyTest;public class ForumServiceImpl implements ForumService &#123; public void removeTopic(int topicId) &#123; System.out.println("模拟删除Topic记录:" + topicId); try &#123; Thread.currentThread(); Thread.sleep(20); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125; public void removeForum(int forumId) &#123; System.out.println("模拟删除Forum记录:" + forumId); try &#123; Thread.currentThread(); Thread.sleep(40); &#125; catch (Exception e) &#123; throw new RuntimeException(e); &#125; &#125;&#125; 在代码清单中的①和②处，原来的性能监视代码被移除了，我们只保留了真正的业务逻辑。 从业务类中移除的性能监视横切代码当然不能漂浮在空气中，它还得找到一个安身之所，InvocationHandler就是横切代码的安家乐园，我们将性能监视的代码安置在PerformanceHandler中，如代码清单所示： 123456789101112131415161718package proxyTest;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;public class PerformanceHandler implements InvocationHandler &#123; private Object target; public PerformanceHandler(Object target) &#123; // ②target为目标的业务类 this.target = target; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; PerformanceMonitor.begin(target.getClass().getName() + "." + method.getName()); Object obj = method.invoke(target, args);// ③-2通过反射方法调用业务类的目标方法 PerformanceMonitor.end(); return obj; &#125;&#125; ③处invoke()方法中粗体所示部分的代码为性能监视的横切代码，我们发现，横切代码只出现一次，而不是原来那样星洒各处。③-2处的method.invoke()语句通过Java反射机制间接调用目标对象的方法，这样InvocationHandler的invoke()方法就将横切逻辑代码（③-1）和业务类方法的业务逻辑代码（③-2）编织到一起了，所以我们可以将InvocationHandler看成是一个编织器。下面，我们对这段代码做进一步的说明。 首先，我们实现InvocationHandler接口，该接口定义了一个 invoke(Object proxy, Method method, Object[] args)的方法，proxy是最终生成的代理实例，一般不会用到；method是被代理目标实例的某个具体方法，通过它可以发起目标实例方法的反射调用；args是通过被代理实例某一个方法的入参，在方法反射调用时使用。 此外，我们在构造函数里通过target传入希望被代理的目标对象，如②处所示，在InvocationHandler接口方法invoke(Object proxy, Method method, Object[] args)里，将目标实例传给method.invoke()方法，调用目标实例的方法，如③所示。下面，我们通过Proxy结合PerformanceHandler创建ForumService接口的代理实例，如代码清单所示：代码清单 TestForumService：创建代理实例1234567891011121314151617181920212223package proxyTest;import java.lang.reflect.Proxy;public class TestForumService &#123; public static void main(String[] args) &#123; // ①希望被代理的目标业务类 ForumService target = new ForumServiceImpl(); // ②将目标业务类和横切代码编织到一起 PerformanceHandler handler = new PerformanceHandler(target); // ③根据编织了目标业务类逻辑和性能监视横切逻辑的InvocationHandler实例创建代理实例 ForumService proxy = (ForumService) Proxy.newProxyInstance(target .getClass().getClassLoader(), target.getClass().getInterfaces(), handler); // ④调用代理实例 proxy.removeForum(10); proxy.removeTopic(1012); &#125;&#125; 上面的代码完成业务类代码和横切代码的编织工作并生成了代理实例。在②处，我们让PerformanceHandler将性能监视横切逻辑编织到ForumService实例中，然后在③处，通过Proxy的newProxyInstance()静态方法为编织了业务类逻辑和性能监视逻辑的handler创建一个符合ForumService接口的代理实例。该方法的第一个入参为类加载器；第二个入参为创建代理实例所需要实现的一组接口；第三个参数是整合了业务逻辑和横切逻辑的编织器对象。 按照③处的设置方式，这个代理实例实现了目标业务类的所有接口，即Forum ServiceImpl的ForumService接口。这样，我们就可以按照调用ForumService接口实例相同的方式调用代理实例，如④所示。运行以上的代码，输出以下信息： begin monitor…模拟删除Forum记录:10end monitor…com.baobaotao.proxy.ForumServiceImpl.removeForum花费47毫秒。begin monitor…模拟删除Topic记录:1012end monitor…com.baobaotao.proxy.ForumServiceImpl.removeTopic花费26毫秒。 我们发现，程序的运行效果和直接在业务类中编写性能监视逻辑的效果一致，但是在这里，原来分散的横切逻辑代码已经被我们抽取到PerformanceHandler中。当其他业务类（如UserService、SystemService等）的业务方法也需要使用性能监视时，我们只要按照代码清单6-7相似的方式，分别为它们创建代理对象就可以了。下面，我们通过时序图描述通过创建代理对象进行业务方法调用的整体逻辑，以进一步认识代理对象的本质，如图所示。 我们在上图中使用虚线的方式对通过Proxy创建的ForumService代理实例加以凸显，ForumService代理实例内部利用PerformaceHandler整合横切逻辑和业务逻辑。调用者调用代理对象的removeForum()和removeTopic()方法时，上图的内部调用时序清晰地告诉我们实际上所发生的一切。 CGLib动态代理使用JDK创建代理有一个限制，即它只能为接口创建代理实例，这一点我们可从Proxy的接口newProxyInstance(ClassLoader loader, Class[] interfaces, InvocationHandler h)的方法签名中就看得很清楚：第二个入参interfaces就是需要代理实例实现的接口列表。虽然面向接口编程的思想被很多大师级人物（包括Rod Johnson）推崇，但在实际开发中，许多开发者也对此深感困惑：难道对一个简单业务表的操作也需要老老实实地创建5个类（领域对象类、Dao接口，Dao实现类，Service接口和Service实现类）吗？难道不能直接通过实现类构建程序吗？对于这个问题，我们很难给出一个孰好孰劣的准确判断，但我们确实发现有很多不使用接口的项目也取得了非常好的效果（包括大家所熟悉的SpringSide开源项目）。 对于没有通过接口定义业务方法的类，如何动态创建代理实例呢？JDK的代理技术显然已经黔驴技穷，CGLib作为一个替代者，填补了这个空缺。 CGLib采用非常底层的字节码技术，可以为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，并顺势织入横切逻辑。下面，我们采用CGLib技术，编写一个可以为任何类创建织入性能监视横切逻辑代理对象的代理创建器，如代码清单 所示： 代码清单 CglibProxy1234567891011121314151617181920212223242526package proxyTest;import java.lang.reflect.Method;import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;public class CglibProxy implements MethodInterceptor &#123; private Enhancer enhancer = new Enhancer(); public Object getProxy(@SuppressWarnings("rawtypes") Class clazz) &#123; enhancer.setSuperclass(clazz); // ① 设置需要创建子类的类 enhancer.setCallback(this); return enhancer.create(); // ②通过字节码技术动态创建子类实例 &#125; // ③拦截父类所有方法的调用 public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; PerformanceMonitor.begin(obj.getClass().getName() + "." + method.getName());// ③-1 Object result = proxy.invokeSuper(obj, args); PerformanceMonitor.end();// ③-1通过代理类调用父类中的方法 return result; &#125;&#125; 在上面代码中，用户可以通过getProxy(Class clazz)为一个类创建动态代理对象，该代理对象通过扩展clazz创建代理对象。在这个代理对象中，我们织入性能监视的横切逻辑（③-1）。intercept(Object obj, Method method, Object[] args,MethodProxy proxy)是CGLib定义的Interceptor接口的方法，它拦截所有目标类方法的调用，obj表示目标类的实例；method为目标类方法的反射对象；args为方法的动态入参；而proxy为代理类实例。 下面，我们通过CglibProxy为ForumServiceImpl类创建代理对象，并测试代理对象的方法，如代码清单所示： 代码清单 TestForumService：测试Cglib创建的代理类 小结Spring AOP的底层就是通过使用JDK动态代理或CGLib动态代理技术为目标Bean织入横切逻辑。在这里，我们对前面两节动态创建代理对象作一个小结。 我们虽然通过PerformanceHandler或CglibProxy实现了性能监视横切逻辑的动态织入，但这种实现方式存在三个明显需要改进的地方： 1）目标类的所有方法都添加了性能监视横切逻辑，而有时，这并不是我们所期望的，我们可能只希望对业务类中的某些特定方法添加横切逻辑；2）我们通过硬编码的方式指定了织入横切逻辑的织入点，即在目标类业务方法的开始和结束前织入代码；3）我们手工编写代理实例的创建过程，为不同类创建代理时，需要分别编写相应的创建代码，无法做到通用。 以上三个问题，在AOP中占用重要的地位，因为Spring AOP的主要工作就是围绕以上三点展开：Spring AOP通过Pointcut（切点）指定在哪些类的哪些方法上织入横切逻辑，通过Advice（增强）描述横切逻辑和方法的具体织入点（方法前、方法后、方法的两端等）。此外，Spring通过Advisor（切面）将Pointcut和Advice两者组装起来。有了Advisor的信息，Spring就可以利用JDK或CGLib的动态代理技术采用统一的方式为目标Bean创建织入切面的代理对象了。 JDK动态代理所创建的代理对象，在JDK 1.3下，性能强差人意。虽然在高版本的JDK中，动态代理对象的性能得到了很大的提高，但是有研究表明，CGLib所创建的动态代理对象的性能依旧比JDK的所创建的代理对象的性能高不少（大概10倍）。但CGLib在创建代理对象时所花费的时间却比JDK动态代理多（大概8倍），所以对于singleton的代理对象或者具有实例池的代理，因为无须频繁创建代理对象，所以比较适合用CGLib动态代理技术，反之适合用JDK动态代理技术。值得一提的是，由于CGLib采用动态创建子类的方式生成代理对象，所以不能对目标类中的final方法进行代理。 begin monitor…模拟删除Forum记录:10end monitor…com.baobaotao.proxy.ForumServiceImpl$EnhancerByCGLIB$$2a9199c0.removeForum花费47毫秒。begin monitor…模拟删除Topic记录:1023end monitor…com.baobaotao.proxy.ForumServiceImpl$EnhancerByCGLIB$$2a9199c0.removeTopic花费16毫秒。 观察以上的输出，除了发现两个业务方法中都织入了性能监控的逻辑外，我们还发现代理类的名字是com.baobaotao.proxy.ForumServiceImpl$EnhancerByCGLIB2a9199c0，这个特殊的类就是CGLib为ForumServiceImpl动态创建的子类。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redis初识]]></title>
      <url>%2Fpost%2Fredis-introduction%2F</url>
      <content type="text"><![CDATA[redis是一个速度非常快的非关系数据库，他可以存储键与5种不同类型的值之间的映射，可以将储存在内存的键值对持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展读性能 reids简介redis是一个速度非常快的非关系数据库，他可以存储键与5种不同类型的值之间的映射，可以将储存在内存的键值对持久化到硬盘，可以使用复制特性来扩展读性能，还可以使用客户端分片来扩展读性能 redis与memcached的区别两者都可以用于储存键值映射，彼此的性能也相差无几，但是redis能够自动以两种方式将数据写入硬盘，并且redis除了能存储普通字符串之外，还可以储存其他4种数据结构 redis拥有两种不同形式的持久化方法他们都可以用小而紧凑的格式将储存在内存中的数据写入硬盘吗，第一种方法是时间点转储，转储既可以在”指定时间段内有指定数量的写操作执行”这一条件被满足时执行，又可以通过调用两条转储到硬盘的命令中的任何一条来执行，第二种持久化方法将所有修改了数据库的命令都写入一个只追加文件里面，用户可以根据数据的重要程度，将只追加写入设置为从不同步，每秒同步一次或者没写入一个命令就同步一次 数据结构 STRING基本命令：GET SET DEL进阶：自增 自减 LIST基本命令：RPUSH LRANGE LINDEX LPOP 进阶：从列表里面移除元素，将元素插入列表中间，将列表修剪至指定长度 SETLIST可以存储多个相同的字符串，而SET则可以通过散列来保证自己存储的每个字符串都是各部相同的 set使用无序的方式储存基本命令：SADD SMEMBER SISMEMBER SREM进阶：SINTER SUNION SDIFF HASH储存的值既可以是字符串也可以是数字值，并且用户同样可以对散列储存的数字值执行自增操作或者自减操作 散列在很多方面就想一个微缩版的redis基本命令：HSET HGET HGETALL HDEL ZSET有序集合的值被称为分值，分值必须是浮点数，有序集合是redis里面为一个既可以根据成员访问，也可以根据分值以及分值的排列顺序来访问元素的结构基本命令：ZADD ZRANGE ZRANGEBYSCORE ZREM （注：内容整理自《redis实战》）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java反射简介]]></title>
      <url>%2Fpost%2Fjava-reflection-introduction%2F</url>
      <content type="text"><![CDATA[Java语言允许通过程序化的方式间接对Class进行操作，Class文件由类装载器装载后，在JVM中将形成一份描述Class结构的元信息对象，通过该元信息对象可以获知Class的结构信息：如构造函数、属性和方法等。Java允许用户借由这个Class相关的元信息对象间接调用Class对象的功能，这就为使用程序化方式操作Class对象开辟了途径。 原文链接：http://stamen.iteye.com/blog/1497981 简单实例我们将从一个简单例子开始探访Java反射机制的征程，下面的Car类拥有两个构造函数、两个方法以及三个属性 代码清单 Car.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647ublic class Car &#123; private String brand; private String color; private int maxSpeed; // ①默认构造函数 public Car() &#123; &#125; // ②带参构造函数 public Car(String brand, String color, int maxSpeed) &#123; this.brand = brand; this.color = color; this.maxSpeed = maxSpeed; &#125; // ③未带参的方法 public void introduce() &#123; System.out.println("brand:" + brand + ";color:" + color + ";maxSpeed:" + maxSpeed); &#125; public String getBrand() &#123; return brand; &#125; public void setBrand(String brand) &#123; this.brand = brand; &#125; public String getColor() &#123; return color; &#125; public void setColor(String color) &#123; this.color = color; &#125; public int getMaxSpeed() &#123; return maxSpeed; &#125; public void setMaxSpeed(int maxSpeed) &#123; this.maxSpeed = maxSpeed; &#125;&#125; 一般情况下，我们会使用如下的代码创建Car的实例： 12Car car = new Car(); car.setBrand("红旗CA72"); 或者1Car car = new Car("红旗CA72","黑色"); 以上两种方法都采用传统方式的直接调用目标类的方法，下面我们通过Java反射机制以一种更加通用的方式间接地操作目标类： 代码清单：ReflectTest.java1234567891011121314151617181920212223242526272829import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;public class ReflectTest &#123; public static Car initByDefaultConst() throws Throwable &#123; // ①通过类装载器获取Car类对象 ClassLoader loader = Thread.currentThread().getContextClassLoader(); Class clazz = loader.loadClass("Car"); // ②获取类的默认构造器对象并通过它实例化Car Constructor cons = clazz.getDeclaredConstructor((Class[]) null); Car car = (Car) cons.newInstance(); // ③通过反射方法设置属性 Method setBrand = clazz.getMethod("setBrand", String.class); setBrand.invoke(car, "红旗CA72"); Method setColor = clazz.getMethod("setColor", String.class); setColor.invoke(car, "黑色"); Method setMaxSpeed = clazz.getMethod("setMaxSpeed", int.class); setMaxSpeed.invoke(car, 200); return car; &#125; public static void main(String[] args) throws Throwable &#123; Car car = initByDefaultConst(); car.introduce(); &#125;&#125; 运行以上程序，在控制台上将打印出以下信息： brand:红旗CA72;color:黑色;maxSpeed:200 这说明我们完全可以通过编程方式调用Class的各项功能，这和直接通过构造函数和方法调用类功能的效果是一致的，只不过前者是间接调用，后者是直接调用罢了。 在ReflectTest中，使用了几个重要的反射类，分别是ClassLoader、Class、Constructor和Method，通过这些反射类就可以间接调用目标Class的各项功能了。在①处，我们获取当前线程的ClassLoader，然后通过指定的全限定类“com.baobaotao.beans.Car”装载Car类对应的反射实例。在②处，我们通过Car的反射类对象获取Car的构造函数对象cons，通过构造函数对象的newInstrance()方法实例化Car对象，其效果等同于new Car()。在③处，我们又通过Car的反射类对象的getMethod（String methodName,Class paramClass）获取属性的Setter方法对象，第一个参数是目标Class的方法名；第二个参数是方法入参的对象类型。获取方法反射对象后，即可通过invoke（Object obj,Object param）方法调用目标类的方法，该方法的第一个参数是操作的目标类对象实例；第二个参数是目标方法的入参。 在代码清单3 10中，粗体所示部分的信息即是通过反射方法操控目标类的元信息，如果我们将这些信息以一个配置文件的方式提供，就可以使用Java语言的反射功能编写一段通用的代码对类似于Car的类进行实例化及功能调用操作了。 类装载器ClassLoader类装载器工作机制类装载器就是寻找类的节码文件并构造出类在JVM内部表示对象的组件。在Java中，类装载器把一个类装入JVM中，要经过以下步骤： [1.]装载：查找和导入Class文件；[2.]链接：执行校验、准备和解析步骤，其中解析步骤是可以选择的：[2.1]校验：检查载入Class文件数据的正确性；[2.2]准备：给类的静态变量分配存储空间；[2.3]解析：将符号引用转成直接引用；[3.]初始化：对类的静态变量、静态代码块执行初始化工作。 类装载工作由ClassLoader及其子类负责，ClassLoader是一个重要的Java运行时系统组件，它负责在运行时查找和装入Class字节码文件。JVM在运行时会产生三个ClassLoader：根装载器、ExtClassLoader（扩展类装载器）和AppClassLoader（系统类装载器）。其中，根装载器不是ClassLoader的子类，它使用C++编写，因此我们在Java中看不到它，根装载器负责装载JRE的核心类库，如JRE目标下的rt.jar、charsets.jar等。ExtClassLoader和AppClassLoader都是ClassLoader的子类。其中ExtClassLoader负责装载JRE扩展目录ext中的JAR类包；AppClassLoader负责装载Classpath路径下的类包。 这三个类装载器之间存在父子层级关系，即根装载器是ExtClassLoader的父装载器，ExtClassLoader是AppClassLoader的父装载器。默认情况下，使用AppClassLoader装载应用程序的类，我们可以做一个实验： 代码清单: ClassLoaderTest.java123456789public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println("current loader:" + loader); System.out.println("parent loader:" + loader.getParent()); System.out.println("grandparent loader:" + loader.getParent().getParent()); &#125;&#125; 运行以上代码，在控制台上将打出以下信息： current loader:sun.misc.Launcher$AppClassLoader@131f71aparent loader:sun.misc.Launcher$ExtClassLoader@15601ea //①根装载器在Java中访问不到，所以返回nullgrandparent loader:null 通过以上的输出信息，我们知道当前的ClassLoader是AppClassLoader，父ClassLoader是ExtClassLoader，祖父ClassLoader是根类装载器，因为在Java中无法获得它的句柄，所以仅返回null。 JVM装载类时使用“全盘负责委托机制”，“全盘负责”是指当一个ClassLoader装载一个类的时，除非显式地使用另一个ClassLoader，该类所依赖及引用的类也由这个ClassLoader载入；“委托机制”是指先委托父装载器寻找目标类，只有在找不到的情况下才从自己的类路径中查找并装载目标类。这一点是从安全角度考虑的，试想如果有人编写了一个恶意的基础类（如java.lang.String）并装载到JVM中将会引起多么可怕的后果。但是由于有了“全盘负责委托机制”，java.lang.String永远是由根装载器来装载的，这样就避免了上述事件的发生。 ClassLoader重要方法在Java中，ClassLoader是一个抽象类，位于java.lang包中。下面对该类的一些重要接口方法进行介绍：Class loadClass(String name)name参数指定类装载器需要装载类的名字，必须使用全限定类名，如com.baobaotao. beans.Car。该方法有一个重载方法loadClass(String name ,boolean resolve)，resolve参数告诉类装载器是否需要解析该类。在初始化类之前，应考虑进行类解析的工作，但并不是所有的类都需要解析，如果JVM只需要知道该类是否存在或找出该类的超类，那么就不需要进行解析。 Class defineClass(String name, byte[] b, int off, int len)将类文件的字节数组转换成JVM内部的java.lang.Class对象。字节数组可以从本地文件系统、远程网络获取。name为字节数组对应的全限定类名。 Class findSystemClass(String name)从本地文件系统载入Class文件，如果本地文件系统不存在该Class文件，将抛出ClassNotFoundException异常。该方法是JVM默认使用的装载机制。Class findLoadedClass(String name)调用该方法来查看ClassLoader是否已装入某个类。如果已装入，那么返回java.lang.Class对象，否则返回null。如果强行装载已存在的类，将会抛出链接错误。 ClassLoader getParent()获取类装载器的父装载器，除根装载器外，所有的类装载器都有且仅有一个父装载器，ExtClassLoader的父装载器是根装载器，因为根装载器非Java编写，所以无法获得，将返回null。 除JVM默认的三个ClassLoader以外，可以编写自己的第三方类装载器，以实现一些特殊的需求。类文件被装载并解析后，在JVM内将拥有一个对应的java.lang.Class类描述对象，该类的实例都拥有指向这个类描述对象的引用，而类描述对象又拥有指向关联ClassLoader的引用，如图3所示。 每一个类在JVM中都拥有一个对应的java.lang.Class对象，它提供了类结构信息的描述。数组、枚举、注解以及基本Java类型（如int、double等），甚至void都拥有对应的Class对象。Class没有public的构造方法。Class对象是在装载类时由JVM通过调用类装载器中的defineClass()方法自动构造的。 Java反射机制Class反射对象描述类语义结构，可以从Class对象中获取构造函数、成员变量、方法类等类元素的反射对象，并以编程的方式通过这些反射对象对目标类对象进行操作。这些反射对象类在java.reflect包中定义，下面是最主要的三个反射类： Constructor：类的构造函数反射类，通过Class#getConstructors()方法可以获得类的所有构造函数反射对象数组。在JDK5.0中，还可以通过getConstructor(Class… parameterTypes)获取拥有特定入参的构造函数反射对象。Constructor的一个主要方法是newInstance(Object[] initargs)，通过该方法可以创建一个对象类的实例，相当于new关键字。在JDK5.0中该方法演化为更为灵活的形式：newInstance (Object… initargs)。 Method：类方法的反射类，通过Class#getDeclaredMethods()方法可以获取类的所有方法反射类对象数组Method[]。在JDK5.0中可以通过getDeclaredMethod(String name, Class… parameterTypes)获取特定签名的方法，name为方法名；Class…为方法入参类型列表。Method最主要的方法是invoke(Object obj, Object[] args)，obj表示操作的目标对象；args为方法入参，代码清单3 10③处演示了这个反射类的使用方法。在JDK 5.0中，该方法的形式调整为invoke(Object obj, Object… args)。此外，Method还有很多用于获取类方法更多信息的方法：1）Class getReturnType()：获取方法的返回值类型；2）Class[] getParameterTypes()：获取方法的入参类型数组；3）Class[] getExceptionTypes()：获取方法的异常类型数组；4）Annotation[][] getParameterAnnotations()：获取方法的注解信息，JDK 5.0中的新方法； Field：类的成员变量的反射类，通过Class#getDeclaredFields()方法可以获取类的成员变量反射对象数组，通过Class#getDeclaredField(String name)则可获取某个特定名称的成员变量反射对象。Field类最主要的方法是set(Object obj, Object value)，obj表示操作的目标对象，通过value为目标对象的成员变量设置值。如果成员变量为基础类型，用户可以使用Field类中提供的带类型名的值设置方法，如setBoolean(Object obj, boolean value)、setInt(Object obj, int value)等。 此外，Java还为包提供了Package反射类，在JDK 5.0中还为注解提供了AnnotatedElement反射类。总之，Java的反射体系保证了可以通过程序化的方式访问目标类中所有的元素，对于private或protected的成员变量和方法，只要JVM的安全机制允许，也可以通过反射进行调用，请看下面的例子： 代码清单 PrivateCar.java 12345678910111213141516171819202122import java.lang.reflect.Field;import java.lang.reflect.Method;public class PrivateCarReflect &#123; public static void main(String[] args) throws Throwable &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); Class clazz = loader.loadClass("PrivateCar"); PrivateCar pcar = (PrivateCar) clazz.newInstance(); Field colorFld = clazz.getDeclaredField("color"); // ①取消Java语言访问检查以访问private变量 colorFld.setAccessible(true); colorFld.set(pcar, "红色"); Method driveMtd = clazz.getDeclaredMethod("drive", (Class[]) null); // Method driveMtd = clazz.getDeclaredMethod("drive"); JDK5.0下使用 // ②取消Java语言访问检查以访问protected方法 driveMtd.setAccessible(true); driveMtd.invoke(pcar, (Object[]) null); &#125;&#125; 运行该类，打印出以下信息： drive private car! the color is:红色 在访问private、protected成员变量和方法时必须通过setAccessible(boolean access)方法取消Java语言检查，否则将抛出IllegalAccessException。如果JVM的安全管理器设置了相应的安全机制，调用该方法将抛出SecurityException。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Sublime Text 3简介]]></title>
      <url>%2Fpost%2Ftool-Sublime-introduction%2F</url>
      <content type="text"><![CDATA[sublime 是一款小巧绿色且速度非常快的代码编辑器，下面介绍一下它的安装和插件的使用 安装官方下载地址：http://www.sublimetext.com/3 破解打开sublime_text.exe，help-&gt;enter License附 license： license：—– BEGIN LICENSE —–K-20Single User LicenseEA7E-9401293A099EC1 C0B5C7C5 33EBF0CF BE82FE3BEAC2164A 4F8EC954 4E87F1E5 7E4E85D6C5605DE6 DAB003B4 D60CA4D0 77CB15333C47F579 FB3E8476 EB3AA9A7 68C43CD98C60B563 80FE367D 8CAD14B3 54FB7A9F4123FFC4 D63312BA 141AF702 F6BBA254B094B9C0 FAA4B04C 06CC9AFC FD41267182E3AEE0 0F0FAAA7 8FA773C9 383A9E18—— END LICENSE —— 更多参考：http://www.janecc.com/sublime-text-3-crack-zh-cn.html Sublime text 3 Package ControlSublime text 3 Package Control是sublime安装插件的管理器 安装步骤：http://jingyan.baidu.com/article/c14654134b8bde0bfcfc4c9a.html emmet的前身是大名鼎鼎的Zen coding，如果你从事Web前端开发的话，对该插件一定不会陌生。它使用仿CSS选择器的语法来生成代码，大大提高了HTML/CSS代码编写的速度 安装emmet插件：http://jingyan.baidu.com/article/ca00d56c76d0fae99eebcfdf.html 更多参考：http://www.iteye.com/news/27580 用emmet插件快速新建html文件，可以新建一个html文件 打开，然后输入 html加tab键 更多参考：http://www.cnblogs.com/freeyiyi1993/p/3629905.html 格式化html代码 tag插件 Ctrl+Alt+F对代码进行格式化参考：http://www.5imb.com/show-83-5884-1.html 快捷键：删除一行 ctrl+shift+k 更多快捷键，参考：http://www.jb51.net/softjc/180873.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Swagger简介]]></title>
      <url>%2Fpost%2Ftool-swagger-introduction%2F</url>
      <content type="text"><![CDATA[Swagger 是一款RESTFUL接口的文档在线自动生成+功能测试功能软件。本文简单介绍了在项目中集成swagger的方法和一些常见问题。 如果想深入分析项目源码，了解更多内容，见参考资料。Swagger 是一个规范和完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。总体目标是使客户端和文件系统作为服务器以同样的速度来更新。文件的方法，参数和模型紧密集成到服务器端的代码，允许API来始终保持同步。Swagger 让部署管理和使用功能强大的API从未如此简单。 使用介绍什么是swaggerSwagger™的目标是为REST APIs 定义一个标准的，与语言无关的接口，使人和计算机在看不到源码或者看不到文档或者不能通过网络流量检测的情况下能发现和理解各种服务的功能。当服务通过Swagger定义，消费者就能与远程的服务互动通过少量的实现逻辑。类似于低级编程接口，Swagger去掉了调用服务时的很多猜测。 浏览 Swagger-Spec 去了解更多关于Swagger 项目的信息，包括附加的支持其他语言的库。 如何集成Swagger-springmvc到我们的项目中?Maven12345&lt;dependency&gt; &lt;groupId&gt;com.mangofactory&lt;/groupId&gt; &lt;artifactId&gt;swagger-springmvc&lt;/artifactId&gt; &lt;version&gt;0.9.4&lt;/version&gt;&lt;/dependency&gt; Gradlerepositories { jcenter() } compile &quot;com.mangofactory:swagger-springmvc:0.9.4&quot; 要最快捷地启动swagger-springmvc项目并且使用默认设置，推荐的方式是使用SwaggerSpringMvc插件 Spring Java Configuration1234567@Configuration@EnableWebMvc@EnableSwagger@ComponentScan("com.myapp.packages")public class WebAppConfig &#123; ...&#125; Spring xml Configuration12&lt;mvc:annotation-driven/&gt; &lt;!-- Required so swagger-springmvc can access spring's RequestMappingHandlerMapping --&gt;&lt;bean class="com.mangofactory.swagger.configuration.SpringSwaggerConfig" /&gt; SwaggerSpringMvcPlugin XML Configuration为了使用这个插件，你需要创造一个spring Java配置类。使用spring的 @Configuration ，这个配置类必须被定义到你的xml上下文12&lt;!-- Required so swagger-springmvc can access spring's RequestMappingHandlerMapping --&gt;&lt;mvc:annotation-driven/&gt; 1234567891011121314151617181920212223242526bean class="com.yourapp.configuration.MySwaggerConfig"/&gt;@Configuration@EnableSwagger //Loads the spring beans required by the frameworkpublic class MySwaggerConfig &#123;private SpringSwaggerConfig springSwaggerConfig;/*** Required to autowire SpringSwaggerConfig*/@Autowiredpublic void setSpringSwaggerConfig(SpringSwaggerConfig springSwaggerConfig) &#123; this.springSwaggerConfig = springSwaggerConfig;&#125;/*** Every SwaggerSpringMvcPlugin bean is picked up by the swagger-mvc framework - allowing for multiple* swagger groups i.e. same code base multiple swagger resource listings. */@Beanpublic SwaggerSpringMvcPlugin customImplementation()&#123; return new SwaggerSpringMvcPlugin(this.springSwaggerConfig) .includePatterns(".*pet.*");&#125;&#125; SwaggerSpringMvcPlugin Spring Java Configuration使用@EnableSwagger注解自动注入SpringSwaggerConfig定义一个或多个SwaggerSpringMvcPlugin实例，通过springs @Bean注解 1234567891011121314151617181920212223242526272829303132@Configuration@EnableWebMvc@EnableSwagger@ComponentScan("com.myapp.controllers")public class CustomJavaPluginConfig &#123;private SpringSwaggerConfig springSwaggerConfig;@Autowiredpublic void setSpringSwaggerConfig(SpringSwaggerConfig springSwaggerConfig) &#123; this.springSwaggerConfig = springSwaggerConfig;&#125;@Bean //Don't forget the @Bean annotationpublic SwaggerSpringMvcPlugin customImplementation()&#123; return new SwaggerSpringMvcPlugin(this.springSwaggerConfig) .apiInfo(apiInfo()) .includePatterns(".*pet.*");&#125;private ApiInfo apiInfo() &#123; ApiInfo apiInfo = new ApiInfo( "My Apps API Title", "My Apps API Description", "My Apps API terms of service", "My Apps API Contact Email", "My Apps API Licence Type", "My Apps API License URL" ); return apiInfo;&#125;&#125; 碰到的问题关于@API注解在Swagger Annotation中： API表示一个开放的API，可以通过description简要描述该API的功能。在一个@API下，可有多个@ApiOperation，表示针对该API的CRUD操作。在ApiOperation Annotation中可以通过value，notes描述该操作的作用，response描述正常情况下该请求的返回对象类型。 在一个ApiOperation下，可以通过ApiResponses描述该API操作可能出现的异常情况。 ApiParam用于描述该API操作接受的参数类型再接着，为项目的Model对象添加Swagger Annotation，这样Swagger Scanner可以获取更多关于Model对象的信息。 12345678910111213141516171819@ApiModel(value = "A SayingRepresentation is a representation of greeting")@JsonSerialize(include = JsonSerialize.Inclusion.NON_NULL)public class SayingRepresentation &#123;private long id;@ApiModelProperty(value = "greeting content", required = true)private String content;public SayingRepresentation(long id, String content) &#123; this.id = id; this.content = content;&#125;public long getId() &#123; return id;&#125;public String getContent() &#123; return content;&#125; 通过上面的步骤，项目已经具备了提供Swagger格式的API信息的能力，接下来，我们把这些信息和Swagger UI集成，以非常美观，实用的方式把这些API信息展示出来。 ##和Swagger UI的集成首先，从github(https://github.com/wordnik/swagger-ui)上下载Swagger-UI, 把该项目dist目录下的内容拷贝到项目的resources的目录assets下。 然后，修改index.html, 把Swagger UI对象中的URL替换为自己的API路径。 window.swaggerUi = new SwaggerUi({ url: &quot;/api/api-docs&quot;, dom_id: &quot;swagger-ui-container&quot;, 最后，为了能访问到该页面，还需要在Service的Initialize方法中，添加AssetsBundle 12345public void initialize(Bootstrap&lt;HelloWorldConfiguration&gt; bootstrap) &#123; //指定配置文件的名字 bootstrap.setName("helloWorld"); bootstrap.addBundle(new AssetsBundle("/assets", "/", "index.html"));&#125; 最后的效果图： 评价Swagger可以充当前后端交流的重要桥梁，方便快捷。很实用。 Swagger项目允许你生产，显示和消费你自己的RESTful服务。不需要代理和第三方服务。是一个依赖自由的资源集合，它能通过Swagger API动态的生成漂亮的文档和沙盒,因为Swagger UI没有依赖，你可以把他部署到任何服务器环境或者是你自己的机器。 参考资料官网：http://swagger.io/ GitHub：swagger-springmvc:https://github.com/martypitt/swagger-springmvc swagger-ui:https://github.com/swagger-api/swagger-ui swagger-core:https://github.com/swagger-api/swagger-core swagger-spec：https://github.com/swagger-api/swagger-spec]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java项目中使用Mybatis入门程序]]></title>
      <url>%2Fpost%2Fmybatis-getting-started%2F</url>
      <content type="text"><![CDATA[MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。 什么是 MyBatis ?MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。 MyBatis githubhttps://github.com/mybatis/mybatis-3 MyBatis 文档http://mybatis.github.io/mybatis-3/zh/index.html 入门程序想要使用 MyBatis 只需将 mybatis-x.x.x.jar 文件置于 classpath 中。如果使用 Maven 构建项目，则需将下面的 dependency 置于 pom.xml 中：12345&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.8.2&lt;/version&gt; &lt;/dependency&gt; 我选择第二种，在Eclipse中新建一个maven项目 项目结构： 各文件介绍： pom文件：由于使用了mySQL数据库，这里还依赖了一个mySQL驱动包 123456789101112131415161718&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;Mybatis&lt;/groupId&gt; &lt;artifactId&gt;Mybatis&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 实体类User ： 123456789101112131415161718192021222324package com.mybatis.domain; public class User &#123; private String name; private Integer age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; &#125; UserMapper接口： 123456package com.mybatis.mapper; import com.mybatis.domain.User; public interface UserMapper &#123; public User findById(String Id); &#125; UserMapper接口的实现，userMapper.xml： 12345678&lt;?xml version="1.0" encoding="UTF-8" ?&gt; &lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt; &lt;!--相当于UserMapper接口的实现 namespace必须是UserMapper类路径--&gt;&lt;mapper namespace="com.mybatis.mapper.UserMapper"&gt; &lt;!-- findById必须和接口中的方法名一样 返回一个User--&gt; &lt;select id="findById" parameterType="String" resultType="com.mybatis.domain.User"&gt; select * from user where id=#&#123;id&#125; &lt;/select&gt; &lt;/mapper&gt; 数据源配置 configuration.xml 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8" ?&gt; &lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;!--数据源配置 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="jdbc" /&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/test" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="123456" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;!--userMapper.xml装载进来 --&gt; &lt;mapper resource="userMapper.xml" /&gt; &lt;/mappers&gt; &lt;/configuration&gt; 测试类MyBatisTest： 12345678910111213141516171819202122232425262728293031323334package com.mybatis.test;import com.mybatis.domain.User;import com.mybatis.mapper.UserMapper;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import java.io.IOException;public class MyBatisTest &#123; /** * MyBatis SqlSessionFactory * SqlSessionFactory????SqlSession????????????SqlSession??????????commit?rollback?close???? * @return */ private static SqlSessionFactory getSessionFactory() &#123; SqlSessionFactory sessionFactory = null; String resource = "configuration.xml"; try &#123; sessionFactory = new SqlSessionFactoryBuilder().build(Resources.getResourceAsReader( resource)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return sessionFactory; &#125; public static void main(String[] args) &#123; SqlSession sqlSession = getSessionFactory().openSession(); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); User user = userMapper.findById("1"); System.out.println(user.getName()); &#125;&#125; 数据库中插入条数据：id=”1” name=”wn” age=”23” 运行测试类，在控制台看到结果：wn]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch简介]]></title>
      <url>%2Fpost%2Felasticsearch-introduction%2F</url>
      <content type="text"><![CDATA[elasticsearch是一个可用于构建搜索应用的成品软件，区别去lucene这种中间件 Apache Lucence简介为什么elasticsearch创始人觉得使用lucene而不是自己开发全文检索库？可能是因为lucene的成熟，高性能，可扩展以及强大的功能，得到创始人的青睐，lucene有很多扩展功能，如 多语言处理，拼写检查，高亮显示等 lucene总体架构文档（document）,字段(field),词项(term)，词条(token) 倒排索引lucene将写入索引的所有信息组成一种名为倒排索引的结构，倒排索引是面向词项而不是面向文档，实际的lucene创建的索引更为复杂，也更先进，因为索引中还储存和很多其他信息，如词向量（为单个字段创建的小索引，储存改字段中所有的词条） 各字段的原始信息，文档删除标记 段（segment）每个段只会被创建一次但是会被查询多次，索引期间，段经创建就不会被修改，例如，文档被删除以后，删除信息被单独的保存在一个文件中，而段本身并没有修改 段合并多个段合并在一起，要么是强制执行，要么是有lucene的内在机制决定在某个时刻执行，段合并非常消耗I/O，且合并期间有些不再使用的信息也将被清理掉，例如:被删除的文档，不要强制执行段合并，只需要配置段合并策略，剩余的事情Lucene会自行搞定 文档数据是如果转化为倒排索引的？这个转换的过程称为分析（analysis），分析由分析器来执行，分析器由分词器(tokenizer)过滤器（filter）和字符映射器组成(character mapper),分词器用来切割词条，过滤器可以移除，修改词条流中的词条，甚至可以创造新的词条，字符映射器用于调用分词器之前的预处理操作，比如HTML文本去标签 ElasticSearch简介简介elasticsearch是一个可用于构建搜索应用的成品软件，区别去lucene这种中间件 索引es将数据储存在一个或多个索引中，就像是sql领域的数据库，es索引可能由一个或多个lucene索引构成，具体细节由es的索引分片，复制机制及其配置决定 文档是es时间中的主要实体，文档之间可能由各自不同的字段集合，且文档并没有固定的模式或强制的结构，这些规则也适用于lucene文档，事实上，es的文档最后都储存为lucene文档了 类型es每个文档都有与之对应的类型（type）定义，这允许用户在一个索引中存储多种文档类型，并为不同文档类型提供不同的映射 节点单个es服务器实例称为节点 集群多个节点来协同处理 分片es将数据散步到多个物理lucene索引上，这些索引称为分片，而散步分片的过程叫做分片处理（sharding），es会自动完成分片处理，并且让这些分片呈现出一个大索引的样子 副本为每个分片创建冗余的副本，处理查询时可以把这些副本用作最初的主分片 es架构背后的关键概念合理的默认配置，默认的分布式工作模式，对等架构可以避免单点故障，易于向集群扩充节点 es工作流程启动过程使用广播技术来发现同一个集群中的其他节点并与他们链接，集群中会有一个节点被选为管理节点，该节点复制集群的状态管理以及在集群拓扑变化时做出反应，分发索引分片到集群的相应节点上 故障检测对每一个丢失的主分片，一个新的主分片将会从原来的主分片的副本中脱颖而出，新分片和副本的放置策略是可配置的，用户可以根据具体需求进行配置 索引数据索引操作只会发生在主分片上，当把一个索引请求发送至某节点时，如果该节点没有对应的主分片或者只有副本，那么这个请求会被转发到拥有正确的主分片的节点 查询操作查询并不是一个简答的，单步骤的操作，一般来说，查询分为两个阶段，分散阶段和合并阶段 （注：内容整理自《深入理解Elasticsearch》）]]></content>
    </entry>

    
  
  
</search>
